{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# os.chdir('/Users/mac/AIworkspace/LLMWORKSPACE/RAG_Rec')\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raptor with rec\n",
    "1. target userì˜ ìµœì‹  ìƒí˜¸ì‘ìš© ëª©ë¡ì„ input queryë¡œ ë„£ìŒ -> ëª‡ê°œ ë„£ì„ì§€ëŠ” ë°”ê¿”ê°€ë©°\n",
    "2. vectordbëŠ” ëª¨ë“  ì‚¬ìš©ìë¥¼ ì—°ê²°ì‹œì¼œë‘” dbì—ì„œ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì„ ì‹œë„í•¨ \n",
    "3. ê°‘ì‘ìŠ¤ëŸ½ê²Œ í‰ì  ë° ì˜í™”ì˜ ì¥ë¥´ê°€ ë³€í™”í•˜ëŠ” ì§€ì ì—ì„œ ëŠì–´ ê° ìƒí˜¸ì‘ìš©ì˜ íŒ¨í„´ì„ headerë¡œ ë§Œë“¦ \n",
    "4. ê°€ì¥ í•˜ë‹¨ ê³„ì¸µì˜ í´ëŸ¬ìŠ¤í„°ì™€ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ì„ ë„˜ëŠ” ì§€ì  or ìœ ì‚¬ë„ì˜ ë³€í™”ìœ¨ì´ ê¸‰ê²©íˆ ë³€í™”í•˜ëŠ” ì‹œì ì—ì„œ ê²€ìƒ‰ ì¤‘ë‹¨ \n",
    "5. í•´ë‹¹ë˜ëŠ” ê²€ìƒ‰ ì‹œì ì˜ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ì‚¬ìš©ìë“¤ë§Œì„ top-100 ë½‘ì•„ëƒ„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜í™”ê¸°ë¡ ë°ì´í„° \n",
    "import pandas as pd\n",
    "file_path = \"data/movies.dat\"\n",
    "df2 = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df2.columns = [\"MovieID\", \"Title\", \"Genres\"]\n",
    "\n",
    "file_path = \"data/ratings.dat\"\n",
    "df = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df.columns = [\"UserId\", \"MovieID\", \"Ratings\",\"timestamp\"]\n",
    "new_df=df.merge(df2, on='MovieID')\n",
    "df_sorted = new_df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. ì‚¬ìš©ìë³„ interaction ë¦¬ìŠ¤íŠ¸ ìƒì„± ---\n",
    "df_sorted['interaction'] = df_sorted.apply(\n",
    "    lambda row: f\"{row['Genres']} (Rating: {row['Ratings']})\", axis=1\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ìë³„ interaction ì—°ê²° (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)\n",
    "user_interactions = df_sorted.groupby('UserId')['interaction'].apply(list).reset_index()\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "user_interactions.columns = ['UserId', 'interaction_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 2: ì¥ë¥´ë³„ í†µê³„ ì €ì¥\n",
    "def extract_genre_stats(interaction_list):\n",
    "    genre_ratings = defaultdict(list)\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„° íŒŒì‹±\n",
    "    for entry in interaction_list:\n",
    "        genres, rating = entry.split(' (Rating: ')\n",
    "        rating = float(rating.replace(')', ''))\n",
    "        \n",
    "        for genre in genres.split('|'):\n",
    "            genre_ratings[genre].append(rating)\n",
    "\n",
    "    # í†µê³„ ìƒì„±\n",
    "    avg = {g : round(pd.Series(r).mean(),2) for g,r in genre_ratings.items()}\n",
    "    # avg_variance = {g: (round(pd.Series(r).mean(), 2), \n",
    "    #                     round(pd.Series(r).var(), 2) if len(r) > 1 else 0) \n",
    "    #                 for g, r in genre_ratings.items()}\n",
    "    \n",
    "    count = {g: len(r) for g, r in genre_ratings.items()}\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    avg_var_text = ', '.join([f\"{g}({v})\" for g, v in avg.items()])\n",
    "    count_text = ', '.join([f\"{g}({v})\" for g, v in count.items()])\n",
    "    \n",
    "    return avg_var_text, count_text\n",
    "\n",
    "# Step 3: ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "user_interactions[['avg_var_text', 'count_text']] = user_interactions['interaction_list'].apply(\n",
    "    lambda x: pd.Series(extract_genre_stats(x))\n",
    ")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼\n",
    "user_interactions_final = user_interactions[['UserId', 'avg_var_text', 'count_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama í™œìš©í•˜ì—¬ ê° ìœ ì €ë³„ë¡œ header ìƒì„± \n",
    "- ì¥ë¥´ë³„ í‰ê·  í‰ì  \n",
    "- ì¥ë¥´ë³„ ë¶„ì‚° \n",
    "- ì‹œì²­íšŸìˆ˜ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/miniconda3/envs/rag_rec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "MY_HF_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.43s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>avg_var_text</th>\n",
       "      <th>count_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...</td>\n",
       "      <td>Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Action(3.5), Adventure(3.74), Romance(3.71), S...</td>\n",
       "      <td>Action(56), Adventure(19), Romance(24), Sci-Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...</td>\n",
       "      <td>Drama(8), Thriller(5), Comedy(30), Action(23),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId                                       avg_var_text  \\\n",
       "0       1  Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...   \n",
       "1       2  Action(3.5), Adventure(3.74), Romance(3.71), S...   \n",
       "2       3  Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...   \n",
       "\n",
       "                                          count_text  \n",
       "0  Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...  \n",
       "1  Action(56), Adventure(19), Romance(24), Sci-Fi...  \n",
       "2  Drama(8), Thriller(5), Comedy(30), Action(23),...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=user_interactions_final.iloc[:3]\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í˜„ì¬ í”„ë¡¬í”„íŠ¸\n",
    "- í‰ì  + ì‹œì²­íšŸìˆ˜ => ë‘˜ë‹¤ ë†’ê³  ë§ìœ¼ë©´ ì„ í˜¸í•˜ëŠ” ì¥ë¥´ / í‰ì ë§Œ ë†’ê³  íšŸìˆ˜ëŠ” ì ìœ¼ë©´ ì ì¬ ì¥ë¥´ \n",
    "\n",
    "##### ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ \n",
    "1. ë‹¤ì–‘í•œ ì¥ë¥´ë¥¼ ê³¨ê³ ë¥´ => ì‹œì²­íšŸìˆ˜ê°€ ë‹¤ ë¹„ìŠ·í•˜ë©´ \n",
    "2. ë¹„ì„ í˜¸í•˜ëŠ” ì¥ë¥´ ê³ ë¥´ê¸° \n",
    "3. ë¶„ì‚°ì„ í†µí•œ ì¼ê´€ì„± or í˜¸ë¶ˆí˜¸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "# Function to Generate Concise Contextual Chunk Header Using CoT\n",
    "def generate_chunk_header(avg_rating_text, count_text):\n",
    "    system_message = (\n",
    "        \"You are an expert in analyzing movie viewing behavior. \"\n",
    "        \"Your task is to analyze the user's movie preferences and generate a concise summary of their overall viewing pattern. \"\n",
    "        \"Do NOT include specific rating numbers, viewing counts, or detailed explanations. \"\n",
    "        \"Only provide a brief and meaningful summary of their primary preferences and emerging interests. \"\n",
    "        \"Your final response should be 1-2 clear and natural sentences.\"\n",
    "    )\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "### INPUT DATA ###\n",
    "1. **Genre Statistics (Average Rating):** {avg_rating_text}\n",
    "2. **Genre Viewing Frequency (Count):** {count_text}\n",
    "\n",
    "### OUTPUT FORMAT EXAMPLE ###\n",
    "\"This user enjoys emotional and family-oriented genres while occasionally exploring niche genres like Sci-Fi.\"\n",
    "\n",
    "### RESPONSE ###\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id = terminators[0],\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    return outputs[0]['generated_text'][2]['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= generate_chunk_header(k['avg_var_text'].iloc[0] , k['count_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions_final['chunk_header'] = user_interactions_final.apply(lambda row: generate_chunk_header(row['avg_var_text'], row['count_text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('header_vanila.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒì„±ëœ headerë¥¼ í™œìš©í•˜ì—¬ raptorì— ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This user tends to favor genres that are emotionally resonant and family-friendly, such as Drama, Children's, and Romance, while occasionally showing interest in more niche and visually engaging genres like Sci-Fi and Animation.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.chunk_header.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì„ë² ë”© ëª¨ë¸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì„í¬íŠ¸ ë¯¸ë¦¬ ì¤€ë¹„\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# ë§Œë“¤ì–´ì§„ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_texts(self, texts: list[str]) -> np.ndarray:\n",
    "        # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "        return self.model.encode(texts, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raptor ê¸°ë°˜ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMClusterer:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_predict(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:\n",
    "        # ğŸ”¥ ë§¤ ë ˆë²¨ë§ˆë‹¤ n_clustersë¥¼ ì „ë‹¬í•˜ë„ë¡ ë³€ê²½\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=self.random_state)\n",
    "        return gmm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaptorTree:\n",
    "    def __init__(self, embedding_generator, clusterer, min_clusters=2, max_level=5, top_level_clusters=100):\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.clusterer = clusterer\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_level = max_level\n",
    "        self.top_level_clusters = top_level_clusters\n",
    "        self.tree = {}\n",
    "        self.user_id_to_text = {}\n",
    "\n",
    "    def build_tree(self, texts: list[str], user_ids: list[str]):\n",
    "        self.user_id_to_text = dict(zip(user_ids, texts))\n",
    "        current_texts = texts\n",
    "        current_user_ids = user_ids\n",
    "        current_level = 0\n",
    "        parent_ids = None\n",
    "\n",
    "        while len(current_texts) > 1 and current_level < self.max_level:\n",
    "            embeddings = self.embedding_generator.embed_texts(current_texts)\n",
    "\n",
    "            # ğŸ”¥ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì„¤ì • ìˆ˜ì • (ë¹ˆ í´ëŸ¬ìŠ¤í„° ë°©ì§€)\n",
    "            n_clusters = max(self.min_clusters, min(len(current_texts) // 2, self.top_level_clusters // (current_level + 1)))\n",
    "\n",
    "            cluster_labels = self.clusterer.fit_predict(embeddings, n_clusters=n_clusters)\n",
    "\n",
    "            cluster_metadata = []\n",
    "            next_level_texts = []\n",
    "            next_level_user_ids = []\n",
    "\n",
    "            for cluster_id in np.unique(cluster_labels):\n",
    "                cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "                cluster_texts = [current_texts[i] for i in cluster_indices]\n",
    "\n",
    "                # ğŸ”¥ ë¹ˆ í´ëŸ¬ìŠ¤í„° ì œê±°\n",
    "                if len(cluster_texts) == 0:\n",
    "                    continue\n",
    "\n",
    "                # ğŸ”¥ ì²« ë²ˆì§¸ ë ˆë²¨ì—ì„œëŠ” user_ids ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                if current_level == 0:\n",
    "                    cluster_user_ids = [current_user_ids[i] for i in cluster_indices]\n",
    "                else:\n",
    "                    # ìƒìœ„ ë ˆë²¨ì—ì„œëŠ” ì´ì „ í´ëŸ¬ìŠ¤í„° ID ê¸°ì¤€ìœ¼ë¡œ `user_ids` ì¶”ê°€\n",
    "                    cluster_user_ids = []\n",
    "                    for idx in cluster_indices:\n",
    "                        child_cluster_id = current_user_ids[idx]\n",
    "                        for child_meta in self.tree[current_level - 1]:\n",
    "                            if child_meta[\"cluster_id\"] == child_cluster_id:\n",
    "                                cluster_user_ids.extend(child_meta[\"user_ids\"])\n",
    "\n",
    "                # ğŸ”¥ ëŒ€í‘œ í…ìŠ¤íŠ¸ (ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì„ ì •)\n",
    "                representative_text = max(cluster_texts, key=len)\n",
    "\n",
    "                cluster_embeddings = embeddings[cluster_indices]\n",
    "                mean_embedding = cluster_embeddings.mean(axis=0)\n",
    "\n",
    "                # ğŸ”¥ ì˜¬ë°”ë¥¸ parent_id ì„¤ì • (ë¶€ëª¨ í´ëŸ¬ìŠ¤í„° 1ê°œë§Œ ì¶”ê°€)\n",
    "                metadata = {\n",
    "                    \"cluster_id\": f\"level_{current_level}_cluster_{cluster_id}\",\n",
    "                    \"level\": current_level,\n",
    "                    \"user_ids\": cluster_user_ids,  # âœ… ì²« ë²ˆì§¸ ë ˆë²¨ì—ì„œë„ user_ids ì¶”ê°€\n",
    "                    \"embedding\": mean_embedding,\n",
    "                    \"parent_id\": parent_ids if parent_ids else [],\n",
    "                    \"child_ids\": None\n",
    "                }\n",
    "\n",
    "                cluster_metadata.append(metadata)\n",
    "                next_level_texts.append(representative_text)\n",
    "                next_level_user_ids.append(metadata[\"cluster_id\"])\n",
    "\n",
    "            # í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "            self.tree[current_level] = cluster_metadata\n",
    "            current_texts = next_level_texts\n",
    "            current_user_ids = next_level_user_ids\n",
    "            parent_ids = current_user_ids\n",
    "            current_level += 1\n",
    "\n",
    "        return self.tree\n",
    "\n",
    "    def search_user_cluster(self, target_user_id: str, target_user_text: str, threshold=0.01):\n",
    "        query_embedding = self.embedding_generator.embed_texts([target_user_text])[0]\n",
    "        current_level = max(self.tree.keys())\n",
    "        previous_similarity = None\n",
    "        best_cluster = None\n",
    "\n",
    "        while current_level >= 0:\n",
    "            clusters = self.tree[current_level]\n",
    "            clusters_filtered = []\n",
    "            clusters_filtered_embeddings = []\n",
    "\n",
    "            for cluster in clusters:\n",
    "                cluster_user_ids = cluster[\"user_ids\"]\n",
    "\n",
    "                # ğŸ”¥ í´ëŸ¬ìŠ¤í„° IDê°€ ì•„ë‹Œ ì‹¤ì œ ìœ ì € IDë§Œ í•„í„°ë§\n",
    "                texts_to_embed = [\n",
    "                    self.user_id_to_text[uid] for uid in cluster_user_ids\n",
    "                    if uid != target_user_id and 'cluster' not in uid\n",
    "                ]\n",
    "                if not texts_to_embed:\n",
    "                    continue\n",
    "\n",
    "                embeddings_cluster = self.embedding_generator.embed_texts(texts_to_embed)\n",
    "                mean_embedding = embeddings_cluster.mean(axis=0)\n",
    "\n",
    "                clusters_filtered.append(cluster)\n",
    "                clusters_filtered_embeddings.append(mean_embedding)\n",
    "\n",
    "            if not clusters_filtered:\n",
    "                break\n",
    "\n",
    "            similarities = cosine_similarity([query_embedding], clusters_filtered_embeddings).flatten()\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_cluster = clusters_filtered[best_idx]\n",
    "            current_similarity = similarities[best_idx]\n",
    "\n",
    "            if previous_similarity and abs(previous_similarity - current_similarity) / previous_similarity > threshold:\n",
    "                break\n",
    "\n",
    "            previous_similarity = current_similarity\n",
    "            current_level -= 1\n",
    "\n",
    "        best_cluster_users_excluded = [uid for uid in best_cluster[\"user_ids\"] if uid != target_user_id]\n",
    "        return best_cluster[\"cluster_id\"], best_cluster_users_excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©° ë³€í™”ìœ¨ì˜ ì¦í­ê¸°ê°„ì—ì„œ ë©ˆì¶¤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìœ ì € 6040ê°€ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°: level_2_cluster_10\n",
      "í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ìœ ì‚¬ ìœ ì € ëª©ë¡ (ë³¸ì¸ ì œì™¸): ['42', '57', '99', '218', '237', '301', '314', '320', '360', '597', '646', '673', '770', '853', '910', '931', '943', '1133', '1169', '1183', '1307', '1368', '1499', '1508', '1546', '1592', '1636', '1661', '1701', '1772', '1817', '1834', '1846', '1942', '2005', '2054', '2173', '2213', '2279', '2288', '2335', '2463', '2566', '2568', '2645', '2752', '2755', '2848', '2863', '2875', '2885', '2972', '3091', '3189', '3206', '3210', '3400', '3412', '3439', '3444', '3632', '3775', '3905', '4034', '4067', '4078', '4161', '4181', '4195', '4210', '4258', '4272', '4281', '4362', '4519', '4737', '4747', '4807', '5323', '5340', '5359', '5397', '5402', '5404', '5494', '5504', '5545', '5706', '5801', '5816', '5880', '5905', '5969', '6002', '6025', '6031', '338', '508', '550', '610', '632', '858', '1047', '1306', '1311', '1410', '1565', '1711', '1826', '1851', '1852', '1961', '2039', '2067', '2099', '2139', '2162', '2224', '2334', '2424', '2713', '2765', '2900', '3046', '3249', '3258', '3260', '3442', '3509', '3611', '3809', '3815', '3897', '4121', '4129', '4215', '4375', '4403', '4548', '4638', '4698', '4793', '4833', '4871', '4904', '4930', '5237', '5249', '5298', '5498', '5576', '5645', '5663', '5672', '5684', '5691', '5698', '5714', '5869', '5965']\n"
     ]
    }
   ],
   "source": [
    "# # ì´ˆê¸°í™” ì˜ˆì‹œ\n",
    "# embedding_gen = EmbeddingGenerator()\n",
    "# clusterer = GMMClusterer()\n",
    "\n",
    "# # ì´ˆê¸° RAPTOR íŠ¸ë¦¬ ìƒì„± (í•œ ë²ˆë§Œ ìˆ˜í–‰)\n",
    "# raptor_tree = RaptorTree(embedding_gen, clusterer)\n",
    "# tree_structure = raptor_tree.build_tree(df.chunk_header.tolist(), df.UserId.astype(str).tolist())\n",
    "\n",
    "# íŠ¹ì • ì‚¬ìš©ì ê²€ìƒ‰ ìˆ˜í–‰ (Self-exclusion ë°©ì‹ ì ìš©)\n",
    "target_user_id = \"6040\"\n",
    "target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "    target_user_id, target_user_text, threshold=0.005\n",
    ")\n",
    "\n",
    "print(f\"ìœ ì € {target_user_id}ê°€ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°: {best_cluster_id}\")\n",
    "print(f\"í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ìœ ì‚¬ ìœ ì € ëª©ë¡ (ë³¸ì¸ ì œì™¸): {similar_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1ì°¨ í•„í„°ë§ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['42', '57', '99', '218', '237']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_users[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. meta chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ìµœì†Œ ì¡°ê±´ ì—†ëŠ” ì²­í‚¹ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"vectorstore_index_ratings_min5_no\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # ì—­ì§ë ¬í™” í—ˆìš©\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1221 (Action|Crime|Drama) ratings: 4']\""
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "file_path='data/train_movie.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# 'movie_explain' ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "data['movie_explain'] = data['movie_explain'].apply(ast.literal_eval)\n",
    "file_path_test='data/test_movie.csv'\n",
    "# ìµœì‹  êµ¬ë§¤ ê¸°ë¡ì„ ê°€ì ¸ì˜´\n",
    "purchase_history=data.iloc[6039]['movie_explain']\n",
    "# ì •ë‹µ ë°ì´í„°ì…‹ ê°€ì ¸ì˜´\n",
    "df_test=pd.read_csv(file_path_test)\n",
    "# ì •ë‹µ\n",
    "df_test.iloc[6039].movie_explain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ëœ ì‚¬ìš©ì ìˆ˜: 500\n",
      "âœ… ê²¹ì¹˜ëŠ” ì‚¬ìš©ì ìˆ˜: 10\n",
      "ğŸ§‘â€ğŸ¤â€ğŸ§‘ ê²¹ì¹˜ëŠ” ì‚¬ìš©ì ID ëª©ë¡: ['2752', '301', '3439', '3509', '4793', '5359', '5504', '5801', '770', '910']\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ìµœì‹  êµ¬ë§¤ ê¸°ë¡ (Query)\n",
    "query = \" \".join(purchase_history[-1:])  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "# ğŸ”¹ similar_usersì˜ íƒ€ì… í™•ì¸ í›„ ë³€í™˜\n",
    "similar_users_str = set(map(str, similar_users))  # ë¬¸ìì—´ ë³€í™˜\n",
    "\n",
    "# ğŸ”¹ FAISSì—ì„œ `similar_users`ë§Œ ê²€ìƒ‰í•˜ë„ë¡ í•„í„° ì¶”ê°€\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 500,\n",
    "        # \"filter\": user_filter  # ğŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "    }\n",
    ")\n",
    "\n",
    "# ğŸ”¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "records = retriever.get_relevant_documents(query)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "intersection = similar_users_str.intersection(set(record_user_ids))\n",
    "\n",
    "# ğŸ”¹ 'similar_users'ì— ì†í•˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§\n",
    "filtered_records = [\n",
    "    record for record in records \n",
    "    if str(record.metadata['UserId']) in similar_users_str\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_documents_with_context(\n",
    "    vectorstore: FAISS,\n",
    "    filtered_records: List[Document],\n",
    "    context_window: int = 1\n",
    ") -> List[List[Document]]:\n",
    "    \"\"\"\n",
    "    intersection ìœ ì €ë“¤ì˜ ê²€ìƒ‰ ê²°ê³¼ì™€ ê° ê²°ê³¼ì˜ ì•ë’¤ ë¬¸ì„œë“¤ì„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤\n",
    "        filtered_records: ê²€ìƒ‰ëœ ìœ ì €ë“¤ì˜ ê¸°ë¡\n",
    "        context_window: ì•ë’¤ë¡œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜ (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        List[List[Document]]: ê° ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ [ì´ì „ ë¬¸ì„œë“¤, í˜„ì¬ ë¬¸ì„œ, ë‹¤ìŒ ë¬¸ì„œë“¤]ì„ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    # ğŸ”¹ ëª¨ë“  ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±\n",
    "    all_docs = {}\n",
    "    for doc_id, doc in enumerate(vectorstore.docstore._dict.values()):\n",
    "        user_id = str(doc.metadata['UserId'])\n",
    "        chunk_idx = doc.metadata['chunk_index']\n",
    "        \n",
    "        if user_id not in all_docs:\n",
    "            all_docs[user_id] = {}\n",
    "        all_docs[user_id][chunk_idx] = doc\n",
    "    \n",
    "    # ğŸ”¹ Context ì¶”ê°€\n",
    "    context_results = []\n",
    "    \n",
    "    for doc in filtered_records:\n",
    "        current_user_id = str(doc.metadata['UserId'])\n",
    "        current_chunk_index = doc.metadata['chunk_index']\n",
    "        \n",
    "        context_docs = []\n",
    "        \n",
    "        # ğŸ”¹ ì´ì „ ë¬¸ì„œë“¤ ì¶”ê°€\n",
    "        for i in range(current_chunk_index - context_window, current_chunk_index):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        # ğŸ”¹ í˜„ì¬ ë¬¸ì„œ ì¶”ê°€\n",
    "        context_docs.append(doc)\n",
    "        \n",
    "        # ğŸ”¹ ë‹¤ìŒ ë¬¸ì„œë“¤ ì¶”ê°€\n",
    "        for i in range(current_chunk_index + 1, current_chunk_index + context_window + 1):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        context_results.append(context_docs)\n",
    "    \n",
    "    return context_results\n",
    "\n",
    "# ğŸ”¹ intersection ìœ ì €ë“¤ì˜ ì²­í¬ ì•ë’¤ ì²­í¬ ì¶”ì¶œ\n",
    "context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1299 (Drama|War) ratings: 3 2067 (Drama|Romance|War) ratings: 5 3448 (Comedy|Drama|War) ratings: 3 1183 (Drama|Romance|War) ratings: 4 1619 (Drama|War) ratings: 3 2287 (Sci-Fi|Thriller|War) ratings: 4 2670 (War) ratings: 4\\n161 (Drama|Thriller|War) ratings: 2 3724 (Drama|War) ratings: 4 1408 (Action|Romance|War) ratings: 5 3654 (Action|Drama|War) ratings: 5 3339 (War) ratings: 5\\n3366 (Action|Adventure|War) ratings: 3 151 (Drama|Romance|War) ratings: 5 516 (Comedy|Drama|War) ratings: 4 1094 (Drama|Romance|War) ratings: 3 2427 (Action|Drama|War) ratings: 1\\n1729 (Crime|Drama) ratings: 5 348 (Comedy) ratings: 1 1408 (Action|Romance|War) ratings: 4 1885 (Comedy|Drama) ratings: 4 417 (Comedy|Romance) ratings: 5\\n161 (Drama|Thriller|War) ratings: 3 1661 (Thriller) ratings: 3 1845 (Comedy|Thriller) ratings: 4 1748 (Film-Noir|Sci-Fi|Thriller) ratings: 4 314 (Drama) ratings: 3\\n3499 (Horror) ratings: 3 431 (Crime|Drama) ratings: 4 164 (Crime|Film-Noir|Mystery|Thriller) ratings: 3 1407 (Horror|Thriller) ratings: 2 1921 (Sci-Fi|Thriller) ratings: 4\\n2406 (Action|Adventure|Comedy|Romance) ratings: 4 1198 (Action|Adventure) ratings: 5 3639 (Action) ratings: 3 1196 (Action|Adventure|Drama|Sci-Fi|War) ratings: 4 2987 (Adventure|Animation|Film-Noir) ratings: 3\\n1617 (Crime|Film-Noir|Mystery|Thriller) ratings: 5 3793 (Action|Sci-Fi) ratings: 4 3753 (Action|Drama|War) ratings: 4 1090 (Drama|War) ratings: 4 3408 (Drama) ratings: 4\\n3836 (Action|Comedy|War) ratings: 4 1028 (Children's|Comedy|Musical) ratings: 5 1235 (Comedy) ratings: 3 1625 (Mystery|Thriller) ratings: 5 293 (Crime|Drama|Romance|Thriller) ratings: 4\\n32 (Drama|Sci-Fi) ratings: 3 1393 (Drama|Romance) ratings: 3 628 (Drama|Thriller) ratings: 5 2329 (Drama) ratings: 5 2761 (Animation|Children's) ratings: 5\\n161 (Drama|Thriller|War) ratings: 2 2268 (Crime|Drama) ratings: 2 3499 (Horror) ratings: 2 2542 (Comedy|Crime|Thriller) ratings: 5 34 (Children's|Comedy|Drama) ratings: 2\\n1408 (Action|Romance|War) ratings: 4 300 (Drama) ratings: 3 16 (Drama|Thriller) ratings: 5 1407 (Horror|Thriller) ratings: 3 474 (Action|Thriller) ratings: 5\\n1616 (Action|Thriller|War) ratings: 3 3671 (Comedy|Western) ratings: 5 1193 (Drama) ratings: 5 2311 (Mystery|Sci-Fi) ratings: 3 2023 (Action|Crime|Drama) ratings: 3 1198 (Action|Adventure) ratings: 4\\n1394 (Comedy) ratings: 5 3543 (Comedy|Drama) ratings: 3 541 (Film-Noir|Sci-Fi) ratings: 5 1079 (Comedy) ratings: 5 1225 (Drama) ratings: 4 1299 (Drama|War) ratings: 5 3683 (Drama|Film-Noir) ratings: 4\\n3365 (Western) ratings: 3 337 (Drama) ratings: 3 3499 (Horror) ratings: 5 3246 (Drama) ratings: 4 2529 (Action|Sci-Fi) ratings: 3\\n161 (Drama|Thriller|War) ratings: 3 1954 (Action|Drama) ratings: 5 21 (Action|Comedy|Drama) ratings: 5 1465 (Drama) ratings: 3 2291 (Drama|Romance) ratings: 5 39 (Comedy|Romance) ratings: 4\\n553 (Western) ratings: 5 1246 (Drama) ratings: 5 1517 (Comedy) ratings: 5 1059 (Drama|Romance) ratings: 3 3763 (Action|Crime|Thriller) ratings: 5\\n1178 (Drama|War) ratings: 5 3545 (Musical|War) ratings: 5 1204 (Adventure|War) ratings: 5 527 (Drama|War) ratings: 5 3742 (Drama|War) ratings: 5 110 (Action|Drama|War) ratings: 5 1939 (Drama|War) ratings: 5 1208 (Drama|War) ratings: 5 1217 (Drama|War) ratings: 5 1233 (Action|Drama|War) ratings: 5 1262 (Adventure|War) ratings: 5 3035 (Comedy|Drama|War) ratings: 5 3196 (Drama|War) ratings: 4 1960 (Drama|War) ratings: 4 2944 (Action|War) ratings: 5 1242 (Action|Drama|War) ratings: 5 1250 (Drama|War) ratings: 5 1294 (Comedy|War) ratings: 4 3342 (Drama|War) ratings: 5 2028 (Action|Drama|War) ratings: 5 969 (Action|Adventure|Romance|War) ratings: 5 1944 (Drama|Romance|War) ratings: 5 1256 (Comedy|War) ratings: 5 1272 (Drama|War) ratings: 5 1090 (Drama|War) ratings: 5 920 (Drama|Romance|War) ratings: 5 3062 (Action|Drama|War) ratings: 5 356 (Comedy|Romance|War) ratings: 5 1222 (Action|Drama|War) ratings: 5 1263 (Drama|War) ratings: 5\\n1299 (Drama|War) ratings: 3 2067 (Drama|Romance|War) ratings: 5 3448 (Comedy|Drama|War) ratings: 3 1183 (Drama|Romance|War) ratings: 4 1619 (Drama|War) ratings: 3 2287 (Sci-Fi|Thriller|War) ratings: 4 2670 (War) ratings: 4\\n161 (Drama|Thriller|War) ratings: 2 3724 (Drama|War) ratings: 4 1408 (Action|Romance|War) ratings: 5 3654 (Action|Drama|War) ratings: 5 3339 (War) ratings: 5\\n741 (Animation|Sci-Fi) ratings: 4 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 2 2193 (Action|Adventure|Fantasy) ratings: 4 2150 (Comedy) ratings: 3 2333 (Drama) ratings: 2\\n1615 (Adventure|Thriller) ratings: 4 3053 (Drama|War) ratings: 3 353 (Action|Romance|Thriller) ratings: 5 3186 (Drama) ratings: 3 2058 (Action|Thriller) ratings: 4\\n2296 (Comedy) ratings: 1 3745 (Adventure|Animation|Sci-Fi) ratings: 4 3889 (Action|Adventure|Fantasy) ratings: 4 3174 (Comedy|Drama) ratings: 4 750 (Sci-Fi|War) ratings: 4\\n1193 (Drama) ratings: 4 1203 (Drama) ratings: 3 1225 (Drama) ratings: 3 1197 (Action|Adventure|Comedy|Romance) ratings: 4 1256 (Comedy|War) ratings: 3\\n1242 (Action|Drama|War) ratings: 5 2728 (Drama) ratings: 4 908 (Drama|Thriller) ratings: 4 953 (Drama) ratings: 3 1172 (Comedy|Drama|Romance) ratings: 3\\n2918 (Comedy) ratings: 5 2947 (Action) ratings: 3 1281 (Comedy) ratings: 4 2398 (Drama) ratings: 3 1132 (Drama) ratings: 4\\n3545 (Musical|War) ratings: 2 581 (Documentary) ratings: 4 1957 (Drama) ratings: 5 432 (Comedy|Western) ratings: 2 3471 (Drama|Sci-Fi) ratings: 3\\n1597 (Action|Mystery|Romance|Thriller) ratings: 3 161 (Drama|Thriller|War) ratings: 2 1094 (Drama|Romance|War) ratings: 4 590 (Adventure|Drama|Western) ratings: 4 1438 (Action|Thriller) ratings: 2\\n2043 (Adventure|Children's|Fantasy) ratings: 3 1008 (Western) ratings: 3 3258 (Comedy) ratings: 3 3430 (Action|Drama) ratings: 4 1246 (Drama) ratings: 5\\n353 (Action|Romance|Thriller) ratings: 2 31 (Drama) ratings: 3 2231 (Crime|Drama) ratings: 3 2617 (Action|Adventure|Horror|Thriller) ratings: 2 140 (Drama|Romance) ratings: 1\\n1608 (Action|Thriller) ratings: 3 1676 (Action|Adventure|Sci-Fi|War) ratings: 4 1441 (Comedy|Romance) ratings: 2 1619 (Drama|War) ratings: 2 1907 (Animation|Children's) ratings: 3\\n1727 (Drama) ratings: 2 3425 (Drama) ratings: 4 266 (Drama|Romance|War|Western) ratings: 4 253 (Drama|Horror) ratings: 4 3809 (Comedy) ratings: 1 2012 (Comedy|Sci-Fi|Western) ratings: 1\\n50 (Crime|Thriller) ratings: 4 920 (Drama|Romance|War) ratings: 5 2289 (Comedy|Drama) ratings: 4 3114 (Animation|Children's|Comedy) ratings: 5 1258 (Horror) ratings: 4\\n1617 (Crime|Film-Noir|Mystery|Thriller) ratings: 5 1 (Animation|Children's|Comedy) ratings: 5 1233 (Action|Drama|War) ratings: 5 1834 (Drama|Thriller) ratings: 3 589 (Action|Sci-Fi|Thriller) ratings: 5\\n1704 (Drama) ratings: 5 2391 (Crime|Thriller) ratings: 3 457 (Action|Thriller) ratings: 4 1797 (Documentary) ratings: 5 1393 (Drama|Romance) ratings: 4\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "record_summary = \"\\n\".join([doc.page_content for doc in flattened_results])\n",
    "record_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 1408: watched by 3 users\n",
      "Movie 3499: watched by 3 users\n",
      "Movie 1619: watched by 2 users\n",
      "Movie 3545: watched by 2 users\n",
      "Movie 1233: watched by 2 users\n",
      "Movie 1256: watched by 2 users\n",
      "Movie 1407: watched by 2 users\n",
      "Movie 1393: watched by 2 users\n",
      "Movie 353: watched by 2 users\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_user_movies(data):\n",
    "    user_movies = defaultdict(list)\n",
    "\n",
    "    for doc in data:\n",
    "        user_id = doc.metadata['UserId']\n",
    "        page_content = doc.page_content\n",
    "\n",
    "        # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì˜í™” ID ì¶”ì¶œ (ratings ì´ì „ ë‚´ìš©ë§Œ)\n",
    "        movie_ids = re.findall(r'(\\d+)(?= \\()', page_content)\n",
    "\n",
    "        # ì‚¬ìš©ìë³„ë¡œ ì˜í™” ID ì¶”ê°€\n",
    "        user_movies[user_id].extend(movie_ids)\n",
    "\n",
    "    return user_movies\n",
    "\n",
    "def create_user_movie_graph(user_movies):\n",
    "    # ê·¸ë˜í”„ ìƒì„±\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ì¶”ê°€ ë° ì—£ì§€ ìƒì„±\n",
    "    for user_id, movies in user_movies.items():\n",
    "        user_node = f\"User {user_id}\"\n",
    "        G.add_node(user_node, type='user')\n",
    "\n",
    "        for movie_id in movies:\n",
    "            movie_node = f\"Movie {movie_id}\"\n",
    "            G.add_node(movie_node, type='movie')\n",
    "            G.add_edge(user_node, movie_node)\n",
    "\n",
    "    return G\n",
    "\n",
    "def visualize_graph(G):\n",
    "    # ê·¸ë˜í”„ ì‹œê°í™”\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ë¶„ë¦¬\n",
    "    users = [node for node in G.nodes() if G.nodes[node]['type'] == 'user']\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "    # ë…¸ë“œ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=users, node_color='lightblue', node_size=300, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=movies, node_color='lightgreen', node_size=200, alpha=0.8)\n",
    "\n",
    "    # ì—£ì§€ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "    # ë ˆì´ë¸” ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(\"User-Movie Relationship Graph\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ê·¸ë˜í”„ ì €ì¥\n",
    "    plt.savefig('user_movie_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "import re\n",
    "\n",
    "# ê³¼ê±° ê¸°ë¡ì—ì„œ ì˜í™” ID ì¶”ì¶œ\n",
    "def extract_previous_movie_ids(purchase_history):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œ (ìˆ«ìì™€ ê´„í˜¸ ì „ê¹Œì§€ë§Œ ê°€ì ¸ì˜´)\n",
    "    previous_movies = re.findall(r'(\\d+)(?=\\s\\()', ' '.join(purchase_history))\n",
    "    return set(previous_movies)\n",
    "\n",
    "# ë™ì‹œ ì‹œì²­ ì˜í™”ì—ì„œ ì´ì „ ê¸°ë¡ì„ ì œì™¸í•˜ëŠ” í•¨ìˆ˜\n",
    "def filter_movies_by_history(top_movies, previous_movie_ids):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ í›„ ì œì™¸\n",
    "    filtered_movies = [(movie, count) for movie, count in top_movies if movie.split()[1] not in previous_movie_ids]\n",
    "    return filtered_movies\n",
    "def get_top_10_common_movies(G):\n",
    "    # ì˜í™” ë…¸ë“œë§Œ í•„í„°ë§\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ì˜í™”ë³„ ì—°ê²°ëœ ì‚¬ìš©ì ìˆ˜ ê³„ì‚°\n",
    "    movie_view_counts = {movie: len(list(G.neighbors(movie))) for movie in movies}\n",
    "\n",
    "    # ì‚¬ìš©ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ 10ê°œ ì¶”ì¶œ\n",
    "    top_10_movies = sorted(movie_view_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    # ê²°ê³¼ ë°˜í™˜\n",
    "    return top_10_movies\n",
    "\n",
    "# ì‚¬ìš©ìë³„ ì˜í™” ID ê°€ì ¸ì˜¤ê¸°\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# Top-10 ì˜í™” ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# ì´ì „ ê¸°ë¡ì— í¬í•¨ë˜ì§€ ì•Šì€ ì˜í™”ë§Œ í•„í„°ë§\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
