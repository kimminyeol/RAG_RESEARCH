{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# os.chdir('/Users/mac/AIworkspace/LLMWORKSPACE/RAG_Rec')\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raptor with rec\n",
    "1. target userì˜ ìµœì‹  ìƒí˜¸ì‘ìš© ëª©ë¡ì„ input queryë¡œ ë„£ìŒ -> ëª‡ê°œ ë„£ì„ì§€ëŠ” ë°”ê¿”ê°€ë©°\n",
    "2. vectordbëŠ” ëª¨ë“  ì‚¬ìš©ìë¥¼ ì—°ê²°ì‹œì¼œë‘” dbì—ì„œ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì„ ì‹œë„í•¨ \n",
    "3. ê°‘ì‘ìŠ¤ëŸ½ê²Œ í‰ì  ë° ì˜í™”ì˜ ì¥ë¥´ê°€ ë³€í™”í•˜ëŠ” ì§€ì ì—ì„œ ëŠì–´ ê° ìƒí˜¸ì‘ìš©ì˜ íŒ¨í„´ì„ headerë¡œ ë§Œë“¦ \n",
    "4. ê°€ì¥ í•˜ë‹¨ ê³„ì¸µì˜ í´ëŸ¬ìŠ¤í„°ì™€ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ì„ ë„˜ëŠ” ì§€ì  or ìœ ì‚¬ë„ì˜ ë³€í™”ìœ¨ì´ ê¸‰ê²©íˆ ë³€í™”í•˜ëŠ” ì‹œì ì—ì„œ ê²€ìƒ‰ ì¤‘ë‹¨ \n",
    "5. í•´ë‹¹ë˜ëŠ” ê²€ìƒ‰ ì‹œì ì˜ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ì‚¬ìš©ìë“¤ë§Œì„ top-100 ë½‘ì•„ëƒ„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜í™”ê¸°ë¡ ë°ì´í„° \n",
    "import pandas as pd\n",
    "file_path = \"data/movies.dat\"\n",
    "df2 = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df2.columns = [\"MovieID\", \"Title\", \"Genres\"]\n",
    "\n",
    "file_path = \"data/ratings.dat\"\n",
    "df = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df.columns = [\"UserId\", \"MovieID\", \"Ratings\",\"timestamp\"]\n",
    "new_df=df.merge(df2, on='MovieID')\n",
    "df_sorted = new_df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. ì‚¬ìš©ìë³„ interaction ë¦¬ìŠ¤íŠ¸ ìƒì„± ---\n",
    "df_sorted['interaction'] = df_sorted.apply(\n",
    "    lambda row: f\"{row['Genres']} (Rating: {row['Ratings']})\", axis=1\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ìë³„ interaction ì—°ê²° (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)\n",
    "user_interactions = df_sorted.groupby('UserId')['interaction'].apply(list).reset_index()\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "user_interactions.columns = ['UserId', 'interaction_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 2: ì¥ë¥´ë³„ í†µê³„ ì €ì¥\n",
    "def extract_genre_stats(interaction_list):\n",
    "    genre_ratings = defaultdict(list)\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„° íŒŒì‹±\n",
    "    for entry in interaction_list:\n",
    "        genres, rating = entry.split(' (Rating: ')\n",
    "        rating = float(rating.replace(')', ''))\n",
    "        \n",
    "        for genre in genres.split('|'):\n",
    "            genre_ratings[genre].append(rating)\n",
    "\n",
    "    # í†µê³„ ìƒì„±\n",
    "    avg = {g : round(pd.Series(r).mean(),2) for g,r in genre_ratings.items()}\n",
    "    # avg_variance = {g: (round(pd.Series(r).mean(), 2), \n",
    "    #                     round(pd.Series(r).var(), 2) if len(r) > 1 else 0) \n",
    "    #                 for g, r in genre_ratings.items()}\n",
    "    \n",
    "    count = {g: len(r) for g, r in genre_ratings.items()}\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    avg_var_text = ', '.join([f\"{g}({v})\" for g, v in avg.items()])\n",
    "    count_text = ', '.join([f\"{g}({v})\" for g, v in count.items()])\n",
    "    \n",
    "    return avg_var_text, count_text\n",
    "\n",
    "# Step 3: ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "user_interactions[['avg_var_text', 'count_text']] = user_interactions['interaction_list'].apply(\n",
    "    lambda x: pd.Series(extract_genre_stats(x))\n",
    ")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼\n",
    "user_interactions_final = user_interactions[['UserId', 'avg_var_text', 'count_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama í™œìš©í•˜ì—¬ ê° ìœ ì €ë³„ë¡œ header ìƒì„± \n",
    "- ì¥ë¥´ë³„ í‰ê·  í‰ì  \n",
    "- ì¥ë¥´ë³„ ë¶„ì‚° \n",
    "- ì‹œì²­íšŸìˆ˜ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/miniconda3/envs/rag_rec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "MY_HF_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.43s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>avg_var_text</th>\n",
       "      <th>count_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...</td>\n",
       "      <td>Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Action(3.5), Adventure(3.74), Romance(3.71), S...</td>\n",
       "      <td>Action(56), Adventure(19), Romance(24), Sci-Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...</td>\n",
       "      <td>Drama(8), Thriller(5), Comedy(30), Action(23),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId                                       avg_var_text  \\\n",
       "0       1  Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...   \n",
       "1       2  Action(3.5), Adventure(3.74), Romance(3.71), S...   \n",
       "2       3  Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...   \n",
       "\n",
       "                                          count_text  \n",
       "0  Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...  \n",
       "1  Action(56), Adventure(19), Romance(24), Sci-Fi...  \n",
       "2  Drama(8), Thriller(5), Comedy(30), Action(23),...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=user_interactions_final.iloc[:3]\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í˜„ì¬ í”„ë¡¬í”„íŠ¸\n",
    "- í‰ì  + ì‹œì²­íšŸìˆ˜ => ë‘˜ë‹¤ ë†’ê³  ë§ìœ¼ë©´ ì„ í˜¸í•˜ëŠ” ì¥ë¥´ / í‰ì ë§Œ ë†’ê³  íšŸìˆ˜ëŠ” ì ìœ¼ë©´ ì ì¬ ì¥ë¥´ \n",
    "\n",
    "##### ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ \n",
    "1. ë‹¤ì–‘í•œ ì¥ë¥´ë¥¼ ê³¨ê³ ë¥´ => ì‹œì²­íšŸìˆ˜ê°€ ë‹¤ ë¹„ìŠ·í•˜ë©´ \n",
    "2. ë¹„ì„ í˜¸í•˜ëŠ” ì¥ë¥´ ê³ ë¥´ê¸° \n",
    "3. ë¶„ì‚°ì„ í†µí•œ ì¼ê´€ì„± or í˜¸ë¶ˆí˜¸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "# Function to Generate Concise Contextual Chunk Header Using CoT\n",
    "def generate_chunk_header(avg_rating_text, count_text):\n",
    "    system_message = (\n",
    "        \"You are an expert in analyzing movie viewing behavior. \"\n",
    "        \"Your task is to analyze the user's movie preferences and generate a concise summary of their overall viewing pattern. \"\n",
    "        \"Do NOT include specific rating numbers, viewing counts, or detailed explanations. \"\n",
    "        \"Only provide a brief and meaningful summary of their primary preferences and emerging interests. \"\n",
    "        \"Your final response should be 1-2 clear and natural sentences.\"\n",
    "    )\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "### INPUT DATA ###\n",
    "1. **Genre Statistics (Average Rating):** {avg_rating_text}\n",
    "2. **Genre Viewing Frequency (Count):** {count_text}\n",
    "\n",
    "### OUTPUT FORMAT EXAMPLE ###\n",
    "\"This user enjoys emotional and family-oriented genres while occasionally exploring niche genres like Sci-Fi.\"\n",
    "\n",
    "### RESPONSE ###\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id = terminators[0],\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    return outputs[0]['generated_text'][2]['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= generate_chunk_header(k['avg_var_text'].iloc[0] , k['count_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions_final['chunk_header'] = user_interactions_final.apply(lambda row: generate_chunk_header(row['avg_var_text'], row['count_text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('header_vanila.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒì„±ëœ headerë¥¼ í™œìš©í•˜ì—¬ raptorì— ì ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì„ë² ë”© ëª¨ë¸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/miniconda3/envs/rag_rec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì„í¬íŠ¸ ë¯¸ë¦¬ ì¤€ë¹„\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# ë§Œë“¤ì–´ì§„ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device= device)\n",
    "    \n",
    "    def embed_texts(self, texts: list[str]) -> np.ndarray:\n",
    "        # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "        return self.model.encode(texts, convert_to_numpy=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raptor ê¸°ë°˜ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMClusterer:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_predict(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:\n",
    "        # ğŸ”¥ ë§¤ ë ˆë²¨ë§ˆë‹¤ n_clustersë¥¼ ì „ë‹¬í•˜ë„ë¡ ë³€ê²½\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=self.random_state)\n",
    "        return gmm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaptorTree:\n",
    "    def __init__(self, embedding_generator, clusterer, min_clusters=2, max_level=5, top_level_clusters=100):\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.clusterer = clusterer\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_level = max_level\n",
    "        self.top_level_clusters = top_level_clusters\n",
    "        self.tree = {}\n",
    "        self.user_id_to_text = {}\n",
    "\n",
    "    def build_tree(self, texts: list[str], user_ids: list[str]):\n",
    "        self.user_id_to_text = dict(zip(user_ids, texts))\n",
    "        current_texts = texts\n",
    "        current_user_ids = user_ids\n",
    "        current_level = 0\n",
    "        parent_ids = None\n",
    "\n",
    "        while len(current_texts) > 1 and current_level < self.max_level:\n",
    "            embeddings = self.embedding_generator.embed_texts(current_texts)\n",
    "\n",
    "            # ğŸ”¥ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì„¤ì • ìˆ˜ì • (ë¹ˆ í´ëŸ¬ìŠ¤í„° ë°©ì§€)\n",
    "            n_clusters = max(self.min_clusters, min(len(current_texts) // 2, self.top_level_clusters // (current_level + 1)))\n",
    "\n",
    "            cluster_labels = self.clusterer.fit_predict(embeddings, n_clusters=n_clusters)\n",
    "\n",
    "            cluster_metadata = []\n",
    "            next_level_texts = []\n",
    "            next_level_user_ids = []\n",
    "\n",
    "            for cluster_id in np.unique(cluster_labels):\n",
    "                cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "                cluster_texts = [current_texts[i] for i in cluster_indices]\n",
    "\n",
    "                # ğŸ”¥ ë¹ˆ í´ëŸ¬ìŠ¤í„° ì œê±°\n",
    "                if len(cluster_texts) == 0:\n",
    "                    continue\n",
    "\n",
    "                # ğŸ”¥ ì²« ë²ˆì§¸ ë ˆë²¨ì—ì„œëŠ” user_ids ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                if current_level == 0:\n",
    "                    cluster_user_ids = [current_user_ids[i] for i in cluster_indices]\n",
    "                else:\n",
    "                    # ìƒìœ„ ë ˆë²¨ì—ì„œëŠ” ì´ì „ í´ëŸ¬ìŠ¤í„° ID ê¸°ì¤€ìœ¼ë¡œ `user_ids` ì¶”ê°€\n",
    "                    cluster_user_ids = []\n",
    "                    for idx in cluster_indices:\n",
    "                        child_cluster_id = current_user_ids[idx]\n",
    "                        for child_meta in self.tree[current_level - 1]:\n",
    "                            if child_meta[\"cluster_id\"] == child_cluster_id:\n",
    "                                cluster_user_ids.extend(child_meta[\"user_ids\"])\n",
    "\n",
    "                # ğŸ”¥ ëŒ€í‘œ í…ìŠ¤íŠ¸ (ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì„ ì •)\n",
    "                representative_text = max(cluster_texts, key=len)\n",
    "\n",
    "                cluster_embeddings = embeddings[cluster_indices]\n",
    "                mean_embedding = cluster_embeddings.mean(axis=0)\n",
    "\n",
    "                # ğŸ”¥ ì˜¬ë°”ë¥¸ parent_id ì„¤ì • (ë¶€ëª¨ í´ëŸ¬ìŠ¤í„° 1ê°œë§Œ ì¶”ê°€)\n",
    "                metadata = {\n",
    "                    \"cluster_id\": f\"level_{current_level}_cluster_{cluster_id}\",\n",
    "                    \"level\": current_level,\n",
    "                    \"user_ids\": cluster_user_ids,  # âœ… ì²« ë²ˆì§¸ ë ˆë²¨ì—ì„œë„ user_ids ì¶”ê°€\n",
    "                    \"embedding\": mean_embedding,\n",
    "                    \"parent_id\": parent_ids if parent_ids else [],\n",
    "                    \"child_ids\": None\n",
    "                }\n",
    "\n",
    "                cluster_metadata.append(metadata)\n",
    "                next_level_texts.append(representative_text)\n",
    "                next_level_user_ids.append(metadata[\"cluster_id\"])\n",
    "\n",
    "            # í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "            self.tree[current_level] = cluster_metadata\n",
    "            current_texts = next_level_texts\n",
    "            current_user_ids = next_level_user_ids\n",
    "            parent_ids = current_user_ids\n",
    "            current_level += 1\n",
    "\n",
    "        return self.tree\n",
    "\n",
    "    def search_user_cluster(self, target_user_id: str, target_user_text: str, threshold=0.01):\n",
    "        query_embedding = self.embedding_generator.embed_texts([target_user_text])[0]\n",
    "        current_level = max(self.tree.keys())\n",
    "        previous_similarity = None\n",
    "        best_cluster = None\n",
    "\n",
    "        while current_level >= 0:\n",
    "            clusters = self.tree[current_level]\n",
    "            clusters_filtered = []\n",
    "            clusters_filtered_embeddings = []\n",
    "\n",
    "            for cluster in clusters:\n",
    "                cluster_user_ids = cluster[\"user_ids\"]\n",
    "\n",
    "                # ğŸ”¥ í´ëŸ¬ìŠ¤í„° IDê°€ ì•„ë‹Œ ì‹¤ì œ ìœ ì € IDë§Œ í•„í„°ë§\n",
    "                texts_to_embed = [\n",
    "                    self.user_id_to_text[uid] for uid in cluster_user_ids\n",
    "                    if uid != target_user_id and 'cluster' not in uid\n",
    "                ]\n",
    "                if not texts_to_embed:\n",
    "                    continue\n",
    "\n",
    "                embeddings_cluster = self.embedding_generator.embed_texts(texts_to_embed)\n",
    "                mean_embedding = embeddings_cluster.mean(axis=0)\n",
    "\n",
    "                clusters_filtered.append(cluster)\n",
    "                clusters_filtered_embeddings.append(mean_embedding)\n",
    "\n",
    "            if not clusters_filtered:\n",
    "                break\n",
    "\n",
    "            similarities = cosine_similarity([query_embedding], clusters_filtered_embeddings).flatten()\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_cluster = clusters_filtered[best_idx]\n",
    "            current_similarity = similarities[best_idx]\n",
    "\n",
    "            if previous_similarity and abs(previous_similarity - current_similarity) / previous_similarity > threshold:\n",
    "                break\n",
    "\n",
    "            previous_similarity = current_similarity\n",
    "            current_level -= 1\n",
    "\n",
    "        best_cluster_users_excluded = [uid for uid in best_cluster[\"user_ids\"] if uid != target_user_id]\n",
    "        return best_cluster[\"cluster_id\"], best_cluster_users_excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©° ë³€í™”ìœ¨ì˜ ì¦í­ê¸°ê°„ì—ì„œ ë©ˆì¶¤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìœ ì € 2ê°€ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°: level_2_cluster_16\n",
      "í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ìœ ì‚¬ ìœ ì € ëª©ë¡ (ë³¸ì¸ ì œì™¸): ['46', '127', '304', '1089', '1302', '1424', '1610', '1726', '1927', '2101', '2126', '2146', '2263', '2297', '2363', '2441', '2514', '3144', '3352', '3460', '3567', '3636', '3673', '4000', '4031', '4109', '4160', '4587', '4690', '4874', '4892', '5040', '5069', '5469', '5703', '5895', '6034', '7', '277', '296', '431', '908', '1030', '1131', '1200', '1398', '1520', '1649', '1866', '2466', '2598', '2663', '3048', '3068', '3307', '3337', '3459', '3461', '3487', '3662', '3818', '3978', '4093', '4183', '4417', '4489', '4499', '4626', '5003', '5029', '5095', '5854', '5870', '5871', '5884', '5912', '5947', '6020', '185', '279', '422', '542', '633', '677', '700', '917', '1260', '1269', '1533', '1548', '1567', '1682', '2062', '2114', '2163', '2211', '2357', '2369', '2388', '2479', '2625', '2642', '2767', '2967', '3027', '3162', '3187', '3455', '3490', '3616', '3733', '3822', '3828', '3855', '4200', '4207', '4531', '4579', '5309', '5389', '5737', '5749', '5859']\n"
     ]
    }
   ],
   "source": [
    "# ì´ˆê¸°í™” ì˜ˆì‹œ\n",
    "embedding_gen = EmbeddingGenerator()\n",
    "clusterer = GMMClusterer()\n",
    "\n",
    "# ì´ˆê¸° RAPTOR íŠ¸ë¦¬ ìƒì„± (í•œ ë²ˆë§Œ ìˆ˜í–‰)\n",
    "# í´ë˜ìŠ¤ ì´ˆê¸°í™” ë¶€ë¶„\n",
    "raptor_tree = RaptorTree(\n",
    "    embedding_gen, \n",
    "    clusterer,\n",
    "    min_clusters=2,\n",
    "    max_level=5,          # ë” ë†’ì´ê±°ë‚˜ ë‚®ì¶°ì„œ ì¡°ì •\n",
    "    top_level_clusters=100 # ìµœìƒìœ„ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì—¬ë³¸ë‹¤\n",
    ")\n",
    "tree_structure = raptor_tree.build_tree(df.chunk_header.tolist(), df.UserId.astype(str).tolist())\n",
    "\n",
    "# íŠ¹ì • ì‚¬ìš©ì ê²€ìƒ‰ ìˆ˜í–‰ (Self-exclusion ë°©ì‹ ì ìš©)\n",
    "target_user_id = \"2\"\n",
    "target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "    target_user_id, target_user_text, threshold=0.005\n",
    ")\n",
    "\n",
    "print(f\"ìœ ì € {target_user_id}ê°€ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°: {best_cluster_id}\")\n",
    "print(f\"í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ìœ ì‚¬ ìœ ì € ëª©ë¡ (ë³¸ì¸ ì œì™¸): {similar_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1ì°¨ í•„í„°ë§ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_users) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. meta chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ìµœì†Œ ì¡°ê±´ ì—†ëŠ” ì²­í‚¹ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/321042087.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"vectorstore_index_ratings_min5_no\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # ì—­ì§ë ¬í™” í—ˆìš©\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "file_path='data/train_movie.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# 'movie_explain' ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "data['movie_explain'] = data['movie_explain'].apply(ast.literal_eval)\n",
    "file_path_test='data/test_movie.csv'\n",
    "# ìµœì‹  êµ¬ë§¤ ê¸°ë¡ì„ ê°€ì ¸ì˜´\n",
    "purchase_history=data.iloc[1]['movie_explain']\n",
    "# ì •ë‹µ ë°ì´í„°ì…‹ ê°€ì ¸ì˜´\n",
    "df_test=pd.read_csv(file_path_test)\n",
    "# ì •ë‹µ\n",
    "df_test.iloc[1].movie_explain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/3104281221.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  records = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ìµœì‹  êµ¬ë§¤ ê¸°ë¡ (Query)\n",
    "query = \" \".join(purchase_history[-1:])  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "# ğŸ”¹ similar_usersì˜ íƒ€ì… í™•ì¸ í›„ ë³€í™˜\n",
    "similar_users_str = set(map(str, similar_users))  # ë¬¸ìì—´ ë³€í™˜\n",
    "\n",
    "# ğŸ”¹ FAISSì—ì„œ `similar_users`ë§Œ ê²€ìƒ‰í•˜ë„ë¡ í•„í„° ì¶”ê°€\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 500,\n",
    "        # \"filter\": user_filter  # ğŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "    }\n",
    ")\n",
    "\n",
    "# ğŸ”¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "records = retriever.get_relevant_documents(query)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "intersection = similar_users_str.intersection(set(record_user_ids))\n",
    "\n",
    "# ğŸ”¹ 'similar_users'ì— ì†í•˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§\n",
    "filtered_records = [\n",
    "    record for record in records \n",
    "    if str(record.metadata['UserId']) in similar_users_str\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_documents_with_context(\n",
    "    vectorstore: FAISS,\n",
    "    filtered_records: List[Document],\n",
    "    context_window: int = 1\n",
    ") -> List[List[Document]]:\n",
    "    \"\"\"\n",
    "    intersection ìœ ì €ë“¤ì˜ ê²€ìƒ‰ ê²°ê³¼ì™€ ê° ê²°ê³¼ì˜ ì•ë’¤ ë¬¸ì„œë“¤ì„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤\n",
    "        filtered_records: ê²€ìƒ‰ëœ ìœ ì €ë“¤ì˜ ê¸°ë¡\n",
    "        context_window: ì•ë’¤ë¡œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜ (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        List[List[Document]]: ê° ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ [ì´ì „ ë¬¸ì„œë“¤, í˜„ì¬ ë¬¸ì„œ, ë‹¤ìŒ ë¬¸ì„œë“¤]ì„ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    # ğŸ”¹ ëª¨ë“  ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±\n",
    "    all_docs = {}\n",
    "    for doc_id, doc in enumerate(vectorstore.docstore._dict.values()):\n",
    "        user_id = str(doc.metadata['UserId'])\n",
    "        chunk_idx = doc.metadata['chunk_index']\n",
    "        \n",
    "        if user_id not in all_docs:\n",
    "            all_docs[user_id] = {}\n",
    "        all_docs[user_id][chunk_idx] = doc\n",
    "    \n",
    "    # ğŸ”¹ Context ì¶”ê°€\n",
    "    context_results = []\n",
    "    \n",
    "    for doc in filtered_records:\n",
    "        current_user_id = str(doc.metadata['UserId'])\n",
    "        current_chunk_index = doc.metadata['chunk_index']\n",
    "        \n",
    "        context_docs = []\n",
    "        \n",
    "        # ğŸ”¹ ì´ì „ ë¬¸ì„œë“¤ ì¶”ê°€\n",
    "        for i in range(current_chunk_index - context_window, current_chunk_index):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        # ğŸ”¹ í˜„ì¬ ë¬¸ì„œ ì¶”ê°€\n",
    "        context_docs.append(doc)\n",
    "        \n",
    "        # ğŸ”¹ ë‹¤ìŒ ë¬¸ì„œë“¤ ì¶”ê°€\n",
    "        for i in range(current_chunk_index + 1, current_chunk_index + context_window + 1):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        context_results.append(context_docs)\n",
    "    \n",
    "    return context_results\n",
    "\n",
    "# ğŸ”¹ intersection ìœ ì €ë“¤ì˜ ì²­í¬ ì•ë’¤ ì²­í¬ ì¶”ì¶œ\n",
    "context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"3020 (Action|Drama) ratings: 5 292 (Action|Drama|Thriller) ratings: 3 1769 (Action|Thriller) ratings: 2 736 (Action|Adventure|Romance|Thriller) ratings: 2 1667 (Action|Drama) ratings: 4 434 (Action|Adventure|Crime) ratings: 3 511 (Action|Drama) ratings: 3 2126 (Action|Crime|Mystery|Thriller) ratings: 3 2094 (Action|Adventure|Sci-Fi) ratings: 3\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 1 420 (Action|Comedy) ratings: 3 208 (Action|Adventure) ratings: 1 2409 (Action|Drama) ratings: 5 2410 (Action|Drama) ratings: 2\\n2411 (Action|Drama) ratings: 5 2412 (Action|Drama) ratings: 2 1954 (Action|Drama) ratings: 5 2657 (Comedy|Horror|Musical|Sci-Fi) ratings: 2 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 4\\n1320 (Action|Horror|Sci-Fi|Thriller) ratings: 5 674 (Adventure|Sci-Fi) ratings: 4 2311 (Mystery|Sci-Fi) ratings: 4 1690 (Action|Horror|Sci-Fi) ratings: 4 2641 (Action|Adventure|Sci-Fi) ratings: 2 3300 (Action|Sci-Fi) ratings: 3 3033 (Comedy|Sci-Fi) ratings: 2 2094 (Action|Adventure|Sci-Fi) ratings: 3 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 2 329 (Action|Adventure|Sci-Fi) ratings: 1\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 196 (Horror|Sci-Fi) ratings: 3 1779 (Adventure|Sci-Fi|Thriller) ratings: 3 173 (Action|Adventure|Sci-Fi) ratings: 2 3354 (Sci-Fi) ratings: 3 2851 (Adventure|Sci-Fi|Thriller) ratings: 3 880 (Sci-Fi|Thriller) ratings: 3\\n160 (Action|Adventure|Mystery|Sci-Fi) ratings: 1 2448 (Horror|Sci-Fi) ratings: 2 1862 (Horror|Sci-Fi) ratings: 3 1129 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1374 (Action|Adventure|Sci-Fi) ratings: 2 2287 (Sci-Fi|Thriller|War) ratings: 3 1356 (Action|Adventure|Sci-Fi) ratings: 2 748 (Action|Sci-Fi|Thriller) ratings: 3 1240 (Action|Sci-Fi|Thriller) ratings: 4 1200 (Action|Sci-Fi|Thriller|War) ratings: 5\\n1603 (Sci-Fi|Thriller) ratings: 3 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 1 1320 (Action|Horror|Sci-Fi|Thriller) ratings: 4 1371 (Action|Adventure|Sci-Fi) ratings: 3 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 1\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 3354 (Sci-Fi) ratings: 3 748 (Action|Sci-Fi|Thriller) ratings: 4 780 (Action|Sci-Fi|War) ratings: 4 2808 (Action|Sci-Fi) ratings: 2\\n3024 (Horror|Sci-Fi) ratings: 4 1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1779 (Adventure|Sci-Fi|Thriller) ratings: 3 172 (Action|Sci-Fi|Thriller) ratings: 1 849 (Action|Adventure|Sci-Fi|Thriller) ratings: 2 880 (Sci-Fi|Thriller) ratings: 2 173 (Action|Adventure|Sci-Fi) ratings: 1\\n1376 (Action|Adventure|Sci-Fi) ratings: 2 3740 (Action|Comedy) ratings: 3 555 (Action|Crime|Romance) ratings: 4 1676 (Action|Adventure|Sci-Fi|War) ratings: 4 480 (Action|Adventure|Sci-Fi) ratings: 2 1527 (Action|Sci-Fi) ratings: 1\\n1573 (Action|Sci-Fi|Thriller) ratings: 5 1580 (Action|Adventure|Comedy|Sci-Fi) ratings: 1 1587 (Action|Adventure) ratings: 3 163 (Action|Romance|Thriller) ratings: 5 2989 (Action) ratings: 2\\n2058 (Action|Thriller) ratings: 4 3444 (Action) ratings: 1 2826 (Action|Horror|Thriller) ratings: 3 1377 (Action|Adventure|Comedy|Crime) ratings: 2 3519 (Action|War) ratings: 3\\n1270 (Comedy|Sci-Fi) ratings: 3 1097 (Children's|Drama|Fantasy|Sci-Fi) ratings: 2 480 (Action|Adventure|Sci-Fi) ratings: 3 920 (Drama|Romance|War) ratings: 5 1589 (Crime|Drama|Mystery) ratings: 4\\n1573 (Action|Sci-Fi|Thriller) ratings: 4 2916 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1580 (Action|Adventure|Comedy|Sci-Fi) ratings: 4 1748 (Film-Noir|Sci-Fi|Thriller) ratings: 4 329 (Action|Adventure|Sci-Fi) ratings: 2 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 2 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 2 2827 (Sci-Fi|Thriller) ratings: 3\\n1219 (Horror|Thriller) ratings: 5 593 (Drama|Thriller) ratings: 5 1617 (Crime|Film-Noir|Mystery|Thriller) ratings: 3 608 (Crime|Drama|Thriller) ratings: 5 50 (Crime|Thriller) ratings: 5 3147 (Drama|Thriller) ratings: 4 2762 (Thriller) ratings: 5\\n628 (Drama|Thriller) ratings: 5 802 (Drama|Romance) ratings: 5 1748 (Film-Noir|Sci-Fi|Thriller) ratings: 3 2353 (Action|Thriller) ratings: 5 1061 (Crime|Drama) ratings: 4 3252 (Drama) ratings: 4 21 (Action|Comedy|Drama) ratings: 4 454 (Drama|Thriller) ratings: 4 3108 (Comedy|Drama|Romance) ratings: 3\\n1573 (Action|Sci-Fi|Thriller) ratings: 5 1754 (Action|Mystery|Thriller) ratings: 5 832 (Drama|Thriller) ratings: 4 3793 (Action|Sci-Fi) ratings: 3 3578 (Action|Drama) ratings: 4\\n1997 (Horror) ratings: 3 3617 (Comedy) ratings: 3 3863 (Sci-Fi|Thriller) ratings: 4 3755 (Action|Adventure|Thriller) ratings: 3 3826 (Horror|Sci-Fi|Thriller) ratings: 3 3717 (Action|Crime) ratings: 3\\n2394 (Animation|Musical) ratings: 3 69 (Comedy) ratings: 4 509 (Drama|Romance) ratings: 2 999 (Crime) ratings: 2 543 (Comedy|Romance|Thriller) ratings: 1\\n1527 (Action|Sci-Fi) ratings: 4 353 (Action|Romance|Thriller) ratings: 2 10 (Action|Adventure|Thriller) ratings: 2 2676 (Drama|Thriller) ratings: 4 991 (Drama|War) ratings: 4 1589 (Crime|Drama|Mystery) ratings: 3\\n371 (Comedy|Drama) ratings: 5 2439 (Drama) ratings: 3 1799 (Crime|Drama) ratings: 4 2427 (Action|Drama|War) ratings: 5 802 (Drama|Romance) ratings: 1\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "record_summary = \"\\n\".join([doc.page_content for doc in flattened_results])\n",
    "record_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 788: watched by 3 users\n",
      "Movie 1573: watched by 3 users\n",
      "Movie 2094: watched by 2 users\n",
      "Movie 1320: watched by 2 users\n",
      "Movie 329: watched by 2 users\n",
      "Movie 1917: watched by 2 users\n",
      "Movie 1779: watched by 2 users\n",
      "Movie 173: watched by 2 users\n",
      "Movie 3354: watched by 2 users\n",
      "Movie 880: watched by 2 users\n",
      "Movie 748: watched by 2 users\n",
      "Movie 1580: watched by 2 users\n",
      "Movie 1589: watched by 2 users\n",
      "Movie 1748: watched by 2 users\n",
      "Movie 802: watched by 2 users\n",
      "Movie 3020: watched by 1 users\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_user_movies(data):\n",
    "    user_movies = defaultdict(list)\n",
    "\n",
    "    for doc in data:\n",
    "        user_id = doc.metadata['UserId']\n",
    "        page_content = doc.page_content\n",
    "\n",
    "        # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì˜í™” ID ì¶”ì¶œ (ratings ì´ì „ ë‚´ìš©ë§Œ)\n",
    "        movie_ids = re.findall(r'(\\d+)(?= \\()', page_content)\n",
    "\n",
    "        # ì‚¬ìš©ìë³„ë¡œ ì˜í™” ID ì¶”ê°€\n",
    "        user_movies[user_id].extend(movie_ids)\n",
    "\n",
    "    return user_movies\n",
    "\n",
    "def create_user_movie_graph(user_movies):\n",
    "    # ê·¸ë˜í”„ ìƒì„±\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ì¶”ê°€ ë° ì—£ì§€ ìƒì„±\n",
    "    for user_id, movies in user_movies.items():\n",
    "        user_node = f\"User {user_id}\"\n",
    "        G.add_node(user_node, type='user')\n",
    "\n",
    "        for movie_id in movies:\n",
    "            movie_node = f\"Movie {movie_id}\"\n",
    "            G.add_node(movie_node, type='movie')\n",
    "            G.add_edge(user_node, movie_node)\n",
    "\n",
    "    return G\n",
    "\n",
    "def visualize_graph(G):\n",
    "    # ê·¸ë˜í”„ ì‹œê°í™”\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ë¶„ë¦¬\n",
    "    users = [node for node in G.nodes() if G.nodes[node]['type'] == 'user']\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "    # ë…¸ë“œ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=users, node_color='lightblue', node_size=300, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=movies, node_color='lightgreen', node_size=200, alpha=0.8)\n",
    "\n",
    "    # ì—£ì§€ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "    # ë ˆì´ë¸” ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(\"User-Movie Relationship Graph\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ê·¸ë˜í”„ ì €ì¥\n",
    "    plt.savefig('user_movie_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "import re\n",
    "\n",
    "# ê³¼ê±° ê¸°ë¡ì—ì„œ ì˜í™” ID ì¶”ì¶œ\n",
    "def extract_previous_movie_ids(purchase_history):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œ (ìˆ«ìì™€ ê´„í˜¸ ì „ê¹Œì§€ë§Œ ê°€ì ¸ì˜´)\n",
    "    previous_movies = re.findall(r'(\\d+)(?=\\s\\()', ' '.join(purchase_history))\n",
    "    return set(previous_movies)\n",
    "\n",
    "# ë™ì‹œ ì‹œì²­ ì˜í™”ì—ì„œ ì´ì „ ê¸°ë¡ì„ ì œì™¸í•˜ëŠ” í•¨ìˆ˜\n",
    "def filter_movies_by_history(top_movies, previous_movie_ids):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ í›„ ì œì™¸\n",
    "    filtered_movies = [(movie, count) for movie, count in top_movies if movie.split()[1] not in previous_movie_ids]\n",
    "    return filtered_movies\n",
    "def get_top_10_common_movies(G):\n",
    "    # ì˜í™” ë…¸ë“œë§Œ í•„í„°ë§\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ì˜í™”ë³„ ì—°ê²°ëœ ì‚¬ìš©ì ìˆ˜ ê³„ì‚°\n",
    "    movie_view_counts = {movie: len(list(G.neighbors(movie))) for movie in movies}\n",
    "\n",
    "    # ì‚¬ìš©ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ 10ê°œ ì¶”ì¶œ\n",
    "    top_10_movies = sorted(movie_view_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    # ê²°ê³¼ ë°˜í™˜\n",
    "    return top_10_movies\n",
    "\n",
    "# ì‚¬ìš©ìë³„ ì˜í™” ID ê°€ì ¸ì˜¤ê¸°\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# Top-10 ì˜í™” ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# ì´ì „ ê¸°ë¡ì— í¬í•¨ë˜ì§€ ì•Šì€ ì˜í™”ë§Œ í•„í„°ë§\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3']\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[1].movie_explain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤í—˜ ì‹œì‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raptorë¡œ 1ì°¨ ê²€ìƒ‰ ìœ ì € í•„í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# íŠ¹ì • ì‚¬ìš©ì ê²€ìƒ‰ ìˆ˜í–‰ (Self-exclusion ë°©ì‹ ì ìš©)\n",
    "target_user_id = \"2\"\n",
    "target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "    target_user_id, target_user_text, threshold=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì‹  êµ¬ë§¤ ê¸°ë¡ì„ ê°€ì ¸ì˜´\n",
    "purchase_history=data.iloc[1]['movie_explain']\n",
    "# ì •ë‹µ\n",
    "answer= df_test.iloc[1].movie_explain\n",
    "# ğŸ”¹ ìµœì‹  êµ¬ë§¤ ê¸°ë¡ (Query)\n",
    "query = \" \".join(purchase_history[-1:])  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "# ğŸ”¹ similar_usersì˜ íƒ€ì… í™•ì¸ í›„ ë³€í™˜\n",
    "similar_users_str = set(map(str, similar_users))  # ë¬¸ìì—´ ë³€í™˜\n",
    "# ğŸ”¹ FAISSì—ì„œ `similar_users`ë§Œ ê²€ìƒ‰í•˜ë„ë¡ í•„í„° ì¶”ê°€\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 500,\n",
    "    }\n",
    ")\n",
    "# ğŸ”¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "records = retriever.get_relevant_documents(query)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "intersection = similar_users_str.intersection(set(record_user_ids))\n",
    "\n",
    "# ğŸ”¹ 'similar_users'ì— ì†í•˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§\n",
    "filtered_records = [\n",
    "    record for record in records \n",
    "    if str(record.metadata['UserId']) in similar_users_str\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 788: watched by 3 users\n",
      "Movie 1573: watched by 3 users\n",
      "Movie 2094: watched by 2 users\n",
      "Movie 1320: watched by 2 users\n",
      "Movie 329: watched by 2 users\n",
      "Movie 1917: watched by 2 users\n",
      "Movie 1779: watched by 2 users\n",
      "Movie 173: watched by 2 users\n",
      "Movie 3354: watched by 2 users\n",
      "Movie 880: watched by 2 users\n",
      "Movie 748: watched by 2 users\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ğŸ”¹ intersection ìœ ì €ë“¤ì˜ ì²­í¬ ì•ë’¤ ì²­í¬ ì¶”ì¶œ\n",
    "context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n",
    "\n",
    "flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "# ì‚¬ìš©ìë³„ ì˜í™” ID ê°€ì ¸ì˜¤ê¸°\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# Top-10 ì˜í™” ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# ì´ì „ ê¸°ë¡ì— í¬í•¨ë˜ì§€ ì•Šì€ ì˜í™”ë§Œ í•„í„°ë§\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ì •ë‹µì„ ë§ì·„ìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ì²œ ì˜í™” ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜í™” IDë§Œ ì¶”ì¶œ\n",
    "filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "# ì •ë‹µ ê²€ì¦\n",
    "if answer_id in filtered_movie_ids:\n",
    "    print(\"ğŸ¯ ì •ë‹µì„ ë§ì·„ìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âŒ ì •ë‹µì„ ë§ì¶”ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¹ˆ DataFrame ìƒì„±\n",
    "results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "# ì „ì²´ ìœ ì € ë°˜ë³µ ë° ê¸°ë¡\n",
    "for idx in range(100):\n",
    "    target_user_id = str(idx + 1)  # ìœ ì € IDëŠ” 1ë¶€í„° ì‹œì‘\n",
    "    target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "    best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "        target_user_id, target_user_text, threshold=0.005\n",
    "    )\n",
    "\n",
    "    # ìµœì‹  êµ¬ë§¤ ê¸°ë¡\n",
    "    purchase_history = data.iloc[idx]['movie_explain']\n",
    "    query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "    # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    records = retriever.get_relevant_documents(query)\n",
    "    record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "    intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "\n",
    "    # 'similar_users'ì— ì†í•˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§\n",
    "    filtered_records = [\n",
    "        record for record in records\n",
    "        if str(record.metadata['UserId']) in intersection\n",
    "    ]\n",
    "\n",
    "    # Intersection ìœ ì €ë“¤ì˜ ì²­í¬ ì•ë’¤ ì²­í¬ ì¶”ì¶œ\n",
    "    context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n",
    "    flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "    # ì‚¬ìš©ìë³„ ì˜í™” ID ê°€ì ¸ì˜¤ê¸°\n",
    "    user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "    # ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "    G = create_user_movie_graph(user_movies)\n",
    "\n",
    "    # Top-10 ì˜í™” ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "    # ì´ì „ ê¸°ë¡ì— í¬í•¨ë˜ì§€ ì•Šì€ ì˜í™”ë§Œ í•„í„°ë§\n",
    "    filtered_movies = filter_movies_by_history(top_movies, extract_previous_movie_ids(purchase_history))\n",
    "\n",
    "    # ğŸ”¹ ì •ë‹µ ì¶”ì¶œ (ì˜í™” IDë§Œ)\n",
    "    answer = df_test.iloc[idx].movie_explain\n",
    "    answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "\n",
    "    # ğŸ”¹ ì¶”ì²œ ì˜í™” ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜í™” IDë§Œ ì¶”ì¶œ\n",
    "    filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "\n",
    "    # ğŸ”¹ Hit ì—¬ë¶€ í™•ì¸\n",
    "    hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "    # ê²°ê³¼ DataFrameì— ê¸°ë¡\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        \"UserId\": [target_user_id],\n",
    "        \"Hit\": [hit],\n",
    "        \"Answer\": [answer_id],\n",
    "        \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "    })], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('rapor_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hit\n",
       "0    0.924669\n",
       "1    0.075331\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.Hit.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## íœ´ë¦¬ìŠ¤í‹± - ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ë“¤ \n",
    "- raptor íŒŒë¼ë¯¸í„° : threshold, \n",
    "- faiss ê²€ìƒ‰ ë²”ìœ„ k \n",
    "- window í™•ì¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/1168534785.py:99: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_results = pd.concat([all_results, results], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# (1) ì‹¤í—˜í•  íŒŒë¼ë¯¸í„° ëª©ë¡ ì •ì˜\n",
    "threshold_values = [0.001, 0.005, 0.01]\n",
    "faiss_k_values = [300, 500, 700]\n",
    "window_values = [1, 2, 3]\n",
    "\n",
    "# ì „ì²´ ê²°ê³¼ë¥¼ ì €ì¥í•  DataFrame (íŒŒë¼ë¯¸í„° + ê²°ê³¼)\n",
    "all_results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\",\n",
    "                                    \"threshold\", \"faiss_k\", \"window\"])\n",
    "\n",
    "# (2) íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ë°˜ë³µ ì‹¤í—˜\n",
    "for threshold in threshold_values:\n",
    "    for k_val in faiss_k_values:\n",
    "        for window_size in window_values:\n",
    "            # ê° íŒŒë¼ë¯¸í„° ì„¸íŒ…ë§ˆë‹¤ 100ëª…(ì˜ˆì‹œ) ìœ ì € ì‹¤í–‰\n",
    "            results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "            for idx in range(100):\n",
    "                target_user_id = str(idx + 1)  # ìœ ì € IDëŠ” 1ë¶€í„° ì‹œì‘\n",
    "                target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "                # (2-1) Raptor Tree ê²€ìƒ‰ (threshold ì„¤ì •)\n",
    "                best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "                    target_user_id, \n",
    "                    target_user_text, \n",
    "                    threshold=threshold   # ğŸ”¥ threshold ë³€ê²½\n",
    "                )\n",
    "\n",
    "                # ìµœì‹  êµ¬ë§¤ ê¸°ë¡\n",
    "                purchase_history = data.iloc[idx]['movie_explain']\n",
    "                query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "                # (2-2) FAISS ê²€ìƒ‰ (k ê°’ ë³€ê²½)\n",
    "                # retriever ì„¤ì • ì‹œ search_kwargsì— k_val ëŒ€ì…\n",
    "                retriever_k = vectorstore.as_retriever(\n",
    "                    search_kwargs={\"k\": k_val}  # ğŸ”¥ kê°’ ë³€ê²½\n",
    "                )\n",
    "                records = retriever_k.get_relevant_documents(query)\n",
    "\n",
    "                record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "                intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "\n",
    "                # 'similar_users'ì— ì†í•˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§\n",
    "                filtered_records = [\n",
    "                    record for record in records\n",
    "                    if str(record.metadata['UserId']) in intersection\n",
    "                ]\n",
    "\n",
    "                # (2-3) ë©”íƒ€ ì²­í¬ ì•ë’¤ ì²­í¬ ì¶”ì¶œ (window_sizeë¡œ ë³€ê²½)\n",
    "                context_results = get_documents_with_context(\n",
    "                    vectorstore, \n",
    "                    filtered_records, \n",
    "                    context_window=window_size  # ğŸ”¥ window ë³€ê²½\n",
    "                )\n",
    "\n",
    "                flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "                # ì‚¬ìš©ìë³„ ì˜í™” ID ê°€ì ¸ì˜¤ê¸°\n",
    "                user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "                # ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "                G = create_user_movie_graph(user_movies)\n",
    "\n",
    "                # Top-10 ì˜í™” ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "                top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "                # ì´ì „ ê¸°ë¡ì— í¬í•¨ë˜ì§€ ì•Šì€ ì˜í™”ë§Œ í•„í„°ë§\n",
    "                filtered_movies = filter_movies_by_history(\n",
    "                    top_movies, \n",
    "                    extract_previous_movie_ids(purchase_history)\n",
    "                )\n",
    "\n",
    "                # ğŸ”¹ ì •ë‹µ ì¶”ì¶œ (ì˜í™” IDë§Œ)\n",
    "                answer = df_test.iloc[idx].movie_explain\n",
    "                answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "\n",
    "                # ğŸ”¹ ì¶”ì²œ ì˜í™” ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜í™” IDë§Œ ì¶”ì¶œ\n",
    "                filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "\n",
    "                # ğŸ”¹ Hit ì—¬ë¶€ í™•ì¸\n",
    "                hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "                # ê²°ê³¼ DataFrameì— ê¸°ë¡ (ì´ë²ˆ íŒŒë¼ë¯¸í„° ì„¸íŒ…ìš©)\n",
    "                results = pd.concat([results, pd.DataFrame({\n",
    "                    \"UserId\": [target_user_id],\n",
    "                    \"Hit\": [hit],\n",
    "                    \"Answer\": [answer_id],\n",
    "                    \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "                })], ignore_index=True)\n",
    "\n",
    "            # (3) ì´ë²ˆ íŒŒë¼ë¯¸í„° ì„¸íŒ…ì˜ ê²°ê³¼ë¥¼ all_resultsì— ë³‘í•©\n",
    "            # íŒŒë¼ë¯¸í„° ì»¬ëŸ¼ ì¶”ê°€\n",
    "            results[\"threshold\"] = threshold\n",
    "            results[\"faiss_k\"] = k_val\n",
    "            results[\"window\"] = window_size\n",
    "\n",
    "            all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "\n",
    "# (4) all_resultsë¥¼ ë¶„ì„í•˜ì—¬ íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "# ì˜ˆ: íŒŒë¼ë¯¸í„°ë³„ Hit Rate ê³„ì‚°\n",
    "grouped = all_results.groupby([\"threshold\", \"faiss_k\", \"window\"])\n",
    "performance = grouped[\"Hit\"].mean().reset_index(name=\"HitRate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.to_csv('perform_100_thresholds_faiss_k_window_size.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
