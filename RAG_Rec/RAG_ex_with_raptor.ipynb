{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# os.chdir('/Users/mac/AIworkspace/LLMWORKSPACE/RAG_Rec')\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raptor with rec\n",
    "1. target user의 최신 상호작용 목록을 input query로 넣음 -> 몇개 넣을지는 바꿔가며\n",
    "2. vectordb는 모든 사용자를 연결시켜둔 db에서 계층적 클러스터링을 시도함 \n",
    "3. 갑작스럽게 평점 및 영화의 장르가 변화하는 지점에서 끊어 각 상호작용의 패턴을 header로 만듦 \n",
    "4. 가장 하단 계층의 클러스터와 비교하여 유사도가 임계값을 넘는 지점 or 유사도의 변화율이 급격히 변화하는 시점에서 검색 중단 \n",
    "5. 해당되는 검색 시점의 클러스터에 있는 사용자들만을 top-100 뽑아냄 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화기록 데이터 \n",
    "import pandas as pd\n",
    "file_path = \"data/movies.dat\"\n",
    "df2 = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df2.columns = [\"MovieID\", \"Title\", \"Genres\"]\n",
    "\n",
    "file_path = \"data/ratings.dat\"\n",
    "df = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df.columns = [\"UserId\", \"MovieID\", \"Ratings\",\"timestamp\"]\n",
    "new_df=df.merge(df2, on='MovieID')\n",
    "df_sorted = new_df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. 사용자별 interaction 리스트 생성 ---\n",
    "df_sorted['interaction'] = df_sorted.apply(\n",
    "    lambda row: f\"{row['Genres']} (Rating: {row['Ratings']})\", axis=1\n",
    ")\n",
    "\n",
    "# 사용자별 interaction 연결 (리스트 형태)\n",
    "user_interactions = df_sorted.groupby('UserId')['interaction'].apply(list).reset_index()\n",
    "\n",
    "# 컬럼명 변경\n",
    "user_interactions.columns = ['UserId', 'interaction_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 2: 장르별 통계 저장\n",
    "def extract_genre_stats(interaction_list):\n",
    "    genre_ratings = defaultdict(list)\n",
    "\n",
    "    # 리스트 형태의 데이터 파싱\n",
    "    for entry in interaction_list:\n",
    "        genres, rating = entry.split(' (Rating: ')\n",
    "        rating = float(rating.replace(')', ''))\n",
    "        \n",
    "        for genre in genres.split('|'):\n",
    "            genre_ratings[genre].append(rating)\n",
    "\n",
    "    # 통계 생성\n",
    "    avg = {g : round(pd.Series(r).mean(),2) for g,r in genre_ratings.items()}\n",
    "    # avg_variance = {g: (round(pd.Series(r).mean(), 2), \n",
    "    #                     round(pd.Series(r).var(), 2) if len(r) > 1 else 0) \n",
    "    #                 for g, r in genre_ratings.items()}\n",
    "    \n",
    "    count = {g: len(r) for g, r in genre_ratings.items()}\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    avg_var_text = ', '.join([f\"{g}({v})\" for g, v in avg.items()])\n",
    "    count_text = ', '.join([f\"{g}({v})\" for g, v in count.items()])\n",
    "    \n",
    "    return avg_var_text, count_text\n",
    "\n",
    "# Step 3: 데이터프레임 생성\n",
    "user_interactions[['avg_var_text', 'count_text']] = user_interactions['interaction_list'].apply(\n",
    "    lambda x: pd.Series(extract_genre_stats(x))\n",
    ")\n",
    "\n",
    "# 최종 결과\n",
    "user_interactions_final = user_interactions[['UserId', 'avg_var_text', 'count_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama 활용하여 각 유저별로 header 생성 \n",
    "- 장르별 평균 평점 \n",
    "- 장르별 분산 \n",
    "- 시청횟수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/miniconda3/envs/rag_rec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "MY_HF_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>avg_var_text</th>\n",
       "      <th>count_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...</td>\n",
       "      <td>Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Action(3.5), Adventure(3.74), Romance(3.71), S...</td>\n",
       "      <td>Action(56), Adventure(19), Romance(24), Sci-Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...</td>\n",
       "      <td>Drama(8), Thriller(5), Comedy(30), Action(23),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId                                       avg_var_text  \\\n",
       "0       1  Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...   \n",
       "1       2  Action(3.5), Adventure(3.74), Romance(3.71), S...   \n",
       "2       3  Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...   \n",
       "\n",
       "                                          count_text  \n",
       "0  Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...  \n",
       "1  Action(56), Adventure(19), Romance(24), Sci-Fi...  \n",
       "2  Drama(8), Thriller(5), Comedy(30), Action(23),...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=user_interactions_final.iloc[:3]\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 프롬프트\n",
    "- 평점 + 시청횟수 => 둘다 높고 많으면 선호하는 장르 / 평점만 높고 횟수는 적으면 잠재 장르 \n",
    "\n",
    "##### 추가할 수 있는 부분 \n",
    "1. 다양한 장르를 골고르 => 시청횟수가 다 비슷하면 \n",
    "2. 비선호하는 장르 고르기 \n",
    "3. 분산을 통한 일관성 or 호불호 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "# Function to Generate Concise Contextual Chunk Header Using CoT\n",
    "def generate_chunk_header(avg_rating_text, count_text):\n",
    "    system_message = (\n",
    "        \"You are an expert in analyzing movie viewing behavior. \"\n",
    "        \"Your task is to analyze the user's movie preferences and generate a concise summary of their overall viewing pattern. \"\n",
    "        \"Do NOT include specific rating numbers, viewing counts, or detailed explanations. \"\n",
    "        \"Only provide a brief and meaningful summary of their primary preferences and emerging interests. \"\n",
    "        \"Your final response should be 1-2 clear and natural sentences.\"\n",
    "    )\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "### INPUT DATA ###\n",
    "1. **Genre Statistics (Average Rating):** {avg_rating_text}\n",
    "2. **Genre Viewing Frequency (Count):** {count_text}\n",
    "\n",
    "### OUTPUT FORMAT EXAMPLE ###\n",
    "\"This user enjoys emotional and family-oriented genres while occasionally exploring niche genres like Sci-Fi.\"\n",
    "\n",
    "### RESPONSE ###\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id = terminators[0],\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    return outputs[0]['generated_text'][2]['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= generate_chunk_header(k['avg_var_text'].iloc[0] , k['count_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions_final['chunk_header'] = user_interactions_final.apply(lambda row: generate_chunk_header(row['avg_var_text'], row['count_text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('header_vanila.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성된 header를 활용하여 raptor에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/miniconda3/envs/rag_rec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리들 임포트 미리 준비\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# 만들어진 청크를 임베딩하는 클래스\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device= device)\n",
    "    \n",
    "    def embed_texts(self, texts: list[str]) -> np.ndarray:\n",
    "        # 텍스트 리스트를 임베딩 벡터로 변환하여 반환\n",
    "        return self.model.encode(texts, convert_to_numpy=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raptor 기반 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMClusterer:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_predict(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:\n",
    "        # 🔥 매 레벨마다 n_clusters를 전달하도록 변경\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=self.random_state)\n",
    "        return gmm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaptorTree:\n",
    "    def __init__(self, embedding_generator, clusterer, min_clusters=2, max_level=5, top_level_clusters=100):\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.clusterer = clusterer\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_level = max_level\n",
    "        self.top_level_clusters = top_level_clusters\n",
    "        self.tree = {}\n",
    "        self.user_id_to_text = {}\n",
    "\n",
    "    def build_tree(self, texts: list[str], user_ids: list[str]):\n",
    "        self.user_id_to_text = dict(zip(user_ids, texts))\n",
    "        current_texts = texts\n",
    "        current_user_ids = user_ids\n",
    "        current_level = 0\n",
    "        parent_ids = None\n",
    "\n",
    "        while len(current_texts) > 1 and current_level < self.max_level:\n",
    "            embeddings = self.embedding_generator.embed_texts(current_texts)\n",
    "\n",
    "            # 🔥 클러스터 개수 설정 수정 (빈 클러스터 방지)\n",
    "            n_clusters = max(self.min_clusters, min(len(current_texts) // 2, self.top_level_clusters // (current_level + 1)))\n",
    "\n",
    "            cluster_labels = self.clusterer.fit_predict(embeddings, n_clusters=n_clusters)\n",
    "\n",
    "            cluster_metadata = []\n",
    "            next_level_texts = []\n",
    "            next_level_user_ids = []\n",
    "\n",
    "            for cluster_id in np.unique(cluster_labels):\n",
    "                cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "                cluster_texts = [current_texts[i] for i in cluster_indices]\n",
    "\n",
    "                # 🔥 빈 클러스터 제거\n",
    "                if len(cluster_texts) == 0:\n",
    "                    continue\n",
    "\n",
    "                # 🔥 첫 번째 레벨에서는 user_ids 그대로 사용\n",
    "                if current_level == 0:\n",
    "                    cluster_user_ids = [current_user_ids[i] for i in cluster_indices]\n",
    "                else:\n",
    "                    # 상위 레벨에서는 이전 클러스터 ID 기준으로 `user_ids` 추가\n",
    "                    cluster_user_ids = []\n",
    "                    for idx in cluster_indices:\n",
    "                        child_cluster_id = current_user_ids[idx]\n",
    "                        for child_meta in self.tree[current_level - 1]:\n",
    "                            if child_meta[\"cluster_id\"] == child_cluster_id:\n",
    "                                cluster_user_ids.extend(child_meta[\"user_ids\"])\n",
    "\n",
    "                # 🔥 대표 텍스트 (가장 긴 텍스트 선정)\n",
    "                representative_text = max(cluster_texts, key=len)\n",
    "\n",
    "                cluster_embeddings = embeddings[cluster_indices]\n",
    "                mean_embedding = cluster_embeddings.mean(axis=0)\n",
    "\n",
    "                # 🔥 올바른 parent_id 설정 (부모 클러스터 1개만 추가)\n",
    "                metadata = {\n",
    "                    \"cluster_id\": f\"level_{current_level}_cluster_{cluster_id}\",\n",
    "                    \"level\": current_level,\n",
    "                    \"user_ids\": cluster_user_ids,  # ✅ 첫 번째 레벨에서도 user_ids 추가\n",
    "                    \"embedding\": mean_embedding,\n",
    "                    \"parent_id\": parent_ids if parent_ids else [],\n",
    "                    \"child_ids\": None\n",
    "                }\n",
    "\n",
    "                cluster_metadata.append(metadata)\n",
    "                next_level_texts.append(representative_text)\n",
    "                next_level_user_ids.append(metadata[\"cluster_id\"])\n",
    "\n",
    "            # 클러스터 메타데이터 추가\n",
    "            self.tree[current_level] = cluster_metadata\n",
    "            current_texts = next_level_texts\n",
    "            current_user_ids = next_level_user_ids\n",
    "            parent_ids = current_user_ids\n",
    "            current_level += 1\n",
    "\n",
    "        return self.tree\n",
    "\n",
    "    def search_user_cluster(self, target_user_id: str, target_user_text: str, threshold=0.01):\n",
    "        query_embedding = self.embedding_generator.embed_texts([target_user_text])[0]\n",
    "        current_level = max(self.tree.keys())\n",
    "        previous_similarity = None\n",
    "        best_cluster = None\n",
    "\n",
    "        while current_level >= 0:\n",
    "            clusters = self.tree[current_level]\n",
    "            clusters_filtered = []\n",
    "            clusters_filtered_embeddings = []\n",
    "\n",
    "            for cluster in clusters:\n",
    "                cluster_user_ids = cluster[\"user_ids\"]\n",
    "\n",
    "                # 🔥 클러스터 ID가 아닌 실제 유저 ID만 필터링\n",
    "                texts_to_embed = [\n",
    "                    self.user_id_to_text[uid] for uid in cluster_user_ids\n",
    "                    if uid != target_user_id and 'cluster' not in uid\n",
    "                ]\n",
    "                if not texts_to_embed:\n",
    "                    continue\n",
    "\n",
    "                embeddings_cluster = self.embedding_generator.embed_texts(texts_to_embed)\n",
    "                mean_embedding = embeddings_cluster.mean(axis=0)\n",
    "\n",
    "                clusters_filtered.append(cluster)\n",
    "                clusters_filtered_embeddings.append(mean_embedding)\n",
    "\n",
    "            if not clusters_filtered:\n",
    "                break\n",
    "\n",
    "            similarities = cosine_similarity([query_embedding], clusters_filtered_embeddings).flatten()\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_cluster = clusters_filtered[best_idx]\n",
    "            current_similarity = similarities[best_idx]\n",
    "\n",
    "            if previous_similarity and abs(previous_similarity - current_similarity) / previous_similarity > threshold:\n",
    "                break\n",
    "\n",
    "            previous_similarity = current_similarity\n",
    "            current_level -= 1\n",
    "\n",
    "        best_cluster_users_excluded = [uid for uid in best_cluster[\"user_ids\"] if uid != target_user_id]\n",
    "        return best_cluster[\"cluster_id\"], best_cluster_users_excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클러스터 수를 늘리며 변화율의 증폭기간에서 멈춤 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유저 2가 가장 유사한 클러스터: level_2_cluster_16\n",
      "해당 클러스터에 속한 유사 유저 목록 (본인 제외): ['46', '127', '304', '1089', '1302', '1424', '1610', '1726', '1927', '2101', '2126', '2146', '2263', '2297', '2363', '2441', '2514', '3144', '3352', '3460', '3567', '3636', '3673', '4000', '4031', '4109', '4160', '4587', '4690', '4874', '4892', '5040', '5069', '5469', '5703', '5895', '6034', '7', '277', '296', '431', '908', '1030', '1131', '1200', '1398', '1520', '1649', '1866', '2466', '2598', '2663', '3048', '3068', '3307', '3337', '3459', '3461', '3487', '3662', '3818', '3978', '4093', '4183', '4417', '4489', '4499', '4626', '5003', '5029', '5095', '5854', '5870', '5871', '5884', '5912', '5947', '6020', '185', '279', '422', '542', '633', '677', '700', '917', '1260', '1269', '1533', '1548', '1567', '1682', '2062', '2114', '2163', '2211', '2357', '2369', '2388', '2479', '2625', '2642', '2767', '2967', '3027', '3162', '3187', '3455', '3490', '3616', '3733', '3822', '3828', '3855', '4200', '4207', '4531', '4579', '5309', '5389', '5737', '5749', '5859']\n"
     ]
    }
   ],
   "source": [
    "# 초기화 예시\n",
    "embedding_gen = EmbeddingGenerator()\n",
    "clusterer = GMMClusterer()\n",
    "\n",
    "# 초기 RAPTOR 트리 생성 (한 번만 수행)\n",
    "# 클래스 초기화 부분\n",
    "raptor_tree = RaptorTree(\n",
    "    embedding_gen, \n",
    "    clusterer,\n",
    "    min_clusters=2,\n",
    "    max_level=5,          # 더 높이거나 낮춰서 조정\n",
    "    top_level_clusters=100 # 최상위 클러스터 수를 늘리거나 줄여본다\n",
    ")\n",
    "tree_structure = raptor_tree.build_tree(df.chunk_header.tolist(), df.UserId.astype(str).tolist())\n",
    "\n",
    "# 특정 사용자 검색 수행 (Self-exclusion 방식 적용)\n",
    "target_user_id = \"2\"\n",
    "target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "    target_user_id, target_user_text, threshold=0.005\n",
    ")\n",
    "\n",
    "print(f\"유저 {target_user_id}가 가장 유사한 클러스터: {best_cluster_id}\")\n",
    "print(f\"해당 클러스터에 속한 유사 유저 목록 (본인 제외): {similar_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차 필터링 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_users) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. meta chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최소 조건 없는 청킹 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/321042087.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"vectorstore_index_ratings_min5_no\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # 역직렬화 허용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "file_path='data/train_movie.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# 'movie_explain' 열을 리스트로 변환\n",
    "data['movie_explain'] = data['movie_explain'].apply(ast.literal_eval)\n",
    "file_path_test='data/test_movie.csv'\n",
    "# 최신 구매 기록을 가져옴\n",
    "purchase_history=data.iloc[1]['movie_explain']\n",
    "# 정답 데이터셋 가져옴\n",
    "df_test=pd.read_csv(file_path_test)\n",
    "# 정답\n",
    "df_test.iloc[1].movie_explain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/3104281221.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  records = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "# 🔹 최신 구매 기록 (Query)\n",
    "query = \" \".join(purchase_history[-1:])  # 리스트를 문자열로 변환\n",
    "\n",
    "# 🔹 similar_users의 타입 확인 후 변환\n",
    "similar_users_str = set(map(str, similar_users))  # 문자열 변환\n",
    "\n",
    "# 🔹 FAISS에서 `similar_users`만 검색하도록 필터 추가\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 500,\n",
    "        # \"filter\": user_filter  # 🔥 수정된 부분\n",
    "    }\n",
    ")\n",
    "\n",
    "# 🔹 검색 수행\n",
    "records = retriever.get_relevant_documents(query)\n",
    "\n",
    "# 🔹 결과 확인\n",
    "record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "intersection = similar_users_str.intersection(set(record_user_ids))\n",
    "\n",
    "# 🔹 'similar_users'에 속하는 유저만 필터링\n",
    "filtered_records = [\n",
    "    record for record in records \n",
    "    if str(record.metadata['UserId']) in similar_users_str\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_documents_with_context(\n",
    "    vectorstore: FAISS,\n",
    "    filtered_records: List[Document],\n",
    "    context_window: int = 1\n",
    ") -> List[List[Document]]:\n",
    "    \"\"\"\n",
    "    intersection 유저들의 검색 결과와 각 결과의 앞뒤 문서들을 함께 반환합니다.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: FAISS 벡터스토어 인스턴스\n",
    "        filtered_records: 검색된 유저들의 기록\n",
    "        context_window: 앞뒤로 가져올 문서 수 (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        List[List[Document]]: 각 검색 결과에 대해 [이전 문서들, 현재 문서, 다음 문서들]을 포함하는 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "    # 🔹 모든 문서와 메타데이터를 딕셔너리로 구성\n",
    "    all_docs = {}\n",
    "    for doc_id, doc in enumerate(vectorstore.docstore._dict.values()):\n",
    "        user_id = str(doc.metadata['UserId'])\n",
    "        chunk_idx = doc.metadata['chunk_index']\n",
    "        \n",
    "        if user_id not in all_docs:\n",
    "            all_docs[user_id] = {}\n",
    "        all_docs[user_id][chunk_idx] = doc\n",
    "    \n",
    "    # 🔹 Context 추가\n",
    "    context_results = []\n",
    "    \n",
    "    for doc in filtered_records:\n",
    "        current_user_id = str(doc.metadata['UserId'])\n",
    "        current_chunk_index = doc.metadata['chunk_index']\n",
    "        \n",
    "        context_docs = []\n",
    "        \n",
    "        # 🔹 이전 문서들 추가\n",
    "        for i in range(current_chunk_index - context_window, current_chunk_index):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        # 🔹 현재 문서 추가\n",
    "        context_docs.append(doc)\n",
    "        \n",
    "        # 🔹 다음 문서들 추가\n",
    "        for i in range(current_chunk_index + 1, current_chunk_index + context_window + 1):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        context_results.append(context_docs)\n",
    "    \n",
    "    return context_results\n",
    "\n",
    "# 🔹 intersection 유저들의 청크 앞뒤 청크 추출\n",
    "context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"3020 (Action|Drama) ratings: 5 292 (Action|Drama|Thriller) ratings: 3 1769 (Action|Thriller) ratings: 2 736 (Action|Adventure|Romance|Thriller) ratings: 2 1667 (Action|Drama) ratings: 4 434 (Action|Adventure|Crime) ratings: 3 511 (Action|Drama) ratings: 3 2126 (Action|Crime|Mystery|Thriller) ratings: 3 2094 (Action|Adventure|Sci-Fi) ratings: 3\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 1 420 (Action|Comedy) ratings: 3 208 (Action|Adventure) ratings: 1 2409 (Action|Drama) ratings: 5 2410 (Action|Drama) ratings: 2\\n2411 (Action|Drama) ratings: 5 2412 (Action|Drama) ratings: 2 1954 (Action|Drama) ratings: 5 2657 (Comedy|Horror|Musical|Sci-Fi) ratings: 2 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 4\\n1320 (Action|Horror|Sci-Fi|Thriller) ratings: 5 674 (Adventure|Sci-Fi) ratings: 4 2311 (Mystery|Sci-Fi) ratings: 4 1690 (Action|Horror|Sci-Fi) ratings: 4 2641 (Action|Adventure|Sci-Fi) ratings: 2 3300 (Action|Sci-Fi) ratings: 3 3033 (Comedy|Sci-Fi) ratings: 2 2094 (Action|Adventure|Sci-Fi) ratings: 3 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 2 329 (Action|Adventure|Sci-Fi) ratings: 1\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 196 (Horror|Sci-Fi) ratings: 3 1779 (Adventure|Sci-Fi|Thriller) ratings: 3 173 (Action|Adventure|Sci-Fi) ratings: 2 3354 (Sci-Fi) ratings: 3 2851 (Adventure|Sci-Fi|Thriller) ratings: 3 880 (Sci-Fi|Thriller) ratings: 3\\n160 (Action|Adventure|Mystery|Sci-Fi) ratings: 1 2448 (Horror|Sci-Fi) ratings: 2 1862 (Horror|Sci-Fi) ratings: 3 1129 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1374 (Action|Adventure|Sci-Fi) ratings: 2 2287 (Sci-Fi|Thriller|War) ratings: 3 1356 (Action|Adventure|Sci-Fi) ratings: 2 748 (Action|Sci-Fi|Thriller) ratings: 3 1240 (Action|Sci-Fi|Thriller) ratings: 4 1200 (Action|Sci-Fi|Thriller|War) ratings: 5\\n1603 (Sci-Fi|Thriller) ratings: 3 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 1 1320 (Action|Horror|Sci-Fi|Thriller) ratings: 4 1371 (Action|Adventure|Sci-Fi) ratings: 3 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 1\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 3354 (Sci-Fi) ratings: 3 748 (Action|Sci-Fi|Thriller) ratings: 4 780 (Action|Sci-Fi|War) ratings: 4 2808 (Action|Sci-Fi) ratings: 2\\n3024 (Horror|Sci-Fi) ratings: 4 1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1779 (Adventure|Sci-Fi|Thriller) ratings: 3 172 (Action|Sci-Fi|Thriller) ratings: 1 849 (Action|Adventure|Sci-Fi|Thriller) ratings: 2 880 (Sci-Fi|Thriller) ratings: 2 173 (Action|Adventure|Sci-Fi) ratings: 1\\n1376 (Action|Adventure|Sci-Fi) ratings: 2 3740 (Action|Comedy) ratings: 3 555 (Action|Crime|Romance) ratings: 4 1676 (Action|Adventure|Sci-Fi|War) ratings: 4 480 (Action|Adventure|Sci-Fi) ratings: 2 1527 (Action|Sci-Fi) ratings: 1\\n1573 (Action|Sci-Fi|Thriller) ratings: 5 1580 (Action|Adventure|Comedy|Sci-Fi) ratings: 1 1587 (Action|Adventure) ratings: 3 163 (Action|Romance|Thriller) ratings: 5 2989 (Action) ratings: 2\\n2058 (Action|Thriller) ratings: 4 3444 (Action) ratings: 1 2826 (Action|Horror|Thriller) ratings: 3 1377 (Action|Adventure|Comedy|Crime) ratings: 2 3519 (Action|War) ratings: 3\\n1270 (Comedy|Sci-Fi) ratings: 3 1097 (Children's|Drama|Fantasy|Sci-Fi) ratings: 2 480 (Action|Adventure|Sci-Fi) ratings: 3 920 (Drama|Romance|War) ratings: 5 1589 (Crime|Drama|Mystery) ratings: 4\\n1573 (Action|Sci-Fi|Thriller) ratings: 4 2916 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1580 (Action|Adventure|Comedy|Sci-Fi) ratings: 4 1748 (Film-Noir|Sci-Fi|Thriller) ratings: 4 329 (Action|Adventure|Sci-Fi) ratings: 2 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 2 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 2 2827 (Sci-Fi|Thriller) ratings: 3\\n1219 (Horror|Thriller) ratings: 5 593 (Drama|Thriller) ratings: 5 1617 (Crime|Film-Noir|Mystery|Thriller) ratings: 3 608 (Crime|Drama|Thriller) ratings: 5 50 (Crime|Thriller) ratings: 5 3147 (Drama|Thriller) ratings: 4 2762 (Thriller) ratings: 5\\n628 (Drama|Thriller) ratings: 5 802 (Drama|Romance) ratings: 5 1748 (Film-Noir|Sci-Fi|Thriller) ratings: 3 2353 (Action|Thriller) ratings: 5 1061 (Crime|Drama) ratings: 4 3252 (Drama) ratings: 4 21 (Action|Comedy|Drama) ratings: 4 454 (Drama|Thriller) ratings: 4 3108 (Comedy|Drama|Romance) ratings: 3\\n1573 (Action|Sci-Fi|Thriller) ratings: 5 1754 (Action|Mystery|Thriller) ratings: 5 832 (Drama|Thriller) ratings: 4 3793 (Action|Sci-Fi) ratings: 3 3578 (Action|Drama) ratings: 4\\n1997 (Horror) ratings: 3 3617 (Comedy) ratings: 3 3863 (Sci-Fi|Thriller) ratings: 4 3755 (Action|Adventure|Thriller) ratings: 3 3826 (Horror|Sci-Fi|Thriller) ratings: 3 3717 (Action|Crime) ratings: 3\\n2394 (Animation|Musical) ratings: 3 69 (Comedy) ratings: 4 509 (Drama|Romance) ratings: 2 999 (Crime) ratings: 2 543 (Comedy|Romance|Thriller) ratings: 1\\n1527 (Action|Sci-Fi) ratings: 4 353 (Action|Romance|Thriller) ratings: 2 10 (Action|Adventure|Thriller) ratings: 2 2676 (Drama|Thriller) ratings: 4 991 (Drama|War) ratings: 4 1589 (Crime|Drama|Mystery) ratings: 3\\n371 (Comedy|Drama) ratings: 5 2439 (Drama) ratings: 3 1799 (Crime|Drama) ratings: 4 2427 (Action|Drama|War) ratings: 5 802 (Drama|Romance) ratings: 1\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "record_summary = \"\\n\".join([doc.page_content for doc in flattened_results])\n",
    "record_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 788: watched by 3 users\n",
      "Movie 1573: watched by 3 users\n",
      "Movie 2094: watched by 2 users\n",
      "Movie 1320: watched by 2 users\n",
      "Movie 329: watched by 2 users\n",
      "Movie 1917: watched by 2 users\n",
      "Movie 1779: watched by 2 users\n",
      "Movie 173: watched by 2 users\n",
      "Movie 3354: watched by 2 users\n",
      "Movie 880: watched by 2 users\n",
      "Movie 748: watched by 2 users\n",
      "Movie 1580: watched by 2 users\n",
      "Movie 1589: watched by 2 users\n",
      "Movie 1748: watched by 2 users\n",
      "Movie 802: watched by 2 users\n",
      "Movie 3020: watched by 1 users\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_user_movies(data):\n",
    "    user_movies = defaultdict(list)\n",
    "\n",
    "    for doc in data:\n",
    "        user_id = doc.metadata['UserId']\n",
    "        page_content = doc.page_content\n",
    "\n",
    "        # 정규식을 사용하여 영화 ID 추출 (ratings 이전 내용만)\n",
    "        movie_ids = re.findall(r'(\\d+)(?= \\()', page_content)\n",
    "\n",
    "        # 사용자별로 영화 ID 추가\n",
    "        user_movies[user_id].extend(movie_ids)\n",
    "\n",
    "    return user_movies\n",
    "\n",
    "def create_user_movie_graph(user_movies):\n",
    "    # 그래프 생성\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 사용자와 영화 노드 추가 및 엣지 생성\n",
    "    for user_id, movies in user_movies.items():\n",
    "        user_node = f\"User {user_id}\"\n",
    "        G.add_node(user_node, type='user')\n",
    "\n",
    "        for movie_id in movies:\n",
    "            movie_node = f\"Movie {movie_id}\"\n",
    "            G.add_node(movie_node, type='movie')\n",
    "            G.add_edge(user_node, movie_node)\n",
    "\n",
    "    return G\n",
    "\n",
    "def visualize_graph(G):\n",
    "    # 그래프 시각화\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # 사용자와 영화 노드 분리\n",
    "    users = [node for node in G.nodes() if G.nodes[node]['type'] == 'user']\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # 레이아웃 설정\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "    # 노드 그리기\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=users, node_color='lightblue', node_size=300, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=movies, node_color='lightgreen', node_size=200, alpha=0.8)\n",
    "\n",
    "    # 엣지 그리기\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "    # 레이블 그리기\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(\"User-Movie Relationship Graph\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 그래프 저장\n",
    "    plt.savefig('user_movie_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "import re\n",
    "\n",
    "# 과거 기록에서 영화 ID 추출\n",
    "def extract_previous_movie_ids(purchase_history):\n",
    "    # 영화 ID만 추출 (숫자와 괄호 전까지만 가져옴)\n",
    "    previous_movies = re.findall(r'(\\d+)(?=\\s\\()', ' '.join(purchase_history))\n",
    "    return set(previous_movies)\n",
    "\n",
    "# 동시 시청 영화에서 이전 기록을 제외하는 함수\n",
    "def filter_movies_by_history(top_movies, previous_movie_ids):\n",
    "    # 영화 ID만 추출하여 비교 후 제외\n",
    "    filtered_movies = [(movie, count) for movie, count in top_movies if movie.split()[1] not in previous_movie_ids]\n",
    "    return filtered_movies\n",
    "def get_top_10_common_movies(G):\n",
    "    # 영화 노드만 필터링\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # 영화별 연결된 사용자 수 계산\n",
    "    movie_view_counts = {movie: len(list(G.neighbors(movie))) for movie in movies}\n",
    "\n",
    "    # 사용자 수 기준으로 내림차순 정렬하여 상위 10개 추출\n",
    "    top_10_movies = sorted(movie_view_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    # 결과 반환\n",
    "    return top_10_movies\n",
    "\n",
    "# 사용자별 영화 ID 가져오기\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# 사용자-영화 그래프 생성\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# Top-10 영화 리스트 가져오기\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# 이전 기록에 포함되지 않은 영화만 필터링\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# 결과 출력\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3']\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[1].movie_explain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raptor로 1차 검색 유저 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 특정 사용자 검색 수행 (Self-exclusion 방식 적용)\n",
    "target_user_id = \"2\"\n",
    "target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "    target_user_id, target_user_text, threshold=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최신 구매 기록을 가져옴\n",
    "purchase_history=data.iloc[1]['movie_explain']\n",
    "# 정답\n",
    "answer= df_test.iloc[1].movie_explain\n",
    "# 🔹 최신 구매 기록 (Query)\n",
    "query = \" \".join(purchase_history[-1:])  # 리스트를 문자열로 변환\n",
    "# 🔹 similar_users의 타입 확인 후 변환\n",
    "similar_users_str = set(map(str, similar_users))  # 문자열 변환\n",
    "# 🔹 FAISS에서 `similar_users`만 검색하도록 필터 추가\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 500,\n",
    "    }\n",
    ")\n",
    "# 🔹 검색 수행\n",
    "records = retriever.get_relevant_documents(query)\n",
    "\n",
    "# 🔹 결과 확인\n",
    "record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "intersection = similar_users_str.intersection(set(record_user_ids))\n",
    "\n",
    "# 🔹 'similar_users'에 속하는 유저만 필터링\n",
    "filtered_records = [\n",
    "    record for record in records \n",
    "    if str(record.metadata['UserId']) in similar_users_str\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 788: watched by 3 users\n",
      "Movie 1573: watched by 3 users\n",
      "Movie 2094: watched by 2 users\n",
      "Movie 1320: watched by 2 users\n",
      "Movie 329: watched by 2 users\n",
      "Movie 1917: watched by 2 users\n",
      "Movie 1779: watched by 2 users\n",
      "Movie 173: watched by 2 users\n",
      "Movie 3354: watched by 2 users\n",
      "Movie 880: watched by 2 users\n",
      "Movie 748: watched by 2 users\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 🔹 intersection 유저들의 청크 앞뒤 청크 추출\n",
    "context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n",
    "\n",
    "flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "# 사용자별 영화 ID 가져오기\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# 사용자-영화 그래프 생성\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# Top-10 영화 리스트 가져오기\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# 이전 기록에 포함되지 않은 영화만 필터링\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# 결과 출력\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 정답을 맞췄습니다!\n"
     ]
    }
   ],
   "source": [
    "# 추천 영화 리스트에서 영화 ID만 추출\n",
    "filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "# 정답 검증\n",
    "if answer_id in filtered_movie_ids:\n",
    "    print(\"🎯 정답을 맞췄습니다!\")\n",
    "else:\n",
    "    print(\"❌ 정답을 맞추지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 결과 저장을 위한 빈 DataFrame 생성\n",
    "results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "# 전체 유저 반복 및 기록\n",
    "for idx in range(100):\n",
    "    target_user_id = str(idx + 1)  # 유저 ID는 1부터 시작\n",
    "    target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "    best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "        target_user_id, target_user_text, threshold=0.005\n",
    "    )\n",
    "\n",
    "    # 최신 구매 기록\n",
    "    purchase_history = data.iloc[idx]['movie_explain']\n",
    "    query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "    # 검색 수행\n",
    "    records = retriever.get_relevant_documents(query)\n",
    "    record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "    intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "\n",
    "    # 'similar_users'에 속하는 유저만 필터링\n",
    "    filtered_records = [\n",
    "        record for record in records\n",
    "        if str(record.metadata['UserId']) in intersection\n",
    "    ]\n",
    "\n",
    "    # Intersection 유저들의 청크 앞뒤 청크 추출\n",
    "    context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n",
    "    flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "    # 사용자별 영화 ID 가져오기\n",
    "    user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "    # 사용자-영화 그래프 생성\n",
    "    G = create_user_movie_graph(user_movies)\n",
    "\n",
    "    # Top-10 영화 리스트 가져오기\n",
    "    top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "    # 이전 기록에 포함되지 않은 영화만 필터링\n",
    "    filtered_movies = filter_movies_by_history(top_movies, extract_previous_movie_ids(purchase_history))\n",
    "\n",
    "    # 🔹 정답 추출 (영화 ID만)\n",
    "    answer = df_test.iloc[idx].movie_explain\n",
    "    answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "\n",
    "    # 🔹 추천 영화 리스트에서 영화 ID만 추출\n",
    "    filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "\n",
    "    # 🔹 Hit 여부 확인\n",
    "    hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "    # 결과 DataFrame에 기록\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        \"UserId\": [target_user_id],\n",
    "        \"Hit\": [hit],\n",
    "        \"Answer\": [answer_id],\n",
    "        \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "    })], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('rapor_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hit\n",
       "0    0.924669\n",
       "1    0.075331\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.Hit.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 휴리스틱 - 수정할 수 있는 변수들 \n",
    "- raptor 파라미터 : threshold, \n",
    "- faiss 검색 범위 k \n",
    "- window 확장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/1168534785.py:99: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_results = pd.concat([all_results, results], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# (1) 실험할 파라미터 목록 정의\n",
    "threshold_values = [0.001, 0.005, 0.01]\n",
    "faiss_k_values = [300, 500, 700]\n",
    "window_values = [1, 2, 3]\n",
    "\n",
    "# 전체 결과를 저장할 DataFrame (파라미터 + 결과)\n",
    "all_results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\",\n",
    "                                    \"threshold\", \"faiss_k\", \"window\"])\n",
    "\n",
    "# (2) 파라미터 조합으로 반복 실험\n",
    "for threshold in threshold_values:\n",
    "    for k_val in faiss_k_values:\n",
    "        for window_size in window_values:\n",
    "            # 각 파라미터 세팅마다 100명(예시) 유저 실행\n",
    "            results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "            for idx in range(100):\n",
    "                target_user_id = str(idx + 1)  # 유저 ID는 1부터 시작\n",
    "                target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "                # (2-1) Raptor Tree 검색 (threshold 설정)\n",
    "                best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "                    target_user_id, \n",
    "                    target_user_text, \n",
    "                    threshold=threshold   # 🔥 threshold 변경\n",
    "                )\n",
    "\n",
    "                # 최신 구매 기록\n",
    "                purchase_history = data.iloc[idx]['movie_explain']\n",
    "                query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "                # (2-2) FAISS 검색 (k 값 변경)\n",
    "                # retriever 설정 시 search_kwargs에 k_val 대입\n",
    "                retriever_k = vectorstore.as_retriever(\n",
    "                    search_kwargs={\"k\": k_val}  # 🔥 k값 변경\n",
    "                )\n",
    "                records = retriever_k.get_relevant_documents(query)\n",
    "\n",
    "                record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "                intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "\n",
    "                # 'similar_users'에 속하는 유저만 필터링\n",
    "                filtered_records = [\n",
    "                    record for record in records\n",
    "                    if str(record.metadata['UserId']) in intersection\n",
    "                ]\n",
    "\n",
    "                # (2-3) 메타 청크 앞뒤 청크 추출 (window_size로 변경)\n",
    "                context_results = get_documents_with_context(\n",
    "                    vectorstore, \n",
    "                    filtered_records, \n",
    "                    context_window=window_size  # 🔥 window 변경\n",
    "                )\n",
    "\n",
    "                flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "                # 사용자별 영화 ID 가져오기\n",
    "                user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "                # 사용자-영화 그래프 생성\n",
    "                G = create_user_movie_graph(user_movies)\n",
    "\n",
    "                # Top-10 영화 리스트 가져오기\n",
    "                top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "                # 이전 기록에 포함되지 않은 영화만 필터링\n",
    "                filtered_movies = filter_movies_by_history(\n",
    "                    top_movies, \n",
    "                    extract_previous_movie_ids(purchase_history)\n",
    "                )\n",
    "\n",
    "                # 🔹 정답 추출 (영화 ID만)\n",
    "                answer = df_test.iloc[idx].movie_explain\n",
    "                answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "\n",
    "                # 🔹 추천 영화 리스트에서 영화 ID만 추출\n",
    "                filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "\n",
    "                # 🔹 Hit 여부 확인\n",
    "                hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "                # 결과 DataFrame에 기록 (이번 파라미터 세팅용)\n",
    "                results = pd.concat([results, pd.DataFrame({\n",
    "                    \"UserId\": [target_user_id],\n",
    "                    \"Hit\": [hit],\n",
    "                    \"Answer\": [answer_id],\n",
    "                    \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "                })], ignore_index=True)\n",
    "\n",
    "            # (3) 이번 파라미터 세팅의 결과를 all_results에 병합\n",
    "            # 파라미터 컬럼 추가\n",
    "            results[\"threshold\"] = threshold\n",
    "            results[\"faiss_k\"] = k_val\n",
    "            results[\"window\"] = window_size\n",
    "\n",
    "            all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "\n",
    "# (4) all_results를 분석하여 파라미터별 성능 비교\n",
    "# 예: 파라미터별 Hit Rate 계산\n",
    "grouped = all_results.groupby([\"threshold\", \"faiss_k\", \"window\"])\n",
    "performance = grouped[\"Hit\"].mean().reset_index(name=\"HitRate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.to_csv('perform_100_thresholds_faiss_k_window_size.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
