{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "# os.chdir('/Users/mac/AIworkspace/LLMWORKSPACE/RAG_Rec')\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Raptor 기반 추천 시스템 실험 흐름 정리\n",
    "\n",
    "이 실험은 영화 평점 데이터를 기반으로 사용자의 시청 패턴을 요약하고, 비슷한 사용자 집단을 찾아 추천하는 **2단계 필터링 기반 추천 시스템**입니다.  \n",
    "사용자의 **장르별 시청 성향 요약 → 유사한 사용자 탐색 → 최신 시청 기록 기반 필터링 → 공통된 영화 추천** 흐름으로 구성됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "##  1차 검색: 사용자 성향 기반 계층적 클러스터링 (Raptor Tree)\n",
    "\n",
    "1. **사용자 영화 시청 패턴 요약 (Header 생성)**  \n",
    "   - 6040명 사용자의 시청 데이터를 기반으로, 각 사용자의 **장르별 평균 평점**과 **시청 횟수**를 계산합니다.\n",
    "   - 이 정보를 LLM에 입력하여 해당 사용자의 시청 성향을 자연어 형태로 요약한 **chunk header**를 생성합니다.\n",
    "     - 예: `\"Drama(4.5), Comedy(3.9), Action(4.2)...\"`\n",
    "\n",
    "2. **유사 사용자 클러스터링 (Raptor Tree 구축)**  \n",
    "   - 사용자 header를 임베딩한 후, 계층적 클러스터링을 통해 유사한 사용자들을 **트리 구조로 그룹화**합니다.\n",
    "   - 클러스터링은 다단계로 수행되며, 상위 레벨에서는 넓은 범위의 유사성을, 하위 레벨에서는 더 정밀한 유사성을 반영합니다.\n",
    "\n",
    "3. **유사도 기반 검색 (트리 탐색)**  \n",
    "   - 특정 사용자의 header 임베딩과 트리 내 클러스터 임베딩 간의 **cosine similarity**를 계산합니다.\n",
    "   - 상위 레벨부터 탐색을 시작하여, **유사도 변화율이 특정 임계값(r)** 을 초과할 때까지 하위 레벨로 내려가며 탐색을 진행합니다.\n",
    "   - 탐색이 멈춘 시점의 클러스터 내 유사 사용자들이 **1차 필터링 결과**입니다.\n",
    "\n",
    "---\n",
    "\n",
    "##  2차 검색: 최신 시청 기록 기반 필터링 및 추천\n",
    "\n",
    "1. **최근 시청 기록 기반 유사 사용자 필터링**  \n",
    "   - 1차 필터링된 유사 사용자 중, **target 사용자가 가장 최근에 본 영화**를 **같이 본 사용자**만 추려냅니다.\n",
    "\n",
    "2. **해당 청크 및 주변 context 추출 (메타청킹 기반)**  \n",
    "   - 필터링된 유저들이 본 청크 중, 최근 본 영화가 포함된 **chunk**와 **앞뒤 window size n 청크**를 함께 추출합니다.\n",
    "   - 이 때 청크는 사용자의 **장르/평점 변화 시점**을 기준으로 분할된 **메타청크**입니다.\n",
    "\n",
    "3. **공통 영화 기반 추천**  \n",
    "   - 최종 필터링된 청크들로부터 **사용자-영화 그래프**를 생성합니다.\n",
    "   - 그래프를 통해 **여러 사용자가 공통으로 본 영화 Top-10**을 추출합니다.\n",
    "   - 이 중에서 **target 사용자가 이미 본 영화는 제외**하고, 나머지를 최종 추천 리스트로 구성합니다.\n",
    "\n",
    "---\n",
    "\n",
    "##  핵심 요약\n",
    "\n",
    "- **1차 필터링**: 사용자 header를 기반으로 의미 기반 유사 사용자 탐색\n",
    "- **2차 필터링**: 최근 시청 영화 기록으로 더욱 정밀하게 필터링\n",
    "- **최종 추천**: 유사 사용자들이 공통적으로 본 영화를 기반으로 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 유저별로 시청기록 정리 후 header 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "영화 평점 데이터를 기반으로 사용자별 영화 시청 이력을 전처리하는 코드입니다.\n",
    "각 사용자가 어떤 장르의 영화를 어떤 평점으로 시청했는지를 시간 순서대로 정리하고,\n",
    "이를 바탕으로 사용자별 interaction 리스트를 생성합니다.\n",
    "\"\"\"\n",
    "\n",
    "# 1. 영화 정보 불러오기\n",
    "file_path = \"data/movies.dat\"\n",
    "df2 = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None, encoding=\"latin1\")\n",
    "df2.columns = [\"MovieID\", \"Title\", \"Genres\"]  # 컬럼 이름 지정\n",
    "\n",
    "# 2. 영화 평점 데이터 불러오기\n",
    "file_path = \"data/ratings.dat\"\n",
    "df = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None, encoding=\"latin1\")\n",
    "df.columns = [\"UserId\", \"MovieID\", \"Ratings\", \"timestamp\"]  # 컬럼 이름 지정\n",
    "\n",
    "# 3. 영화 정보와 평점 데이터를 MovieID를 기준으로 병합\n",
    "new_df = df.merge(df2, on='MovieID')\n",
    "\n",
    "# 4. 사용자 ID와 타임스탬프를 기준으로 정렬\n",
    "df_sorted = new_df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# 5. 각 영화 시청 기록을 \"장르 (Rating: 평점)\" 형식으로 변환\n",
    "df_sorted['interaction'] = df_sorted.apply(\n",
    "    lambda row: f\"{row['Genres']} (Rating: {row['Ratings']})\", axis=1\n",
    ")\n",
    "\n",
    "# 6. 사용자별로 interaction을 리스트 형태로 집계\n",
    "user_interactions = df_sorted.groupby('UserId')['interaction'].apply(list).reset_index()\n",
    "\n",
    "# 7. 컬럼명 명확하게 변경\n",
    "user_interactions.columns = ['UserId', 'interaction_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "사용자별 시청 이력에서 장르별 평점 평균 및 시청 횟수를 추출하여 통계 정보를 생성합니다.\n",
    "이 정보를 기반으로 LLM에게 입력할 수 있는 요약 형태의 텍스트를 구성합니다.\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 2: 사용자별 장르 통계 정보 추출 함수 정의\n",
    "def extract_genre_stats(interaction_list):\n",
    "    \"\"\"\n",
    "    interaction_list: 사용자가 시청한 영화의 \"장르 (Rating: 평점)\" 형태의 리스트\n",
    "\n",
    "    반환값:\n",
    "        - avg_var_text: 장르별 평균 평점을 나타내는 문자열 (예: \"Drama(4.2), Action(3.8)\")\n",
    "        - count_text: 장르별 시청 횟수를 나타내는 문자열 (예: \"Drama(12), Action(8)\")\n",
    "    \"\"\"\n",
    "    genre_ratings = defaultdict(list)\n",
    "\n",
    "    # 각 interaction 항목을 순회하며 장르별로 평점을 분리 저장\n",
    "    for entry in interaction_list:\n",
    "        genres, rating = entry.split(' (Rating: ')\n",
    "        rating = float(rating.replace(')', ''))  # 괄호 제거 후 float 변환\n",
    "\n",
    "        for genre in genres.split('|'):\n",
    "            genre_ratings[genre].append(rating)\n",
    "\n",
    "    # 장르별 평균 평점 계산 (소수점 2자리 반올림)\n",
    "    avg = {g: round(pd.Series(r).mean(), 2) for g, r in genre_ratings.items()}\n",
    "\n",
    "    # 장르별 시청 횟수 계산\n",
    "    count = {g: len(r) for g, r in genre_ratings.items()}\n",
    "\n",
    "    # 평균 평점 텍스트 생성\n",
    "    avg_var_text = ', '.join([f\"{g}({v})\" for g, v in avg.items()])\n",
    "\n",
    "    # 시청 횟수 텍스트 생성\n",
    "    count_text = ', '.join([f\"{g}({v})\" for g, v in count.items()])\n",
    "\n",
    "    return avg_var_text, count_text\n",
    "\n",
    "# Step 3: 사용자별 통계 정보를 DataFrame에 적용\n",
    "user_interactions[['avg_var_text', 'count_text']] = user_interactions['interaction_list'].apply(\n",
    "    lambda x: pd.Series(extract_genre_stats(x))\n",
    ")\n",
    "\n",
    "# 최종적으로 사용할 열만 선택하여 정리\n",
    "user_interactions_final = user_interactions[['UserId', 'avg_var_text', 'count_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>avg_var_text</th>\n",
       "      <th>count_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...</td>\n",
       "      <td>Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Action(3.5), Adventure(3.74), Romance(3.71), S...</td>\n",
       "      <td>Action(56), Adventure(19), Romance(24), Sci-Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...</td>\n",
       "      <td>Drama(8), Thriller(5), Comedy(30), Action(23),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Action(4.16), Adventure(3.83), Romance(4.0), S...</td>\n",
       "      <td>Action(19), Adventure(6), Romance(2), Sci-Fi(9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comedy(3.41), Horror(2.8), Drama(3.1), Thrille...</td>\n",
       "      <td>Comedy(56), Horror(10), Drama(104), Thriller(3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6036</td>\n",
       "      <td>Drama(3.51), Romance(3.35), Horror(2.99), Sci-...</td>\n",
       "      <td>Drama(372), Romance(122), Horror(74), Sci-Fi(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6037</td>\n",
       "      <td>Action(3.64), Sci-Fi(3.69), Western(3.75), Dra...</td>\n",
       "      <td>Action(28), Sci-Fi(39), Western(4), Drama(98),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6038</td>\n",
       "      <td>Drama(3.89), Romance(4.17), War(4.0), Children...</td>\n",
       "      <td>Drama(9), Romance(6), War(4), Children's(1), C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6039</td>\n",
       "      <td>Drama(4.0), Thriller(4.14), Romance(3.8), War(...</td>\n",
       "      <td>Drama(28), Thriller(14), Romance(30), War(9), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6040</td>\n",
       "      <td>Action(2.98), Crime(3.92), Drama(3.82), Thrill...</td>\n",
       "      <td>Action(42), Crime(25), Drama(185), Thriller(41...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserId                                       avg_var_text  \\\n",
       "0          1  Drama(4.43), Comedy(4.14), Sci-Fi(4.33), Roman...   \n",
       "1          2  Action(3.5), Adventure(3.74), Romance(3.71), S...   \n",
       "2          3  Drama(4.0), Thriller(3.8), Comedy(3.77), Actio...   \n",
       "3          4  Action(4.16), Adventure(3.83), Romance(4.0), S...   \n",
       "4          5  Comedy(3.41), Horror(2.8), Drama(3.1), Thrille...   \n",
       "...      ...                                                ...   \n",
       "6035    6036  Drama(3.51), Romance(3.35), Horror(2.99), Sci-...   \n",
       "6036    6037  Action(3.64), Sci-Fi(3.69), Western(3.75), Dra...   \n",
       "6037    6038  Drama(3.89), Romance(4.17), War(4.0), Children...   \n",
       "6038    6039  Drama(4.0), Thriller(4.14), Romance(3.8), War(...   \n",
       "6039    6040  Action(2.98), Crime(3.92), Drama(3.82), Thrill...   \n",
       "\n",
       "                                             count_text  \n",
       "0     Drama(21), Comedy(14), Sci-Fi(3), Romance(6), ...  \n",
       "1     Action(56), Adventure(19), Romance(24), Sci-Fi...  \n",
       "2     Drama(8), Thriller(5), Comedy(30), Action(23),...  \n",
       "3     Action(19), Adventure(6), Romance(2), Sci-Fi(9...  \n",
       "4     Comedy(56), Horror(10), Drama(104), Thriller(3...  \n",
       "...                                                 ...  \n",
       "6035  Drama(372), Romance(122), Horror(74), Sci-Fi(1...  \n",
       "6036  Action(28), Sci-Fi(39), Western(4), Drama(98),...  \n",
       "6037  Drama(9), Romance(6), War(4), Children's(1), C...  \n",
       "6038  Drama(28), Thriller(14), Romance(30), War(9), ...  \n",
       "6039  Action(42), Crime(25), Drama(185), Thriller(41...  \n",
       "\n",
       "[6040 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_interactions_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama 활용하여 각 유저별로 header 생성 \n",
    "- 장르별 평균 평점 \n",
    "- 장르별 분산 \n",
    "- 시청횟수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_HF_TOKEN = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    "    \n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 프롬프트\n",
    "- 평점 + 시청횟수 => 둘다 높고 많으면 선호하는 장르 / 평점만 높고 횟수는 적으면 잠재 장르 \n",
    "\n",
    "##### 추가할 수 있는 부분 \n",
    "1. 다양한 장르를 골고르 => 시청횟수가 다 비슷하면 \n",
    "2. 비선호하는 장르 고르기 \n",
    "3. 분산을 통한 일관성 or 호불호 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to Generate Concise Contextual Chunk Header Using CoT\n",
    "def generate_chunk_header(avg_rating_text, count_text):\n",
    "    system_message = (\n",
    "        \"You are an expert in analyzing movie viewing behavior. \"\n",
    "        \"Your task is to analyze the user's movie preferences and generate a concise summary of their overall viewing pattern. \"\n",
    "        \"Do NOT include specific rating numbers, viewing counts, or detailed explanations. \"\n",
    "        \"Only provide a brief and meaningful summary of their primary preferences and emerging interests. \"\n",
    "        \"Your final response should be 1-2 clear and natural sentences.\"\n",
    "    )\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "### INPUT DATA ###\n",
    "1. **Genre Statistics (Average Rating):** {avg_rating_text}\n",
    "2. **Genre Viewing Frequency (Count):** {count_text}\n",
    "\n",
    "### OUTPUT FORMAT EXAMPLE ###\n",
    "\"This user enjoys emotional and family-oriented genres while occasionally exploring niche genres like Sci-Fi.\"\n",
    "\n",
    "### RESPONSE ###\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id = terminators[0],\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    return outputs[0]['generated_text'][2]['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions_final['chunk_header'] = user_interactions_final.apply(lambda row: generate_chunk_header(row['avg_var_text'], row['count_text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저별 헤더 데이터 불러오기\n",
    "df= pd.read_csv('header_vanila.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.생성된 header를 활용하여 raptor에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/miniconda3/envs/rag_rec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# 만들어진 청크를 임베딩하는 클래스\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device= device)\n",
    "    \n",
    "    def embed_texts(self, texts: list[str]) -> np.ndarray:\n",
    "        # 텍스트 리스트를 임베딩 벡터로 변환하여 반환\n",
    "        return self.model.encode(texts, convert_to_numpy=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raptor 기반 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "\n",
    "class GMMClusterer:\n",
    "    \"\"\"\n",
    "    Gaussian Mixture Model(GMM)을 이용해 클러스터링을 수행하는 클래스입니다.\n",
    "    주로 텍스트 임베딩 벡터에 대해 사용되며, Raptor Tree 등에서 계층적 클러스터링에 활용됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_predict(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        입력된 임베딩 벡터에 대해 GMM 기반 클러스터링을 수행하고, 각 데이터 포인트에 대해 클러스터 레이블을 반환합니다.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        embeddings : np.ndarray\n",
    "            클러스터링에 사용할 텍스트 임베딩 벡터 (2차원 배열).\n",
    "        n_clusters : int\n",
    "            클러스터의 개수.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        np.ndarray\n",
    "            각 데이터 포인트가 속한 클러스터의 레이블 배열.\n",
    "        \"\"\"\n",
    "        # 각 계층마다 지정된 클러스터 수로 GMM 클러스터링 수행\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=self.random_state)\n",
    "        return gmm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class RaptorTree:\n",
    "    \"\"\"\n",
    "    사용자의 요약된 텍스트(예: 장르 기반 헤더)를 계층적으로 클러스터링하여\n",
    "    유사한 사용자를 탐색하기 위한 트리 구조를 생성하는 클래스.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    embedding_generator : object\n",
    "        텍스트를 임베딩으로 변환해주는 클래스 (예: SentenceTransformer).\n",
    "    clusterer : object\n",
    "        클러스터링 알고리즘 (예: GMMClusterer).\n",
    "    min_clusters : int\n",
    "        각 레벨에서 최소 클러스터 개수.\n",
    "    max_level : int\n",
    "        트리의 최대 깊이.\n",
    "    top_level_clusters : int\n",
    "        최상위에서 시작할 클러스터 개수.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator, clusterer, min_clusters=2, max_level=5, top_level_clusters=100):\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.clusterer = clusterer\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_level = max_level\n",
    "        self.top_level_clusters = top_level_clusters\n",
    "        self.tree = {}  # 레벨별 클러스터 메타데이터 저장\n",
    "        self.user_id_to_text = {}  # 유저 ID와 해당 텍스트 매핑\n",
    "\n",
    "    def build_tree(self, texts: list[str], user_ids: list[str]):\n",
    "        \"\"\"\n",
    "        계층적 클러스터링을 수행하여 Raptor Tree를 구성합니다.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        texts : list[str]\n",
    "            사용자별 헤더 텍스트 리스트\n",
    "        user_ids : list[str]\n",
    "            텍스트와 매핑되는 사용자 ID 리스트\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            트리 구조 (레벨별 클러스터 정보 딕셔너리)\n",
    "        \"\"\"\n",
    "        self.user_id_to_text = dict(zip(user_ids, texts))\n",
    "        current_texts = texts\n",
    "        current_user_ids = user_ids\n",
    "        current_level = 0\n",
    "        parent_ids = None\n",
    "\n",
    "        while len(current_texts) > 1 and current_level < self.max_level:\n",
    "            embeddings = self.embedding_generator.embed_texts(current_texts)\n",
    "\n",
    "            # 현재 단계에서 적절한 클러스터 수 계산\n",
    "            n_clusters = max(\n",
    "                self.min_clusters,\n",
    "                min(len(current_texts) // 2, self.top_level_clusters // (current_level + 1))\n",
    "            )\n",
    "\n",
    "            cluster_labels = self.clusterer.fit_predict(embeddings, n_clusters=n_clusters)\n",
    "\n",
    "            cluster_metadata = []\n",
    "            next_level_texts = []\n",
    "            next_level_user_ids = []\n",
    "\n",
    "            for cluster_id in np.unique(cluster_labels):\n",
    "                cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "                cluster_texts = [current_texts[i] for i in cluster_indices]\n",
    "\n",
    "                if len(cluster_texts) == 0:\n",
    "                    continue\n",
    "\n",
    "                if current_level == 0:\n",
    "                    cluster_user_ids = [current_user_ids[i] for i in cluster_indices]\n",
    "                else:\n",
    "                    cluster_user_ids = []\n",
    "                    for idx in cluster_indices:\n",
    "                        child_cluster_id = current_user_ids[idx]\n",
    "                        for child_meta in self.tree[current_level - 1]:\n",
    "                            if child_meta[\"cluster_id\"] == child_cluster_id:\n",
    "                                cluster_user_ids.extend(child_meta[\"user_ids\"])\n",
    "\n",
    "                representative_text = max(cluster_texts, key=len)\n",
    "                cluster_embeddings = embeddings[cluster_indices]\n",
    "                mean_embedding = cluster_embeddings.mean(axis=0)\n",
    "\n",
    "                metadata = {\n",
    "                    \"cluster_id\": f\"level_{current_level}_cluster_{cluster_id}\",\n",
    "                    \"level\": current_level,\n",
    "                    \"user_ids\": cluster_user_ids,\n",
    "                    \"embedding\": mean_embedding,\n",
    "                    \"parent_id\": parent_ids if parent_ids else [],\n",
    "                    \"child_ids\": None\n",
    "                }\n",
    "\n",
    "                cluster_metadata.append(metadata)\n",
    "                next_level_texts.append(representative_text)\n",
    "                next_level_user_ids.append(metadata[\"cluster_id\"])\n",
    "\n",
    "            self.tree[current_level] = cluster_metadata\n",
    "            current_texts = next_level_texts\n",
    "            current_user_ids = next_level_user_ids\n",
    "            parent_ids = current_user_ids\n",
    "            current_level += 1\n",
    "\n",
    "        return self.tree\n",
    "\n",
    "    def search_user_cluster(self, target_user_id: str, target_user_text: str, threshold=0.01):\n",
    "        \"\"\"\n",
    "        계층적으로 클러스터를 탐색하여, 특정 유저와 가장 유사한 클러스터를 찾습니다.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        target_user_id : str\n",
    "            검색 대상 사용자 ID\n",
    "        target_user_text : str\n",
    "            해당 사용자의 헤더 텍스트\n",
    "        threshold : float\n",
    "            유사도 변화율 임계값 (값이 클수록 상위에서 탐색 중단)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple[str, list[str]]\n",
    "            가장 유사한 클러스터 ID, 해당 클러스터 내 유사 사용자 목록 (본인 제외)\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedding_generator.embed_texts([target_user_text])[0]\n",
    "        current_level = max(self.tree.keys())\n",
    "        previous_similarity = None\n",
    "        best_cluster = None\n",
    "\n",
    "        while current_level >= 0:\n",
    "            clusters = self.tree[current_level]\n",
    "            clusters_filtered = []\n",
    "            clusters_filtered_embeddings = []\n",
    "\n",
    "            for cluster in clusters:\n",
    "                cluster_user_ids = cluster[\"user_ids\"]\n",
    "\n",
    "                # 실제 사용자 텍스트만 추출 (클러스터 ID는 제외)\n",
    "                texts_to_embed = [\n",
    "                    self.user_id_to_text[uid]\n",
    "                    for uid in cluster_user_ids\n",
    "                    if uid != target_user_id and 'cluster' not in uid\n",
    "                ]\n",
    "                if not texts_to_embed:\n",
    "                    continue\n",
    "\n",
    "                embeddings_cluster = self.embedding_generator.embed_texts(texts_to_embed)\n",
    "                mean_embedding = embeddings_cluster.mean(axis=0)\n",
    "\n",
    "                clusters_filtered.append(cluster)\n",
    "                clusters_filtered_embeddings.append(mean_embedding)\n",
    "\n",
    "            if not clusters_filtered:\n",
    "                break\n",
    "\n",
    "            similarities = cosine_similarity([query_embedding], clusters_filtered_embeddings).flatten()\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_cluster = clusters_filtered[best_idx]\n",
    "            current_similarity = similarities[best_idx]\n",
    "\n",
    "            # 유사도 변화율이 threshold를 넘으면 멈춤\n",
    "            if previous_similarity and abs(previous_similarity - current_similarity) / previous_similarity > threshold:\n",
    "                break\n",
    "\n",
    "            previous_similarity = current_similarity\n",
    "            current_level -= 1\n",
    "\n",
    "        best_cluster_users_excluded = [\n",
    "            uid for uid in best_cluster[\"user_ids\"] if uid != target_user_id\n",
    "        ]\n",
    "        return best_cluster[\"cluster_id\"], best_cluster_users_excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클러스터 수를 늘리며 변화율의 증폭기간에서 멈춤 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유저 2가 가장 유사한 클러스터: level_2_cluster_16\n",
      "해당 클러스터에 속한 유사 유저 목록 (본인 제외): ['46', '127', '304', '1089', '1302', '1424', '1610', '1726', '1927', '2101', '2126', '2146', '2263', '2297', '2363', '2441', '2514', '3144', '3352', '3460', '3567', '3636', '3673', '4000', '4031', '4109', '4160', '4587', '4690', '4874', '4892', '5040', '5069', '5469', '5703', '5895', '6034', '7', '277', '296', '431', '908', '1030', '1131', '1200', '1398', '1520', '1649', '1866', '2466', '2598', '2663', '3048', '3068', '3307', '3337', '3459', '3461', '3487', '3662', '3818', '3978', '4093', '4183', '4417', '4489', '4499', '4626', '5003', '5029', '5095', '5854', '5870', '5871', '5884', '5912', '5947', '6020', '185', '279', '422', '542', '633', '677', '700', '917', '1260', '1269', '1533', '1548', '1567', '1682', '2062', '2114', '2163', '2211', '2357', '2369', '2388', '2479', '2625', '2642', '2767', '2967', '3027', '3162', '3187', '3455', '3490', '3616', '3733', '3822', '3828', '3855', '4200', '4207', '4531', '4579', '5309', '5389', '5737', '5749', '5859']\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 생성기 및 클러스터링 알고리즘 초기화\n",
    "embedding_gen = EmbeddingGenerator()\n",
    "clusterer = GMMClusterer()\n",
    "\n",
    "# RAPTOR 트리 초기화\n",
    "# min_clusters: 각 계층에서 최소 클러스터 개수\n",
    "# max_level: 트리의 최대 깊이 (계층 수)\n",
    "# top_level_clusters: 최상위 계층에서 생성할 최대 클러스터 수\n",
    "# 계층이 내려갈수록 클러스터 개수는 줄어듦\n",
    "raptor_tree = RaptorTree(\n",
    "    embedding_gen, \n",
    "    clusterer,\n",
    "    min_clusters=2,\n",
    "    max_level=5,\n",
    "    top_level_clusters=100\n",
    ")\n",
    "\n",
    "# 사용자 header 텍스트와 ID 리스트를 기반으로 트리 구조 생성\n",
    "tree_structure = raptor_tree.build_tree(\n",
    "    df.chunk_header.tolist(), \n",
    "    df.UserId.astype(str).tolist()\n",
    ")\n",
    "\n",
    "# 특정 사용자의 클러스터 검색 수행\n",
    "# Self-exclusion 방식: 본인은 유사 사용자 집합에서 제외\n",
    "target_user_id = \"2\"\n",
    "target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "# 트리 탐색을 통해 유사한 클러스터 및 사용자 집합 탐색\n",
    "# threshold: 유사도의 변화율이 해당 값 이상이면 탐색 종료\n",
    "best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "    target_user_id, \n",
    "    target_user_text, \n",
    "    threshold=0.005\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"유저 {target_user_id}가 가장 유사한 클러스터: {best_cluster_id}\")\n",
    "print(f\"해당 클러스터에 속한 유사 유저 목록 (본인 제외): {similar_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차 필터링 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(similar_users) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 2차 필터링 meta chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/321042087.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "메타 청크 해놓은 vectordb 불러오기\n",
    "'''\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"vectorstore_index_ratings_min5_no\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # 역직렬화 허용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차 검색과 2차 필터링(메타청킹) 한 것들의 교집합의 user들을 뽑음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "file_path = 'data/train_movie.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 'movie_explain' 열의 문자열을 실제 리스트 형태로 변환\n",
    "data['movie_explain'] = data['movie_explain'].apply(ast.literal_eval)\n",
    "\n",
    "# 테스트용 정답 데이터셋 로드\n",
    "file_path_test = 'data/test_movie.csv'\n",
    "df_test = pd.read_csv(file_path_test)\n",
    "\n",
    "# (1) 사용자의 최신 영화 구매 기록 가져오기\n",
    "# 예시: 2번째 유저 (index=1)의 마지막 영화 기록\n",
    "purchase_history = data.iloc[1]['movie_explain']\n",
    "\n",
    "# (2) 정답 확인 (정답 평가용)\n",
    "true_answer = df_test.iloc[1].movie_explain\n",
    "\n",
    "# (3) 최신 구매 기록 중 마지막 영화를 쿼리로 설정\n",
    "query = \" \".join(purchase_history[-1:])  # 쿼리로 사용할 텍스트\n",
    "\n",
    "# (4) 유사 사용자 리스트를 문자열 타입으로 변환\n",
    "similar_users_str = set(map(str, similar_users))\n",
    "\n",
    "# (5) FAISS 기반 벡터 검색기 설정\n",
    "# top-k=500개의 유사한 문서를 검색\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 500,\n",
    "        # 필요 시 사용자 ID 기반 필터 추가 가능\n",
    "    }\n",
    ")\n",
    "\n",
    "# (6) 최신 구매 영화(query)를 기반으로 문서 검색 수행\n",
    "records = retriever.get_relevant_documents(query)\n",
    "\n",
    "# (7) 검색된 문서들의 UserId 추출\n",
    "record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "\n",
    "# (8) 검색 결과 중 유사 사용자와의 교집합 계산\n",
    "intersection = similar_users_str.intersection(set(record_user_ids))\n",
    "\n",
    "# (9) 검색된 문서 중 유사 사용자(similar_users)에 해당하는 것만 필터링\n",
    "filtered_records = [\n",
    "    record for record in records \n",
    "    if str(record.metadata['UserId']) in similar_users_str\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_records)\n",
    "# 7명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_documents_with_context(\n",
    "    vectorstore: FAISS,\n",
    "    filtered_records: List[Document],\n",
    "    context_window: int = 1\n",
    ") -> List[List[Document]]:\n",
    "    \"\"\"\n",
    "    특정 유저의 검색된 문서를 기준으로, 해당 문서의 앞뒤 문맥(context)을 함께 수집합니다.\n",
    "\n",
    "    예를 들어, 한 사용자의 청크 인덱스가 3이라면,\n",
    "    context_window=1일 경우 인덱스 2, 3, 4에 해당하는 문서를 모두 포함하여 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        vectorstore (FAISS): 전체 문서가 저장된 FAISS 벡터스토어 인스턴스\n",
    "        filtered_records (List[Document]): 필터링된 검색 결과 (특정 유저들의 핵심 문서들)\n",
    "        context_window (int): 앞뒤로 몇 개의 문서를 함께 가져올지 설정하는 윈도우 크기\n",
    "\n",
    "    Returns:\n",
    "        List[List[Document]]: 각 문서에 대해 해당 문서와 주변 문서들을 포함한 리스트의 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "    # 전체 문서를 UserId, chunk_index 기준으로 저장\n",
    "    all_docs = {}\n",
    "    for doc in vectorstore.docstore._dict.values():\n",
    "        user_id = str(doc.metadata['UserId'])\n",
    "        chunk_idx = doc.metadata['chunk_index']\n",
    "        \n",
    "        if user_id not in all_docs:\n",
    "            all_docs[user_id] = {}\n",
    "        all_docs[user_id][chunk_idx] = doc\n",
    "    \n",
    "    # 각 문서에 대해 앞뒤 문맥 문서들을 포함한 결과 생성\n",
    "    context_results = []\n",
    "    \n",
    "    for doc in filtered_records:\n",
    "        current_user_id = str(doc.metadata['UserId'])\n",
    "        current_chunk_index = doc.metadata['chunk_index']\n",
    "        \n",
    "        context_docs = []\n",
    "\n",
    "        # 이전 문서 추가\n",
    "        for i in range(current_chunk_index - context_window, current_chunk_index):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        # 현재 문서 추가\n",
    "        context_docs.append(doc)\n",
    "        \n",
    "        # 이후 문서 추가\n",
    "        for i in range(current_chunk_index + 1, current_chunk_index + context_window + 1):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        context_results.append(context_docs)\n",
    "    \n",
    "    return context_results\n",
    "\n",
    "# 사용 예시: intersection 유저들의 청크 앞뒤 청크 추출\n",
    "context_results = get_documents_with_context(\n",
    "    vectorstore,\n",
    "    filtered_records,\n",
    "    context_window=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"3020 (Action|Drama) ratings: 5 292 (Action|Drama|Thriller) ratings: 3 1769 (Action|Thriller) ratings: 2 736 (Action|Adventure|Romance|Thriller) ratings: 2 1667 (Action|Drama) ratings: 4 434 (Action|Adventure|Crime) ratings: 3 511 (Action|Drama) ratings: 3 2126 (Action|Crime|Mystery|Thriller) ratings: 3 2094 (Action|Adventure|Sci-Fi) ratings: 3\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 1 420 (Action|Comedy) ratings: 3 208 (Action|Adventure) ratings: 1 2409 (Action|Drama) ratings: 5 2410 (Action|Drama) ratings: 2\\n2411 (Action|Drama) ratings: 5 2412 (Action|Drama) ratings: 2 1954 (Action|Drama) ratings: 5 2657 (Comedy|Horror|Musical|Sci-Fi) ratings: 2 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 4\\n1320 (Action|Horror|Sci-Fi|Thriller) ratings: 5 674 (Adventure|Sci-Fi) ratings: 4 2311 (Mystery|Sci-Fi) ratings: 4 1690 (Action|Horror|Sci-Fi) ratings: 4 2641 (Action|Adventure|Sci-Fi) ratings: 2 3300 (Action|Sci-Fi) ratings: 3 3033 (Comedy|Sci-Fi) ratings: 2 2094 (Action|Adventure|Sci-Fi) ratings: 3 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 2 329 (Action|Adventure|Sci-Fi) ratings: 1\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 196 (Horror|Sci-Fi) ratings: 3 1779 (Adventure|Sci-Fi|Thriller) ratings: 3 173 (Action|Adventure|Sci-Fi) ratings: 2 3354 (Sci-Fi) ratings: 3 2851 (Adventure|Sci-Fi|Thriller) ratings: 3 880 (Sci-Fi|Thriller) ratings: 3\\n160 (Action|Adventure|Mystery|Sci-Fi) ratings: 1 2448 (Horror|Sci-Fi) ratings: 2 1862 (Horror|Sci-Fi) ratings: 3 1129 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1374 (Action|Adventure|Sci-Fi) ratings: 2 2287 (Sci-Fi|Thriller|War) ratings: 3 1356 (Action|Adventure|Sci-Fi) ratings: 2 748 (Action|Sci-Fi|Thriller) ratings: 3 1240 (Action|Sci-Fi|Thriller) ratings: 4 1200 (Action|Sci-Fi|Thriller|War) ratings: 5\\n1603 (Sci-Fi|Thriller) ratings: 3 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 1 1320 (Action|Horror|Sci-Fi|Thriller) ratings: 4 1371 (Action|Adventure|Sci-Fi) ratings: 3 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 1\\n1544 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 3354 (Sci-Fi) ratings: 3 748 (Action|Sci-Fi|Thriller) ratings: 4 780 (Action|Sci-Fi|War) ratings: 4 2808 (Action|Sci-Fi) ratings: 2\\n3024 (Horror|Sci-Fi) ratings: 4 1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1779 (Adventure|Sci-Fi|Thriller) ratings: 3 172 (Action|Sci-Fi|Thriller) ratings: 1 849 (Action|Adventure|Sci-Fi|Thriller) ratings: 2 880 (Sci-Fi|Thriller) ratings: 2 173 (Action|Adventure|Sci-Fi) ratings: 1\\n1376 (Action|Adventure|Sci-Fi) ratings: 2 3740 (Action|Comedy) ratings: 3 555 (Action|Crime|Romance) ratings: 4 1676 (Action|Adventure|Sci-Fi|War) ratings: 4 480 (Action|Adventure|Sci-Fi) ratings: 2 1527 (Action|Sci-Fi) ratings: 1\\n1573 (Action|Sci-Fi|Thriller) ratings: 5 1580 (Action|Adventure|Comedy|Sci-Fi) ratings: 1 1587 (Action|Adventure) ratings: 3 163 (Action|Romance|Thriller) ratings: 5 2989 (Action) ratings: 2\\n2058 (Action|Thriller) ratings: 4 3444 (Action) ratings: 1 2826 (Action|Horror|Thriller) ratings: 3 1377 (Action|Adventure|Comedy|Crime) ratings: 2 3519 (Action|War) ratings: 3\\n1270 (Comedy|Sci-Fi) ratings: 3 1097 (Children's|Drama|Fantasy|Sci-Fi) ratings: 2 480 (Action|Adventure|Sci-Fi) ratings: 3 920 (Drama|Romance|War) ratings: 5 1589 (Crime|Drama|Mystery) ratings: 4\\n1573 (Action|Sci-Fi|Thriller) ratings: 4 2916 (Action|Adventure|Sci-Fi|Thriller) ratings: 3 1580 (Action|Adventure|Comedy|Sci-Fi) ratings: 4 1748 (Film-Noir|Sci-Fi|Thriller) ratings: 4 329 (Action|Adventure|Sci-Fi) ratings: 2 788 (Comedy|Fantasy|Romance|Sci-Fi) ratings: 2 2628 (Action|Adventure|Fantasy|Sci-Fi) ratings: 2 2827 (Sci-Fi|Thriller) ratings: 3\\n1219 (Horror|Thriller) ratings: 5 593 (Drama|Thriller) ratings: 5 1617 (Crime|Film-Noir|Mystery|Thriller) ratings: 3 608 (Crime|Drama|Thriller) ratings: 5 50 (Crime|Thriller) ratings: 5 3147 (Drama|Thriller) ratings: 4 2762 (Thriller) ratings: 5\\n628 (Drama|Thriller) ratings: 5 802 (Drama|Romance) ratings: 5 1748 (Film-Noir|Sci-Fi|Thriller) ratings: 3 2353 (Action|Thriller) ratings: 5 1061 (Crime|Drama) ratings: 4 3252 (Drama) ratings: 4 21 (Action|Comedy|Drama) ratings: 4 454 (Drama|Thriller) ratings: 4 3108 (Comedy|Drama|Romance) ratings: 3\\n1573 (Action|Sci-Fi|Thriller) ratings: 5 1754 (Action|Mystery|Thriller) ratings: 5 832 (Drama|Thriller) ratings: 4 3793 (Action|Sci-Fi) ratings: 3 3578 (Action|Drama) ratings: 4\\n1997 (Horror) ratings: 3 3617 (Comedy) ratings: 3 3863 (Sci-Fi|Thriller) ratings: 4 3755 (Action|Adventure|Thriller) ratings: 3 3826 (Horror|Sci-Fi|Thriller) ratings: 3 3717 (Action|Crime) ratings: 3\\n2394 (Animation|Musical) ratings: 3 69 (Comedy) ratings: 4 509 (Drama|Romance) ratings: 2 999 (Crime) ratings: 2 543 (Comedy|Romance|Thriller) ratings: 1\\n1527 (Action|Sci-Fi) ratings: 4 353 (Action|Romance|Thriller) ratings: 2 10 (Action|Adventure|Thriller) ratings: 2 2676 (Drama|Thriller) ratings: 4 991 (Drama|War) ratings: 4 1589 (Crime|Drama|Mystery) ratings: 3\\n371 (Comedy|Drama) ratings: 5 2439 (Drama) ratings: 3 1799 (Crime|Drama) ratings: 4 2427 (Action|Drama|War) ratings: 5 802 (Drama|Romance) ratings: 1\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "record_summary = \"\\n\".join([doc.page_content for doc in flattened_results])\n",
    "record_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 그래프구조를 활용하여 가장 추출한 청크에서 공통으로 가장 많이 본 영화를 추출한다. \n",
    "- 이를 활용하여 generator의 프롬프트에 넣어 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 788: watched by 3 users\n",
      "Movie 1573: watched by 3 users\n",
      "Movie 2094: watched by 2 users\n",
      "Movie 1320: watched by 2 users\n",
      "Movie 329: watched by 2 users\n",
      "Movie 1917: watched by 2 users\n",
      "Movie 1779: watched by 2 users\n",
      "Movie 173: watched by 2 users\n",
      "Movie 3354: watched by 2 users\n",
      "Movie 880: watched by 2 users\n",
      "Movie 748: watched by 2 users\n",
      "Movie 1580: watched by 2 users\n",
      "Movie 1589: watched by 2 users\n",
      "Movie 1748: watched by 2 users\n",
      "Movie 802: watched by 2 users\n",
      "Movie 3020: watched by 1 users\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_user_movies(data):\n",
    "    \"\"\"\n",
    "    주어진 문서 리스트에서 사용자별로 시청한 영화 ID를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        data (List[Document]): 각 문서는 사용자와 영화 정보가 포함된 page_content를 가짐.\n",
    "\n",
    "    Returns:\n",
    "        dict: user_id를 키로, 해당 유저가 시청한 영화 ID 리스트를 값으로 가지는 딕셔너리.\n",
    "    \"\"\"\n",
    "    user_movies = defaultdict(list)\n",
    "\n",
    "    for doc in data:\n",
    "        user_id = doc.metadata['UserId']\n",
    "        page_content = doc.page_content\n",
    "\n",
    "        # 정규식을 사용하여 영화 ID 추출 (형식: \"영화ID (장르) ratings: N\")\n",
    "        movie_ids = re.findall(r'(\\d+)(?= \\()', page_content)\n",
    "        user_movies[user_id].extend(movie_ids)\n",
    "\n",
    "    return user_movies\n",
    "\n",
    "\n",
    "def create_user_movie_graph(user_movies):\n",
    "    \"\"\"\n",
    "    사용자-영화 관계를 나타내는 bipartite 그래프를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        user_movies (dict): 사용자별 영화 ID 리스트\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: 사용자-영화 이분 그래프\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for user_id, movies in user_movies.items():\n",
    "        user_node = f\"User {user_id}\"\n",
    "        G.add_node(user_node, type='user')\n",
    "\n",
    "        for movie_id in movies:\n",
    "            movie_node = f\"Movie {movie_id}\"\n",
    "            G.add_node(movie_node, type='movie')\n",
    "            G.add_edge(user_node, movie_node)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def visualize_graph(G):\n",
    "    \"\"\"\n",
    "    사용자-영화 그래프를 시각화하여 이미지 파일로 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph): 생성된 사용자-영화 그래프\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    users = [node for node in G.nodes() if G.nodes[node]['type'] == 'user']\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=users, node_color='lightblue', node_size=300, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=movies, node_color='lightgreen', node_size=200, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(\"User-Movie Relationship Graph\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('user_movie_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def extract_previous_movie_ids(purchase_history):\n",
    "    \"\"\"\n",
    "    구매 기록에서 영화 ID만 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        purchase_history (List[str]): 사용자의 영화 구매 기록\n",
    "\n",
    "    Returns:\n",
    "        set: 구매했던 영화들의 ID 집합\n",
    "    \"\"\"\n",
    "    return set(re.findall(r'(\\d+)(?=\\s\\()', ' '.join(purchase_history)))\n",
    "\n",
    "\n",
    "def filter_movies_by_history(top_movies, previous_movie_ids):\n",
    "    \"\"\"\n",
    "    이미 본 영화는 제외하고 추천할 영화를 필터링합니다.\n",
    "\n",
    "    Args:\n",
    "        top_movies (List[Tuple[str, int]]): (영화명, 시청 유저 수) 리스트\n",
    "        previous_movie_ids (set): 사용자 과거 시청 영화 ID\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: 필터링된 추천 영화 리스트\n",
    "    \"\"\"\n",
    "    filtered_movies = [\n",
    "        (movie, count)\n",
    "        for movie, count in top_movies\n",
    "        if movie.split()[1] not in previous_movie_ids  # \"Movie 1234\" → \"1234\"\n",
    "    ]\n",
    "    return filtered_movies\n",
    "\n",
    "\n",
    "def get_top_10_common_movies(G):\n",
    "    \"\"\"\n",
    "    여러 유저들이 공통으로 많이 본 영화 Top-N을 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph): 사용자-영화 이분 그래프\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: 영화 ID와 시청 유저 수를 포함한 상위 영화 리스트\n",
    "    \"\"\"\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "    movie_view_counts = {movie: len(list(G.neighbors(movie))) for movie in movies}\n",
    "    top_10_movies = sorted(movie_view_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    return top_10_movies\n",
    "\n",
    "\n",
    "# --- 아래는 위 함수들을 이용한 실행 흐름 예시 ---\n",
    "\n",
    "# 사용자별 영화 ID 추출\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# 사용자-영화 관계 그래프 생성\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "# 사용자 구매 이력에서 과거 시청 영화 ID 추출\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# 사용자들이 많이 본 상위 영화 추출\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# 과거에 본 영화 제외한 추천 후보 필터링\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# 최종 추천 결과 출력\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raptor로 1차 검색 유저 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "#  상위 100명의 사용자에 대해 반복 평가 수행\n",
    "for idx in range(100):\n",
    "    # (1) 현재 유저 정보 불러오기\n",
    "    target_user_id = str(idx + 1)  # UserId는 1부터 시작\n",
    "    target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "    # (2) RAPTOR 트리 탐색 → 가장 유사한 클러스터 및 유사 유저 추출\n",
    "    best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "        target_user_id, target_user_text, threshold=0.005\n",
    "    )\n",
    "\n",
    "    # (3) 최신 구매 영화 정보를 기반으로 검색 쿼리 생성\n",
    "    purchase_history = data.iloc[idx]['movie_explain']\n",
    "    query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "    # (4) FAISS 기반 검색 수행\n",
    "    records = retriever.get_relevant_documents(query)\n",
    "    record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "\n",
    "    # (5) 검색 결과 중 유사 유저와 겹치는 유저만 필터링\n",
    "    intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "    filtered_records = [\n",
    "        record for record in records\n",
    "        if str(record.metadata['UserId']) in intersection\n",
    "    ]\n",
    "\n",
    "    # (6) 필터링된 문서에서 앞뒤 문맥 청크까지 포함한 문서 리스트 생성\n",
    "    context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n",
    "    flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "    # (7) 사용자-영화 관계 그래프 구축\n",
    "    user_movies = get_user_movies(flattened_results)\n",
    "    G = create_user_movie_graph(user_movies)\n",
    "\n",
    "    # (8) 상위 인기 영화 Top-N 추출 후, 과거 시청 영화는 제거\n",
    "    top_movies = get_top_10_common_movies(G)\n",
    "    filtered_movies = filter_movies_by_history(top_movies, extract_previous_movie_ids(purchase_history))\n",
    "\n",
    "    # (9) 정답 영화 ID 추출 (Test Set에서)\n",
    "    answer = df_test.iloc[idx].movie_explain\n",
    "    answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "\n",
    "    # (10) 추천 영화 목록에서 영화 ID만 추출\n",
    "    filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "\n",
    "    # (11) 추천 목록에 정답이 포함되어 있는지 여부 판단 (Hit = 1, Miss = 0)\n",
    "    hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "    # (12) 결과 기록\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        \"UserId\": [target_user_id],\n",
    "        \"Hit\": [hit],\n",
    "        \"Answer\": [answer_id],\n",
    "        \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "    })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('rapor_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hit\n",
       "0    0.924669\n",
       "1    0.075331\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.Hit.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 휴리스틱 - 수정할 수 있는 변수들 \n",
    "- raptor 파라미터 : threshold, \n",
    "- faiss 검색 범위 k \n",
    "- window 확장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412706/1168534785.py:99: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_results = pd.concat([all_results, results], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# (1) 실험할 파라미터 목록 정의\n",
    "threshold_values = [0.001, 0.005, 0.01]  # RAPTOR 트리 탐색 정밀도 조절\n",
    "faiss_k_values = [300, 500, 700]        # FAISS 검색 시 top-k 문서 수\n",
    "window_values = [1, 2, 3]               # 메타청크 앞뒤 문맥 포함 범위\n",
    "\n",
    "# 전체 실험 결과를 저장할 DataFrame 초기화\n",
    "all_results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\",\n",
    "                                    \"threshold\", \"faiss_k\", \"window\"])\n",
    "\n",
    "# (2) 모든 파라미터 조합에 대해 실험 반복\n",
    "for threshold in threshold_values:\n",
    "    for k_val in faiss_k_values:\n",
    "        for window_size in window_values:\n",
    "            # 각 파라미터 세팅별 결과 저장용 DataFrame\n",
    "            results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "            # 100명의 유저에 대해 추천 및 평가 반복\n",
    "            for idx in range(100):\n",
    "                target_user_id = str(idx + 1)\n",
    "                target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "                # (2-1) RAPTOR 트리로 유사 사용자 탐색 (threshold 적용)\n",
    "                best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "                    target_user_id, \n",
    "                    target_user_text, \n",
    "                    threshold=threshold\n",
    "                )\n",
    "\n",
    "                # 최신 구매 영화 → 검색 쿼리로 사용\n",
    "                purchase_history = data.iloc[idx]['movie_explain']\n",
    "                query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "                # (2-2) FAISS 검색기 설정 (top-k 변경)\n",
    "                retriever_k = vectorstore.as_retriever(search_kwargs={\"k\": k_val})\n",
    "                records = retriever_k.get_relevant_documents(query)\n",
    "\n",
    "                # 유사 사용자 중 검색된 문서만 필터링\n",
    "                record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "                intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "                filtered_records = [\n",
    "                    record for record in records\n",
    "                    if str(record.metadata['UserId']) in intersection\n",
    "                ]\n",
    "\n",
    "                # (2-3) 메타청크 앞뒤 문맥 포함 (window 적용)\n",
    "                context_results = get_documents_with_context(\n",
    "                    vectorstore, filtered_records, context_window=window_size\n",
    "                )\n",
    "                flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "                # (2-4) 사용자-영화 그래프 생성 및 공통 영화 추출\n",
    "                user_movies = get_user_movies(flattened_results)\n",
    "                G = create_user_movie_graph(user_movies)\n",
    "                top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "                # (2-5) 이전 구매 영화는 추천에서 제거\n",
    "                filtered_movies = filter_movies_by_history(\n",
    "                    top_movies, extract_previous_movie_ids(purchase_history)\n",
    "                )\n",
    "\n",
    "                # (2-6) 정답(정답 영화 ID)과 추천 비교 → Hit 여부 계산\n",
    "                answer = df_test.iloc[idx].movie_explain\n",
    "                answer_id = re.search(r\"(\\d+)\", answer).group(1)\n",
    "                filtered_movie_ids = [re.search(r\"(\\d+)\", movie).group(1) for movie, _ in filtered_movies]\n",
    "                hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "                # (2-7) 단일 유저 결과 저장\n",
    "                results = pd.concat([results, pd.DataFrame({\n",
    "                    \"UserId\": [target_user_id],\n",
    "                    \"Hit\": [hit],\n",
    "                    \"Answer\": [answer_id],\n",
    "                    \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "                })], ignore_index=True)\n",
    "\n",
    "            # (3) 각 파라미터 조합 결과에 파라미터 값 명시\n",
    "            results[\"threshold\"] = threshold\n",
    "            results[\"faiss_k\"] = k_val\n",
    "            results[\"window\"] = window_size\n",
    "\n",
    "            # 전체 결과 병합\n",
    "            all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "\n",
    "# (4) 파라미터 조합별 평균 Hit Rate 계산\n",
    "grouped = all_results.groupby([\"threshold\", \"faiss_k\", \"window\"])\n",
    "performance = grouped[\"Hit\"].mean().reset_index(name=\"HitRate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.to_csv('perform_100_thresholds_faiss_k_window_size.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('perform_100_thresholds_faiss_k_window_size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
