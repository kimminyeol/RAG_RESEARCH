{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /home/rlaalsduf/.local/lib/python3.10/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain_community in /home/rlaalsduf/.local/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain_openai in /home/rlaalsduf/.local/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: sentence-transformers in /home/rlaalsduf/.local/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (3.11.14)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (2.2.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_openai) (1.66.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: Pillow in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: tqdm in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: scikit-learn in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scipy in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: networkx in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community langchain_openai sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "import ast\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_google_genai in /home/rlaalsduf/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/rlaalsduf/.local/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: sentence_transformers in /home/rlaalsduf/.local/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.43 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (0.3.45)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (1.2.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (2.10.6)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (0.6.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: Pillow in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (0.29.3)\n",
      "Requirement already satisfied: scikit-learn in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.29.3)\n",
      "Requirement already satisfied: filelock in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (9.0.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.3.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (3.10.15)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_google_genai faiss-cpu sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_284918/3790377730.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/home/rlaalsduf/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x786433f75900>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩과 벡터스토어 불러오기\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "new_vectorstore = FAISS.load_local(\"vectorstore_index_contextual_nonoverlapping\", embeddings, allow_dangerous_deserialization=True)\n",
    "new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# JSON 파일에서 클러스터 매핑 불러오기\n",
    "def load_cluster_mapping(filename=\"for_header_ex/cluster_mapping.json\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        cluster_mapping = json.load(f)\n",
    "    return cluster_mapping\n",
    "\n",
    "# 클러스터 매핑 불러오기\n",
    "cluster_mapping = load_cluster_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x786421367520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베당과 벡터스토어 불러오기\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "new_vectorstore = FAISS.load_local(\"vectorstore_index_contextual_nonoverlapping\", embeddings, allow_dangerous_deserialization=True)\n",
    "new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "# 그래프 임베딩 데이터 불러오기\n",
    "data = pd.read_csv(\"for_header_ex/user_with_chunk_embeddings.csv\")\n",
    "\n",
    "# 'chunk_header_embedding' 컬럼의 각 값을 문자열에서 리스트로 변환한 후 numpy 배열로 변환\n",
    "data['chunk_header_embedding'] = data['chunk_header_embedding'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "data['movie_explain'] = data['movie_explain'].apply(ast.literal_eval)\n",
    "\n",
    "# RAPTOR 트리 불러오기\n",
    "with open(\"for_header_ex/raptor_tree.pkl\", \"rb\") as f:\n",
    "    loaded_raptor_tree = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    두 벡터 간 코사인 유사도를 계산합니다.\n",
    "    두 벡터는 1차원 배열이어야 하며, 반환 값은 1 - cosine_distance (즉, 1에 가까울수록 유사도가 높음)\n",
    "    \"\"\"\n",
    "    vec1 = np.array(vec1, dtype=np.float32).flatten()\n",
    "    vec2 = np.array(vec2, dtype=np.float32).flatten()\n",
    "    if vec1.shape[0] != vec2.shape[0]:\n",
    "        raise ValueError(f\"Dimension mismatch: {vec1.shape[0]} vs {vec2.shape[0]}\")\n",
    "    return 1 - cdist([vec1], [vec2], metric=\"cosine\")[0][0]\n",
    "\n",
    "# Raptor 검색\n",
    "def retrieve_documents_from_tree(query_embedding: np.ndarray,\n",
    "                                 raptor_tree: dict,\n",
    "                                 neighbors_per_level: list = [16, 8, 4, 2, 1, 0],\n",
    "                                 target_user_id: str = None,\n",
    "                                 similarity_threshold: float = 0.1) -> list:\n",
    "    results = []\n",
    "    q_vec = query_embedding.flatten()\n",
    "\n",
    "    for level, n_neighbors in enumerate(neighbors_per_level):\n",
    "        if n_neighbors <= 0 or level not in raptor_tree:\n",
    "            continue\n",
    "\n",
    "        df = raptor_tree[level].copy()\n",
    "\n",
    "        # 🚀 numpy 벡터 연산으로 코사인 유사도 계산 최적화\n",
    "        embeddings_matrix = np.stack(df[\"embedding\"].values)  # 리스트 → np 배열\n",
    "        similarities = 1 - cdist([q_vec], embeddings_matrix, metric=\"cosine\")[0]\n",
    "\n",
    "        df[\"similarity\"] = similarities\n",
    "        df = df[(df[\"similarity\"] >= similarity_threshold) & (df[\"similarity\"] < 1)]\n",
    "\n",
    "        # 🚀 유사도 내림차순 후 n_neighbors개 추출 (벡터 연산 최적화)\n",
    "        top_docs = df.nlargest(n_neighbors, \"similarity\").copy()\n",
    "        top_docs[\"level\"] = level\n",
    "        results.extend(top_docs.to_dict(orient=\"records\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_nearest(n) :\n",
    "\n",
    "    # Target User의 그래프 임베딩 가져오기\n",
    "    query = np.array(data.iloc[n]['chunk_header_embedding'], dtype=np.float32)\n",
    "    purchase_history=data.iloc[n]['movie_explain']\n",
    "\n",
    "    # 쿼리 벡터를 FAISS 검색에 사용할 수 있도록 변환\n",
    "    query_embedding = query.reshape(1, -1)  # FAISS는 2D 배열을 요구하므로 변환\n",
    "\n",
    "    # 기존에 retrieve_documents_from_tree 함수를 사용하여 검색한 결과 (각 결과는 dict 형태)\n",
    "    retrieved_docs = retrieve_documents_from_tree(query_embedding, loaded_raptor_tree)\n",
    "\n",
    "    # 리스트 초기화\n",
    "    node_id = []\n",
    "    level = []\n",
    "    sim = []\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    for doc in retrieved_docs:\n",
    "        node_id.append(doc.get(\"user_id\", \"N/A\"))\n",
    "        level.append(doc.get(\"level\", \"N/A\"))\n",
    "        sim.append(doc.get(\"similarity\", 0))\n",
    "\n",
    "    result_df = pd.DataFrame({'ID': node_id, 'Level': level, 'Similarity': sim}).nlargest(10, 'Similarity')\n",
    "\n",
    "    return result_df, purchase_history\n",
    "\n",
    "def classification_ids(result_df) :\n",
    "    # 선택된 ID 리스트 (클러스터 ID와 개별 ID 혼합)\n",
    "    selected_ids = result_df['ID'].tolist()\n",
    "\n",
    "    # 클러스터 ID와 개별 사용자 ID 구분\n",
    "    cluster_ids = [sid for sid in selected_ids if isinstance(sid, str) and \"cluster\" in sid]\n",
    "    individual_ids = [sid for sid in selected_ids if sid not in cluster_ids]\n",
    "\n",
    "    return cluster_ids, individual_ids\n",
    "\n",
    "# 클러스터에서 개별 사용자 ID 추출\n",
    "def get_individual_ids_from_cluster(cluster_id: str, cluster_mapping: dict) -> list:\n",
    "    \"\"\"\n",
    "    클러스터 매핑 딕셔너리를 활용하여 특정 클러스터에 속한 개별 사용자 ID들을 반환합니다.\n",
    "    \"\"\"\n",
    "    return cluster_mapping.get(cluster_id, [])\n",
    "\n",
    "def aggregate_movies_from_docs(docs: list) -> pd.DataFrame:\n",
    "    \"\"\"메타 청크(Document) 리스트에서 영화 정보 집계\"\"\"\n",
    "    movie_stats = defaultdict(lambda: {\"count\": 0, \"rating_sum\": 0, \"genre\": None})\n",
    "    movie_pattern = re.compile(r'(\\d+)\\s*\\(([^)]+)\\)\\s*ratings:\\s*(\\d+)')\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        matches = movie_pattern.findall(text)\n",
    "        for movie_id, genre, rating in matches:\n",
    "            movie_id = int(movie_id)\n",
    "            rating = float(rating)\n",
    "            movie_stats[movie_id][\"count\"] += 1\n",
    "            movie_stats[movie_id][\"rating_sum\"] += rating\n",
    "            if movie_stats[movie_id][\"genre\"] is None:\n",
    "                movie_stats[movie_id][\"genre\"] = genre\n",
    "\n",
    "    movies_data = []\n",
    "    for movie_id, stats in movie_stats.items():\n",
    "        avg_rating = stats[\"rating_sum\"] / stats[\"count\"]\n",
    "        movies_data.append({\n",
    "            \"movie_id\": movie_id,\n",
    "            \"frequency\": stats[\"count\"],\n",
    "            \"avg_rating\": avg_rating,\n",
    "            \"genre\": stats[\"genre\"]\n",
    "        })\n",
    "    df = pd.DataFrame(movies_data)\n",
    "    return df\n",
    "\n",
    "def summarize_clusters(cluster_ids, cluster_mapping, vectorstore):\n",
    "    \"\"\"\n",
    "    클러스터 ID를 받아 해당 클러스터에 속한 개별 사용자들의 문서들을 찾아 요약하고,\n",
    "    사용자별 시청한 영화 ID를 함께 반환함.\n",
    "    \"\"\"\n",
    "    all_docs = list(vectorstore.docstore._dict.values())  # FAISS 문서 가져오기\n",
    "    cluster_summaries = {}\n",
    "    cluster_movies = {}\n",
    "\n",
    "    for cluster_id in cluster_ids:\n",
    "        member_ids = cluster_mapping.get(cluster_id, [])\n",
    "        cluster_docs = [doc for doc in all_docs if doc.metadata.get(\"UserId\") in member_ids]\n",
    "\n",
    "        if not cluster_docs:\n",
    "            cluster_summaries[cluster_id] = f\"{cluster_id}: 해당 청크 없음\"\n",
    "            cluster_movies[cluster_id] = []\n",
    "            continue\n",
    "\n",
    "        # 사용자별 시청한 영화 ID 추출\n",
    "        user_movies = get_user_movies(cluster_docs)\n",
    "\n",
    "        # 영화 정보 집계 및 상위 10개 추출\n",
    "        movies_df = aggregate_movies_from_docs(cluster_docs)\n",
    "        top_movies_df = movies_df.sort_values(by=[\"frequency\", \"avg_rating\"], ascending=False).head(10)\n",
    "\n",
    "        # 요약 및 영화 ID 정보 함께 저장\n",
    "        cluster_summaries[cluster_id] = \"\\n\".join(\n",
    "            f\"{row['movie_id']} ({row['genre']}) Rating: {row['avg_rating']:.2f}\"\n",
    "            for _, row in top_movies_df.iterrows()\n",
    "        )\n",
    "\n",
    "        # 클러스터에 속한 모든 사용자들의 시청 영화 ID 저장\n",
    "        cluster_movies[cluster_id] = list(set(movie_id for user_id in member_ids for movie_id in user_movies[user_id]))\n",
    "\n",
    "    return cluster_summaries, cluster_movies\n",
    "\n",
    "def summarize_individuals(individual_ids, vectorstore, purchase_history, embeddings):\n",
    "    \"\"\"\n",
    "    각 개별 사용자에 대해, 해당 사용자의 여러 청크 중 purchase_history와\n",
    "    코사인 유사도가 가장 높은 청크를 선택하여 요약하며, 본 영화 목록도 함께 반환함.\n",
    "    \"\"\"\n",
    "    if isinstance(purchase_history, list):\n",
    "        purchase_history = \" \".join(purchase_history)  # 리스트 → 문자열 변환\n",
    "\n",
    "    purchase_emb = embeddings.embed_documents([purchase_history])[0]  # 문서 임베딩\n",
    "\n",
    "    individual_summaries = {}\n",
    "    individual_movies = {}\n",
    "\n",
    "    for uid in individual_ids:\n",
    "        user_docs = [\n",
    "            doc for doc in vectorstore.docstore._dict.values()\n",
    "            if doc.metadata.get(\"UserId\") == uid\n",
    "        ]\n",
    "        if user_docs:\n",
    "            # 개별 사용자의 시청 영화 목록 가져오기\n",
    "            user_movies = get_user_movies(user_docs)\n",
    "\n",
    "            # 각 청크의 임베딩 계산\n",
    "            doc_embeddings = embeddings.embed_documents([doc.page_content for doc in user_docs])\n",
    "\n",
    "            # 가장 유사도가 높은 청크 선택\n",
    "            best_doc_idx = max(range(len(user_docs)), key=lambda i: cosine_similarity(purchase_emb, doc_embeddings[i]))\n",
    "            best_doc = user_docs[best_doc_idx]\n",
    "\n",
    "            individual_summaries[uid] = best_doc.page_content\n",
    "            individual_movies[uid] = user_movies[uid]  # 개별 사용자의 영화 목록 추가\n",
    "        else:\n",
    "            individual_summaries[uid] = f\"{uid}: 해당 청크 없음\"\n",
    "            individual_movies[uid] = []\n",
    "\n",
    "    return individual_summaries, individual_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터별 영화 정보를 원하는 형식으로 변환\n",
    "def format_cluster_summaries(cluster_summaries):\n",
    "    \"\"\"\n",
    "    클러스터별 요약을 변환하여, 개별 영화 정보만 유지하는 형태로 변환합니다.\n",
    "    \"\"\"\n",
    "    formatted_movies = []\n",
    "    for cluster_id, summary in cluster_summaries.items():\n",
    "        for line in summary.split(\"\\n\"):\n",
    "            formatted_movies.append(line)  # 개별 영화 정보를 리스트에 추가\n",
    "    return \" \".join(formatted_movies)  # 공백으로 연결하여 하나의 문자열로 변환\n",
    "\n",
    "# 개별 사용자 메타 청크 데이터와 합치기\n",
    "def merge_summaries(cluster_summaries, individual_summaries):\n",
    "    \"\"\"\n",
    "    클러스터에서 추출한 영화 요약과 개별 사용자의 요약을 하나로 병합합니다.\n",
    "    \"\"\"\n",
    "    # 클러스터 데이터를 포맷팅하여 단일 텍스트로 변환\n",
    "    formatted_cluster_summary = format_cluster_summaries(cluster_summaries)\n",
    "\n",
    "    # 개별 사용자 요약을 하나의 텍스트로 병합\n",
    "    formatted_individual_summary = \" \".join(individual_summaries.values())\n",
    "\n",
    "    # 두 요약을 합치기\n",
    "    final_summary = f\"{formatted_cluster_summary} {formatted_individual_summary}\"\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# 클러스터별 영화 정보를 원하는 형식으로 변환\n",
    "def format_cluster_summaries(cluster_summaries):\n",
    "    \"\"\"\n",
    "    클러스터별 요약을 변환하여, 개별 영화 정보만 유지하는 형태로 변환합니다.\n",
    "    \"\"\"\n",
    "    formatted_movies = []\n",
    "    for cluster_id, summary in cluster_summaries.items():\n",
    "        for line in summary.split(\"\\n\"):\n",
    "            formatted_movies.append(line)  # 개별 영화 정보를 리스트에 추가\n",
    "    return \" \".join(formatted_movies)  # 공백으로 연결하여 하나의 문자열로 변환\n",
    "\n",
    "# 개별 사용자 메타 청크 데이터와 합치기\n",
    "def merge_summaries(cluster_summaries, cluster_movies, individual_summaries, individual_movies):\n",
    "    \"\"\"\n",
    "    클러스터에서 추출한 영화 요약과 개별 사용자의 요약을 하나로 병합하며,\n",
    "    각 사용자가 시청한 영화 ID 목록도 함께 포함합니다.\n",
    "    \"\"\"\n",
    "    # 클러스터 데이터를 포맷팅하여 단일 텍스트로 변환\n",
    "    formatted_cluster_summary = format_cluster_summaries(cluster_summaries)\n",
    "\n",
    "    # 개별 사용자 요약을 하나의 텍스트로 병합\n",
    "    formatted_individual_summary = \" \".join(individual_summaries.values())\n",
    "\n",
    "    # 두 요약을 합침\n",
    "    final_summary = f\"{formatted_cluster_summary} {formatted_individual_summary}\"\n",
    "\n",
    "    # 사용자가 본 영화 ID 정보 병합\n",
    "    merged_movie_ids = {**cluster_movies, **individual_movies}\n",
    "\n",
    "    return final_summary, merged_movie_ids\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# LangChain 프롬프트 생성\n",
    "template = \"\"\"\n",
    "## item ##\n",
    "Here is the summary of frequently watched movies from other users:\n",
    "{record_summary}\n",
    "\n",
    "## role ##\n",
    "Based on the provided watching record summary ({record_summary}), recommend exactly 10 movies for the user to watch next.\n",
    "You must prioritize movies that:\n",
    "1. Appear frequently in the given watching record summary ({record_summary}).\n",
    "\n",
    "For each recommended movie, provide a brief reason why it is recommended. The reason must include:\n",
    "- How frequently the movie appears in the record summary.\n",
    "- Why it might be appealing to the user based on similar preferences.\n",
    "\n",
    "Provide the recommendations as a numbered list in the following format:\n",
    "1. Movie ID - Reason\n",
    "2. Movie ID - Reason\n",
    "...\n",
    "10. Movie ID - Reason\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer(purchase_history, final_summary) :\n",
    "    # OpenAI 모델 설정 (LangChain 최신 방식)\n",
    "    response= chain.invoke({\"purchase_history\":purchase_history, \"record_summary\": final_summary })\n",
    "    return response\n",
    "\n",
    "# # 프롬프트 생성\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "# chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>movie_explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"48 (Animation|Children's|Musical|Romance) ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['1917 (Action|Adventure|Sci-Fi|Thriller) rati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"2081 (Animation|Children's|Comedy|Musical|Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['1954 (Action|Drama) ratings: 5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['288 (Action|Thriller) ratings: 2']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId                                      movie_explain\n",
       "0       1  [\"48 (Animation|Children's|Musical|Romance) ra...\n",
       "1       2  ['1917 (Action|Adventure|Sci-Fi|Thriller) rati...\n",
       "2       3  [\"2081 (Animation|Children's|Comedy|Musical|Ro...\n",
       "3       4                 ['1954 (Action|Drama) ratings: 5']\n",
       "4       5               ['288 (Action|Thriller) ratings: 2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI API 키 설정 (환경 변수에서 가져오기)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "# 테스트 데이터 불러오기\n",
    "df_test = pd.read_csv('./data/test_movie.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch_geometric in /home/rlaalsduf/.local/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (2.2.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: aiohttp in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (3.11.14)\n",
      "Requirement already satisfied: tqdm in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_user_movies(data):\n",
    "    user_movies = defaultdict(list)\n",
    "\n",
    "    for doc in data:\n",
    "        user_id = doc.metadata['UserId']\n",
    "        page_content = doc.page_content\n",
    "\n",
    "        # 정규식을 사용하여 영화 ID 추출 (ratings 이전 내용만)\n",
    "        movie_ids = re.findall(r'(\\d+)(?= \\()', page_content)\n",
    "\n",
    "        # 사용자별로 영화 ID 추가\n",
    "        user_movies[user_id].extend(movie_ids)\n",
    "\n",
    "    return user_movies\n",
    "\n",
    "def create_user_movie_graph(user_movies):\n",
    "    # 그래프 생성\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 사용자와 영화 노드 추가 및 엣지 생성\n",
    "    for user_id, movies in user_movies.items():\n",
    "        user_node = f\"User {user_id}\"\n",
    "        G.add_node(user_node, type='user')\n",
    "\n",
    "        for movie_id in movies:\n",
    "            movie_node = f\"Movie {movie_id}\"\n",
    "            G.add_node(movie_node, type='movie')\n",
    "            G.add_edge(user_node, movie_node)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def visualize_graph(G):\n",
    "    # 그래프 시각화\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # 사용자와 영화 노드 분리\n",
    "    users = [node for node in G.nodes() if G.nodes[node]['type'] == 'user']\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # 레이아웃 설정\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "    # 노드 그리기\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=users, node_color='lightblue', node_size=300, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=movies, node_color='lightgreen', node_size=200, alpha=0.8)\n",
    "\n",
    "    # 엣지 그리기\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "    # 레이블 그리기\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(\"User-Movie Relationship Graph\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 그래프 저장\n",
    "    plt.savefig('user_movie_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "import re\n",
    "\n",
    "# 과거 기록에서 영화 ID 추출\n",
    "def extract_previous_movie_ids(purchase_history):\n",
    "    # 영화 ID만 추출 (숫자와 괄호 전까지만 가져옴)\n",
    "    previous_movies = re.findall(r'(\\d+)(?=\\s\\()', ' '.join(purchase_history))\n",
    "    return set(previous_movies)\n",
    "\n",
    "# 동시 시청 영화에서 이전 기록을 제외하는 함수\n",
    "def filter_movies_by_history(top_movies, previous_movie_ids):\n",
    "    # 영화 ID만 추출하여 비교 후 제외\n",
    "    filtered_movies = [(movie, count) for movie, count in top_movies if movie.split()[1] not in previous_movie_ids]\n",
    "    return filtered_movies\n",
    "def get_top_10_common_movies(G):\n",
    "    # 영화 노드만 필터링\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # 영화별 연결된 사용자 수 계산\n",
    "    movie_view_counts = {movie: len(list(G.neighbors(movie))) for movie in movies}\n",
    "\n",
    "    # 사용자 수 기준으로 내림차순 정렬하여 상위 10개 추출\n",
    "    top_10_movies = sorted(movie_view_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    # 결과 반환\n",
    "    return top_10_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users: 100%|██████████| 6040/6040 [5:06:11<00:00,  3.04s/user]  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "# 🚀 전체 문서 한 번만 로드하여 캐싱\n",
    "all_docs = list(new_vectorstore.docstore._dict.values())\n",
    "\n",
    "# # 🚀 사용자-영화 관계 데이터 미리 생성\n",
    "# user_movies = get_user_movies(all_docs)\n",
    "\n",
    "# # 🚀 PyTorch Geometric 그래프 생성 (GPU 사용)\n",
    "# G = create_user_movie_graph_fast(user_movies)\n",
    "\n",
    "# 1부터 6040까지 반복하면서 진행률 표시 (tqdm 사용)\n",
    "for idx in tqdm(range(6040), desc=\"Processing users\", unit=\"user\"):\n",
    "\n",
    "    # 정답 추출\n",
    "    test_answer = df_test.iloc[idx]['movie_explain']\n",
    "\n",
    "    # movie_name 추출\n",
    "    try:\n",
    "        movie_name = re.search(r\"\\d+\", test_answer).group()\n",
    "    except AttributeError:\n",
    "        movie_name = None  # 매칭 실패 시 None으로 처리\n",
    "\n",
    "    # 쿼리에 따른 최근접 이웃 추출 + 쿼리 사용자의 최신 상호작용 추출\n",
    "    result_df, purchase_history = extract_nearest(idx)\n",
    "    cluster_ids, individual_ids = classification_ids(result_df)\n",
    "\n",
    "    # 🚀 클러스터 & 개별 사용자 데이터를 한 번에 캐싱하여 처리\n",
    "    all_docs = list(new_vectorstore.docstore._dict.values())\n",
    "\n",
    "    # 클러스터 및 개별 사용자의 요약 및 시청 영화 ID 정보 가져오기\n",
    "    cluster_summaries, cluster_movies = summarize_clusters(cluster_ids, cluster_mapping, new_vectorstore)\n",
    "    individual_summaries, individual_movies = summarize_individuals(individual_ids, new_vectorstore, purchase_history, embeddings)\n",
    "\n",
    "    # 최종 요약과 사용자의 시청 영화 ID 목록 병합\n",
    "    final_summary, merged_movie_ids = merge_summaries(cluster_summaries, cluster_movies, individual_summaries, individual_movies)\n",
    "\n",
    "    # 사용자-영화 그래프 생성\n",
    "    user_movies = get_user_movies(new_vectorstore.docstore._dict.values())\n",
    "\n",
    "    G = create_user_movie_graph(user_movies)\n",
    "\n",
    "    # 이용자의 과거 시청 영화 목록 가져오기\n",
    "    previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "    # Top-10 가장 많이 본 영화 가져오기\n",
    "    top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "    # 과거 시청한 영화 제외 후 추천 영화 필터링\n",
    "    filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "    # 리스트 내부 영화 정보를 문자열로 변환 후 정규식 적용\n",
    "    movie_ids = [re.search(r'(\\d+)', movie[0]).group() for movie in filtered_movies if re.search(r'(\\d+)', movie[0])]\n",
    "\n",
    "    # movie_name이 movie_titles에 포함되는지 여부\n",
    "    is_included = movie_name in movie_ids if movie_name else False\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"movie_title\": movie_ids,\n",
    "        \"movie_name\": movie_name,\n",
    "        \"is_included\": is_included\n",
    "    })\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "final_results = pd.DataFrame(results)\n",
    "final_results.head()\n",
    "\n",
    "final_results.to_csv('graph_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@10: 0.030794701986754967\n"
     ]
    }
   ],
   "source": [
    "# Hit@k 계산 (k=10)\n",
    "k = 10\n",
    "final_results['hit'] = final_results.apply(lambda row: row['movie_name'] in row['movie_title'][:k], axis=1)\n",
    "\n",
    "# 전체 Hit@k 계산\n",
    "hit_at_k = final_results['hit'].mean()  # Hit@k 비율\n",
    "print(f\"Hit@{k}: {hit_at_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
