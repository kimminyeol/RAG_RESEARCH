{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /home/rlaalsduf/.local/lib/python3.10/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain_community in /home/rlaalsduf/.local/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain_openai in /home/rlaalsduf/.local/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: sentence-transformers in /home/rlaalsduf/.local/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (3.11.14)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_community) (2.2.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_openai) (1.66.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: Pillow in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: tqdm in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: scikit-learn in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scipy in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: networkx in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community langchain_openai sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "import ast\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_google_genai in /home/rlaalsduf/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/rlaalsduf/.local/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: sentence_transformers in /home/rlaalsduf/.local/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.43 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (0.3.45)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (1.2.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (2.10.6)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain_google_genai) (0.6.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: Pillow in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (0.29.3)\n",
      "Requirement already satisfied: scikit-learn in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.29.3)\n",
      "Requirement already satisfied: filelock in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (9.0.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.3.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (3.10.15)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_google_genai faiss-cpu sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_284918/3790377730.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/home/rlaalsduf/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x786433f75900>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì„ë² ë”©ê³¼ ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "new_vectorstore = FAISS.load_local(\"vectorstore_index_contextual_nonoverlapping\", embeddings, allow_dangerous_deserialization=True)\n",
    "new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# JSON íŒŒì¼ì—ì„œ í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_cluster_mapping(filename=\"for_header_ex/cluster_mapping.json\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        cluster_mapping = json.load(f)\n",
    "    return cluster_mapping\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "cluster_mapping = load_cluster_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x786421367520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì„ë² ë‹¹ê³¼ ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "new_vectorstore = FAISS.load_local(\"vectorstore_index_contextual_nonoverlapping\", embeddings, allow_dangerous_deserialization=True)\n",
    "new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "# ê·¸ë˜í”„ ì„ë² ë”© ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "data = pd.read_csv(\"for_header_ex/user_with_chunk_embeddings.csv\")\n",
    "\n",
    "# 'chunk_header_embedding' ì»¬ëŸ¼ì˜ ê° ê°’ì„ ë¬¸ìì—´ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ í›„ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "data['chunk_header_embedding'] = data['chunk_header_embedding'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "data['movie_explain'] = data['movie_explain'].apply(ast.literal_eval)\n",
    "\n",
    "# RAPTOR íŠ¸ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"for_header_ex/raptor_tree.pkl\", \"rb\") as f:\n",
    "    loaded_raptor_tree = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    ë‘ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    ë‘ ë²¡í„°ëŠ” 1ì°¨ì› ë°°ì—´ì´ì–´ì•¼ í•˜ë©°, ë°˜í™˜ ê°’ì€ 1 - cosine_distance (ì¦‰, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ)\n",
    "    \"\"\"\n",
    "    vec1 = np.array(vec1, dtype=np.float32).flatten()\n",
    "    vec2 = np.array(vec2, dtype=np.float32).flatten()\n",
    "    if vec1.shape[0] != vec2.shape[0]:\n",
    "        raise ValueError(f\"Dimension mismatch: {vec1.shape[0]} vs {vec2.shape[0]}\")\n",
    "    return 1 - cdist([vec1], [vec2], metric=\"cosine\")[0][0]\n",
    "\n",
    "# Raptor ê²€ìƒ‰\n",
    "def retrieve_documents_from_tree(query_embedding: np.ndarray,\n",
    "                                 raptor_tree: dict,\n",
    "                                 neighbors_per_level: list = [16, 8, 4, 2, 1, 0],\n",
    "                                 target_user_id: str = None,\n",
    "                                 similarity_threshold: float = 0.1) -> list:\n",
    "    results = []\n",
    "    q_vec = query_embedding.flatten()\n",
    "\n",
    "    for level, n_neighbors in enumerate(neighbors_per_level):\n",
    "        if n_neighbors <= 0 or level not in raptor_tree:\n",
    "            continue\n",
    "\n",
    "        df = raptor_tree[level].copy()\n",
    "\n",
    "        # ğŸš€ numpy ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”\n",
    "        embeddings_matrix = np.stack(df[\"embedding\"].values)  # ë¦¬ìŠ¤íŠ¸ â†’ np ë°°ì—´\n",
    "        similarities = 1 - cdist([q_vec], embeddings_matrix, metric=\"cosine\")[0]\n",
    "\n",
    "        df[\"similarity\"] = similarities\n",
    "        df = df[(df[\"similarity\"] >= similarity_threshold) & (df[\"similarity\"] < 1)]\n",
    "\n",
    "        # ğŸš€ ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ í›„ n_neighborsê°œ ì¶”ì¶œ (ë²¡í„° ì—°ì‚° ìµœì í™”)\n",
    "        top_docs = df.nlargest(n_neighbors, \"similarity\").copy()\n",
    "        top_docs[\"level\"] = level\n",
    "        results.extend(top_docs.to_dict(orient=\"records\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_nearest(n) :\n",
    "\n",
    "    # Target Userì˜ ê·¸ë˜í”„ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°\n",
    "    query = np.array(data.iloc[n]['chunk_header_embedding'], dtype=np.float32)\n",
    "    purchase_history=data.iloc[n]['movie_explain']\n",
    "\n",
    "    # ì¿¼ë¦¬ ë²¡í„°ë¥¼ FAISS ê²€ìƒ‰ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜\n",
    "    query_embedding = query.reshape(1, -1)  # FAISSëŠ” 2D ë°°ì—´ì„ ìš”êµ¬í•˜ë¯€ë¡œ ë³€í™˜\n",
    "\n",
    "    # ê¸°ì¡´ì— retrieve_documents_from_tree í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•œ ê²°ê³¼ (ê° ê²°ê³¼ëŠ” dict í˜•íƒœ)\n",
    "    retrieved_docs = retrieve_documents_from_tree(query_embedding, loaded_raptor_tree)\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    node_id = []\n",
    "    level = []\n",
    "    sim = []\n",
    "\n",
    "    # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    for doc in retrieved_docs:\n",
    "        node_id.append(doc.get(\"user_id\", \"N/A\"))\n",
    "        level.append(doc.get(\"level\", \"N/A\"))\n",
    "        sim.append(doc.get(\"similarity\", 0))\n",
    "\n",
    "    result_df = pd.DataFrame({'ID': node_id, 'Level': level, 'Similarity': sim}).nlargest(10, 'Similarity')\n",
    "\n",
    "    return result_df, purchase_history\n",
    "\n",
    "def classification_ids(result_df) :\n",
    "    # ì„ íƒëœ ID ë¦¬ìŠ¤íŠ¸ (í´ëŸ¬ìŠ¤í„° IDì™€ ê°œë³„ ID í˜¼í•©)\n",
    "    selected_ids = result_df['ID'].tolist()\n",
    "\n",
    "    # í´ëŸ¬ìŠ¤í„° IDì™€ ê°œë³„ ì‚¬ìš©ì ID êµ¬ë¶„\n",
    "    cluster_ids = [sid for sid in selected_ids if isinstance(sid, str) and \"cluster\" in sid]\n",
    "    individual_ids = [sid for sid in selected_ids if sid not in cluster_ids]\n",
    "\n",
    "    return cluster_ids, individual_ids\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ì—ì„œ ê°œë³„ ì‚¬ìš©ì ID ì¶”ì¶œ\n",
    "def get_individual_ids_from_cluster(cluster_id: str, cluster_mapping: dict) -> list:\n",
    "    \"\"\"\n",
    "    í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ë¥¼ í™œìš©í•˜ì—¬ íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê°œë³„ ì‚¬ìš©ì IDë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    return cluster_mapping.get(cluster_id, [])\n",
    "\n",
    "def aggregate_movies_from_docs(docs: list) -> pd.DataFrame:\n",
    "    \"\"\"ë©”íƒ€ ì²­í¬(Document) ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜í™” ì •ë³´ ì§‘ê³„\"\"\"\n",
    "    movie_stats = defaultdict(lambda: {\"count\": 0, \"rating_sum\": 0, \"genre\": None})\n",
    "    movie_pattern = re.compile(r'(\\d+)\\s*\\(([^)]+)\\)\\s*ratings:\\s*(\\d+)')\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        matches = movie_pattern.findall(text)\n",
    "        for movie_id, genre, rating in matches:\n",
    "            movie_id = int(movie_id)\n",
    "            rating = float(rating)\n",
    "            movie_stats[movie_id][\"count\"] += 1\n",
    "            movie_stats[movie_id][\"rating_sum\"] += rating\n",
    "            if movie_stats[movie_id][\"genre\"] is None:\n",
    "                movie_stats[movie_id][\"genre\"] = genre\n",
    "\n",
    "    movies_data = []\n",
    "    for movie_id, stats in movie_stats.items():\n",
    "        avg_rating = stats[\"rating_sum\"] / stats[\"count\"]\n",
    "        movies_data.append({\n",
    "            \"movie_id\": movie_id,\n",
    "            \"frequency\": stats[\"count\"],\n",
    "            \"avg_rating\": avg_rating,\n",
    "            \"genre\": stats[\"genre\"]\n",
    "        })\n",
    "    df = pd.DataFrame(movies_data)\n",
    "    return df\n",
    "\n",
    "def summarize_clusters(cluster_ids, cluster_mapping, vectorstore):\n",
    "    \"\"\"\n",
    "    í´ëŸ¬ìŠ¤í„° IDë¥¼ ë°›ì•„ í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê°œë³„ ì‚¬ìš©ìë“¤ì˜ ë¬¸ì„œë“¤ì„ ì°¾ì•„ ìš”ì•½í•˜ê³ ,\n",
    "    ì‚¬ìš©ìë³„ ì‹œì²­í•œ ì˜í™” IDë¥¼ í•¨ê»˜ ë°˜í™˜í•¨.\n",
    "    \"\"\"\n",
    "    all_docs = list(vectorstore.docstore._dict.values())  # FAISS ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "    cluster_summaries = {}\n",
    "    cluster_movies = {}\n",
    "\n",
    "    for cluster_id in cluster_ids:\n",
    "        member_ids = cluster_mapping.get(cluster_id, [])\n",
    "        cluster_docs = [doc for doc in all_docs if doc.metadata.get(\"UserId\") in member_ids]\n",
    "\n",
    "        if not cluster_docs:\n",
    "            cluster_summaries[cluster_id] = f\"{cluster_id}: í•´ë‹¹ ì²­í¬ ì—†ìŒ\"\n",
    "            cluster_movies[cluster_id] = []\n",
    "            continue\n",
    "\n",
    "        # ì‚¬ìš©ìë³„ ì‹œì²­í•œ ì˜í™” ID ì¶”ì¶œ\n",
    "        user_movies = get_user_movies(cluster_docs)\n",
    "\n",
    "        # ì˜í™” ì •ë³´ ì§‘ê³„ ë° ìƒìœ„ 10ê°œ ì¶”ì¶œ\n",
    "        movies_df = aggregate_movies_from_docs(cluster_docs)\n",
    "        top_movies_df = movies_df.sort_values(by=[\"frequency\", \"avg_rating\"], ascending=False).head(10)\n",
    "\n",
    "        # ìš”ì•½ ë° ì˜í™” ID ì •ë³´ í•¨ê»˜ ì €ì¥\n",
    "        cluster_summaries[cluster_id] = \"\\n\".join(\n",
    "            f\"{row['movie_id']} ({row['genre']}) Rating: {row['avg_rating']:.2f}\"\n",
    "            for _, row in top_movies_df.iterrows()\n",
    "        )\n",
    "\n",
    "        # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ëª¨ë“  ì‚¬ìš©ìë“¤ì˜ ì‹œì²­ ì˜í™” ID ì €ì¥\n",
    "        cluster_movies[cluster_id] = list(set(movie_id for user_id in member_ids for movie_id in user_movies[user_id]))\n",
    "\n",
    "    return cluster_summaries, cluster_movies\n",
    "\n",
    "def summarize_individuals(individual_ids, vectorstore, purchase_history, embeddings):\n",
    "    \"\"\"\n",
    "    ê° ê°œë³„ ì‚¬ìš©ìì— ëŒ€í•´, í•´ë‹¹ ì‚¬ìš©ìì˜ ì—¬ëŸ¬ ì²­í¬ ì¤‘ purchase_historyì™€\n",
    "    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ì²­í¬ë¥¼ ì„ íƒí•˜ì—¬ ìš”ì•½í•˜ë©°, ë³¸ ì˜í™” ëª©ë¡ë„ í•¨ê»˜ ë°˜í™˜í•¨.\n",
    "    \"\"\"\n",
    "    if isinstance(purchase_history, list):\n",
    "        purchase_history = \" \".join(purchase_history)  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜\n",
    "\n",
    "    purchase_emb = embeddings.embed_documents([purchase_history])[0]  # ë¬¸ì„œ ì„ë² ë”©\n",
    "\n",
    "    individual_summaries = {}\n",
    "    individual_movies = {}\n",
    "\n",
    "    for uid in individual_ids:\n",
    "        user_docs = [\n",
    "            doc for doc in vectorstore.docstore._dict.values()\n",
    "            if doc.metadata.get(\"UserId\") == uid\n",
    "        ]\n",
    "        if user_docs:\n",
    "            # ê°œë³„ ì‚¬ìš©ìì˜ ì‹œì²­ ì˜í™” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "            user_movies = get_user_movies(user_docs)\n",
    "\n",
    "            # ê° ì²­í¬ì˜ ì„ë² ë”© ê³„ì‚°\n",
    "            doc_embeddings = embeddings.embed_documents([doc.page_content for doc in user_docs])\n",
    "\n",
    "            # ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì²­í¬ ì„ íƒ\n",
    "            best_doc_idx = max(range(len(user_docs)), key=lambda i: cosine_similarity(purchase_emb, doc_embeddings[i]))\n",
    "            best_doc = user_docs[best_doc_idx]\n",
    "\n",
    "            individual_summaries[uid] = best_doc.page_content\n",
    "            individual_movies[uid] = user_movies[uid]  # ê°œë³„ ì‚¬ìš©ìì˜ ì˜í™” ëª©ë¡ ì¶”ê°€\n",
    "        else:\n",
    "            individual_summaries[uid] = f\"{uid}: í•´ë‹¹ ì²­í¬ ì—†ìŒ\"\n",
    "            individual_movies[uid] = []\n",
    "\n",
    "    return individual_summaries, individual_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ëŸ¬ìŠ¤í„°ë³„ ì˜í™” ì •ë³´ë¥¼ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "def format_cluster_summaries(cluster_summaries):\n",
    "    \"\"\"\n",
    "    í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ì„ ë³€í™˜í•˜ì—¬, ê°œë³„ ì˜í™” ì •ë³´ë§Œ ìœ ì§€í•˜ëŠ” í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    formatted_movies = []\n",
    "    for cluster_id, summary in cluster_summaries.items():\n",
    "        for line in summary.split(\"\\n\"):\n",
    "            formatted_movies.append(line)  # ê°œë³„ ì˜í™” ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    return \" \".join(formatted_movies)  # ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "# ê°œë³„ ì‚¬ìš©ì ë©”íƒ€ ì²­í¬ ë°ì´í„°ì™€ í•©ì¹˜ê¸°\n",
    "def merge_summaries(cluster_summaries, individual_summaries):\n",
    "    \"\"\"\n",
    "    í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¶”ì¶œí•œ ì˜í™” ìš”ì•½ê³¼ ê°œë³„ ì‚¬ìš©ìì˜ ìš”ì•½ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    formatted_cluster_summary = format_cluster_summaries(cluster_summaries)\n",
    "\n",
    "    # ê°œë³„ ì‚¬ìš©ì ìš”ì•½ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³‘í•©\n",
    "    formatted_individual_summary = \" \".join(individual_summaries.values())\n",
    "\n",
    "    # ë‘ ìš”ì•½ì„ í•©ì¹˜ê¸°\n",
    "    final_summary = f\"{formatted_cluster_summary} {formatted_individual_summary}\"\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ ì˜í™” ì •ë³´ë¥¼ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "def format_cluster_summaries(cluster_summaries):\n",
    "    \"\"\"\n",
    "    í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ì„ ë³€í™˜í•˜ì—¬, ê°œë³„ ì˜í™” ì •ë³´ë§Œ ìœ ì§€í•˜ëŠ” í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    formatted_movies = []\n",
    "    for cluster_id, summary in cluster_summaries.items():\n",
    "        for line in summary.split(\"\\n\"):\n",
    "            formatted_movies.append(line)  # ê°œë³„ ì˜í™” ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    return \" \".join(formatted_movies)  # ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "# ê°œë³„ ì‚¬ìš©ì ë©”íƒ€ ì²­í¬ ë°ì´í„°ì™€ í•©ì¹˜ê¸°\n",
    "def merge_summaries(cluster_summaries, cluster_movies, individual_summaries, individual_movies):\n",
    "    \"\"\"\n",
    "    í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¶”ì¶œí•œ ì˜í™” ìš”ì•½ê³¼ ê°œë³„ ì‚¬ìš©ìì˜ ìš”ì•½ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ë©°,\n",
    "    ê° ì‚¬ìš©ìê°€ ì‹œì²­í•œ ì˜í™” ID ëª©ë¡ë„ í•¨ê»˜ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    formatted_cluster_summary = format_cluster_summaries(cluster_summaries)\n",
    "\n",
    "    # ê°œë³„ ì‚¬ìš©ì ìš”ì•½ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³‘í•©\n",
    "    formatted_individual_summary = \" \".join(individual_summaries.values())\n",
    "\n",
    "    # ë‘ ìš”ì•½ì„ í•©ì¹¨\n",
    "    final_summary = f\"{formatted_cluster_summary} {formatted_individual_summary}\"\n",
    "\n",
    "    # ì‚¬ìš©ìê°€ ë³¸ ì˜í™” ID ì •ë³´ ë³‘í•©\n",
    "    merged_movie_ids = {**cluster_movies, **individual_movies}\n",
    "\n",
    "    return final_summary, merged_movie_ids\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# LangChain í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "template = \"\"\"\n",
    "## item ##\n",
    "Here is the summary of frequently watched movies from other users:\n",
    "{record_summary}\n",
    "\n",
    "## role ##\n",
    "Based on the provided watching record summary ({record_summary}), recommend exactly 10 movies for the user to watch next.\n",
    "You must prioritize movies that:\n",
    "1. Appear frequently in the given watching record summary ({record_summary}).\n",
    "\n",
    "For each recommended movie, provide a brief reason why it is recommended. The reason must include:\n",
    "- How frequently the movie appears in the record summary.\n",
    "- Why it might be appealing to the user based on similar preferences.\n",
    "\n",
    "Provide the recommendations as a numbered list in the following format:\n",
    "1. Movie ID - Reason\n",
    "2. Movie ID - Reason\n",
    "...\n",
    "10. Movie ID - Reason\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer(purchase_history, final_summary) :\n",
    "    # OpenAI ëª¨ë¸ ì„¤ì • (LangChain ìµœì‹  ë°©ì‹)\n",
    "    response= chain.invoke({\"purchase_history\":purchase_history, \"record_summary\": final_summary })\n",
    "    return response\n",
    "\n",
    "# # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "# chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>movie_explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"48 (Animation|Children's|Musical|Romance) ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['1917 (Action|Adventure|Sci-Fi|Thriller) rati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"2081 (Animation|Children's|Comedy|Musical|Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['1954 (Action|Drama) ratings: 5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['288 (Action|Thriller) ratings: 2']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId                                      movie_explain\n",
       "0       1  [\"48 (Animation|Children's|Musical|Romance) ra...\n",
       "1       2  ['1917 (Action|Adventure|Sci-Fi|Thriller) rati...\n",
       "2       3  [\"2081 (Animation|Children's|Comedy|Musical|Ro...\n",
       "3       4                 ['1954 (Action|Drama) ratings: 5']\n",
       "4       5               ['288 (Action|Thriller) ratings: 2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_test = pd.read_csv('./data/test_movie.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch_geometric in /home/rlaalsduf/.local/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (2.2.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: aiohttp in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (3.11.14)\n",
      "Requirement already satisfied: tqdm in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/rlaalsduf/.local/lib/python3.10/site-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/rlaalsduf/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_user_movies(data):\n",
    "    user_movies = defaultdict(list)\n",
    "\n",
    "    for doc in data:\n",
    "        user_id = doc.metadata['UserId']\n",
    "        page_content = doc.page_content\n",
    "\n",
    "        # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì˜í™” ID ì¶”ì¶œ (ratings ì´ì „ ë‚´ìš©ë§Œ)\n",
    "        movie_ids = re.findall(r'(\\d+)(?= \\()', page_content)\n",
    "\n",
    "        # ì‚¬ìš©ìë³„ë¡œ ì˜í™” ID ì¶”ê°€\n",
    "        user_movies[user_id].extend(movie_ids)\n",
    "\n",
    "    return user_movies\n",
    "\n",
    "def create_user_movie_graph(user_movies):\n",
    "    # ê·¸ë˜í”„ ìƒì„±\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ì¶”ê°€ ë° ì—£ì§€ ìƒì„±\n",
    "    for user_id, movies in user_movies.items():\n",
    "        user_node = f\"User {user_id}\"\n",
    "        G.add_node(user_node, type='user')\n",
    "\n",
    "        for movie_id in movies:\n",
    "            movie_node = f\"Movie {movie_id}\"\n",
    "            G.add_node(movie_node, type='movie')\n",
    "            G.add_edge(user_node, movie_node)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def visualize_graph(G):\n",
    "    # ê·¸ë˜í”„ ì‹œê°í™”\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ë¶„ë¦¬\n",
    "    users = [node for node in G.nodes() if G.nodes[node]['type'] == 'user']\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "    # ë…¸ë“œ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=users, node_color='lightblue', node_size=300, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=movies, node_color='lightgreen', node_size=200, alpha=0.8)\n",
    "\n",
    "    # ì—£ì§€ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "    # ë ˆì´ë¸” ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(\"User-Movie Relationship Graph\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ê·¸ë˜í”„ ì €ì¥\n",
    "    plt.savefig('user_movie_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "import re\n",
    "\n",
    "# ê³¼ê±° ê¸°ë¡ì—ì„œ ì˜í™” ID ì¶”ì¶œ\n",
    "def extract_previous_movie_ids(purchase_history):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œ (ìˆ«ìì™€ ê´„í˜¸ ì „ê¹Œì§€ë§Œ ê°€ì ¸ì˜´)\n",
    "    previous_movies = re.findall(r'(\\d+)(?=\\s\\()', ' '.join(purchase_history))\n",
    "    return set(previous_movies)\n",
    "\n",
    "# ë™ì‹œ ì‹œì²­ ì˜í™”ì—ì„œ ì´ì „ ê¸°ë¡ì„ ì œì™¸í•˜ëŠ” í•¨ìˆ˜\n",
    "def filter_movies_by_history(top_movies, previous_movie_ids):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ í›„ ì œì™¸\n",
    "    filtered_movies = [(movie, count) for movie, count in top_movies if movie.split()[1] not in previous_movie_ids]\n",
    "    return filtered_movies\n",
    "def get_top_10_common_movies(G):\n",
    "    # ì˜í™” ë…¸ë“œë§Œ í•„í„°ë§\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ì˜í™”ë³„ ì—°ê²°ëœ ì‚¬ìš©ì ìˆ˜ ê³„ì‚°\n",
    "    movie_view_counts = {movie: len(list(G.neighbors(movie))) for movie in movies}\n",
    "\n",
    "    # ì‚¬ìš©ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ 10ê°œ ì¶”ì¶œ\n",
    "    top_10_movies = sorted(movie_view_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    # ê²°ê³¼ ë°˜í™˜\n",
    "    return top_10_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6040/6040 [5:06:11<00:00,  3.04s/user]  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "# ğŸš€ ì „ì²´ ë¬¸ì„œ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ìºì‹±\n",
    "all_docs = list(new_vectorstore.docstore._dict.values())\n",
    "\n",
    "# # ğŸš€ ì‚¬ìš©ì-ì˜í™” ê´€ê³„ ë°ì´í„° ë¯¸ë¦¬ ìƒì„±\n",
    "# user_movies = get_user_movies(all_docs)\n",
    "\n",
    "# # ğŸš€ PyTorch Geometric ê·¸ë˜í”„ ìƒì„± (GPU ì‚¬ìš©)\n",
    "# G = create_user_movie_graph_fast(user_movies)\n",
    "\n",
    "# 1ë¶€í„° 6040ê¹Œì§€ ë°˜ë³µí•˜ë©´ì„œ ì§„í–‰ë¥  í‘œì‹œ (tqdm ì‚¬ìš©)\n",
    "for idx in tqdm(range(6040), desc=\"Processing users\", unit=\"user\"):\n",
    "\n",
    "    # ì •ë‹µ ì¶”ì¶œ\n",
    "    test_answer = df_test.iloc[idx]['movie_explain']\n",
    "\n",
    "    # movie_name ì¶”ì¶œ\n",
    "    try:\n",
    "        movie_name = re.search(r\"\\d+\", test_answer).group()\n",
    "    except AttributeError:\n",
    "        movie_name = None  # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì²˜ë¦¬\n",
    "\n",
    "    # ì¿¼ë¦¬ì— ë”°ë¥¸ ìµœê·¼ì ‘ ì´ì›ƒ ì¶”ì¶œ + ì¿¼ë¦¬ ì‚¬ìš©ìì˜ ìµœì‹  ìƒí˜¸ì‘ìš© ì¶”ì¶œ\n",
    "    result_df, purchase_history = extract_nearest(idx)\n",
    "    cluster_ids, individual_ids = classification_ids(result_df)\n",
    "\n",
    "    # ğŸš€ í´ëŸ¬ìŠ¤í„° & ê°œë³„ ì‚¬ìš©ì ë°ì´í„°ë¥¼ í•œ ë²ˆì— ìºì‹±í•˜ì—¬ ì²˜ë¦¬\n",
    "    all_docs = list(new_vectorstore.docstore._dict.values())\n",
    "\n",
    "    # í´ëŸ¬ìŠ¤í„° ë° ê°œë³„ ì‚¬ìš©ìì˜ ìš”ì•½ ë° ì‹œì²­ ì˜í™” ID ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    cluster_summaries, cluster_movies = summarize_clusters(cluster_ids, cluster_mapping, new_vectorstore)\n",
    "    individual_summaries, individual_movies = summarize_individuals(individual_ids, new_vectorstore, purchase_history, embeddings)\n",
    "\n",
    "    # ìµœì¢… ìš”ì•½ê³¼ ì‚¬ìš©ìì˜ ì‹œì²­ ì˜í™” ID ëª©ë¡ ë³‘í•©\n",
    "    final_summary, merged_movie_ids = merge_summaries(cluster_summaries, cluster_movies, individual_summaries, individual_movies)\n",
    "\n",
    "    # ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "    user_movies = get_user_movies(new_vectorstore.docstore._dict.values())\n",
    "\n",
    "    G = create_user_movie_graph(user_movies)\n",
    "\n",
    "    # ì´ìš©ìì˜ ê³¼ê±° ì‹œì²­ ì˜í™” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "    previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "    # Top-10 ê°€ì¥ ë§ì´ ë³¸ ì˜í™” ê°€ì ¸ì˜¤ê¸°\n",
    "    top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "    # ê³¼ê±° ì‹œì²­í•œ ì˜í™” ì œì™¸ í›„ ì¶”ì²œ ì˜í™” í•„í„°ë§\n",
    "    filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ ì˜í™” ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì •ê·œì‹ ì ìš©\n",
    "    movie_ids = [re.search(r'(\\d+)', movie[0]).group() for movie in filtered_movies if re.search(r'(\\d+)', movie[0])]\n",
    "\n",
    "    # movie_nameì´ movie_titlesì— í¬í•¨ë˜ëŠ”ì§€ ì—¬ë¶€\n",
    "    is_included = movie_name in movie_ids if movie_name else False\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results.append({\n",
    "        \"movie_title\": movie_ids,\n",
    "        \"movie_name\": movie_name,\n",
    "        \"is_included\": is_included\n",
    "    })\n",
    "\n",
    "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "final_results = pd.DataFrame(results)\n",
    "final_results.head()\n",
    "\n",
    "final_results.to_csv('graph_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@10: 0.030794701986754967\n"
     ]
    }
   ],
   "source": [
    "# Hit@k ê³„ì‚° (k=10)\n",
    "k = 10\n",
    "final_results['hit'] = final_results.apply(lambda row: row['movie_name'] in row['movie_title'][:k], axis=1)\n",
    "\n",
    "# ì „ì²´ Hit@k ê³„ì‚°\n",
    "hit_at_k = final_results['hit'].mean()  # Hit@k ë¹„ìœ¨\n",
    "print(f\"Hit@{k}: {hit_at_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
