{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# os.chdir('/Users/mac/AIworkspace/LLMWORKSPACE/RAG_Rec')\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜í™”ê¸°ë¡ ë°ì´í„° \n",
    "import pandas as pd\n",
    "file_path = \"data/movies.dat\"\n",
    "df2 = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df2.columns = [\"MovieID\", \"Title\", \"Genres\"]\n",
    "\n",
    "file_path = \"data/ratings.dat\"\n",
    "df = pd.read_csv(file_path, delimiter=\"::\", engine=\"python\", header=None,encoding=\"latin1\")\n",
    "df.columns = [\"UserId\", \"MovieID\", \"Ratings\",\"timestamp\"]\n",
    "new_df=df.merge(df2, on='MovieID')\n",
    "df_sorted = new_df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. ì‚¬ìš©ìë³„ interaction ë¦¬ìŠ¤íŠ¸ ìƒì„± ---\n",
    "df_sorted['interaction'] = df_sorted.apply(\n",
    "    lambda row: f\"{row['Genres']} (Rating: {row['Ratings']})\", axis=1\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ìë³„ interaction ì—°ê²° (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)\n",
    "user_interactions = df_sorted.groupby('UserId')['interaction'].apply(list).reset_index()\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "user_interactions.columns = ['UserId', 'interaction_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('header_vanila.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "class GMMClusterer:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_predict(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:\n",
    "        # ğŸ”¥ ë§¤ ë ˆë²¨ë§ˆë‹¤ n_clustersë¥¼ ì „ë‹¬í•˜ë„ë¡ ë³€ê²½\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=self.random_state)\n",
    "        return gmm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaptorTree:\n",
    "    def __init__(self, embedding_generator, clusterer, min_clusters=2, max_level=5, top_level_clusters=100):\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.clusterer = clusterer\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_level = max_level\n",
    "        self.top_level_clusters = top_level_clusters\n",
    "        self.tree = {}\n",
    "        self.user_id_to_text = {}\n",
    "\n",
    "    def build_tree(self, texts: list[str], user_ids: list[str]):\n",
    "        self.user_id_to_text = dict(zip(user_ids, texts))\n",
    "        current_texts = texts\n",
    "        current_user_ids = user_ids\n",
    "        current_level = 0\n",
    "        parent_ids = None\n",
    "\n",
    "        while len(current_texts) > 1 and current_level < self.max_level:\n",
    "            embeddings = self.embedding_generator.embed_texts(current_texts)\n",
    "\n",
    "            # ğŸ”¥ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì„¤ì • ìˆ˜ì • (ë¹ˆ í´ëŸ¬ìŠ¤í„° ë°©ì§€)\n",
    "            n_clusters = max(self.min_clusters, min(len(current_texts) // 2, self.top_level_clusters // (current_level + 1)))\n",
    "\n",
    "            cluster_labels = self.clusterer.fit_predict(embeddings, n_clusters=n_clusters)\n",
    "\n",
    "            cluster_metadata = []\n",
    "            next_level_texts = []\n",
    "            next_level_user_ids = []\n",
    "\n",
    "            for cluster_id in np.unique(cluster_labels):\n",
    "                cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "                cluster_texts = [current_texts[i] for i in cluster_indices]\n",
    "\n",
    "                # ğŸ”¥ ë¹ˆ í´ëŸ¬ìŠ¤í„° ì œê±°\n",
    "                if len(cluster_texts) == 0:\n",
    "                    continue\n",
    "\n",
    "                # ğŸ”¥ ì²« ë²ˆì§¸ ë ˆë²¨ì—ì„œëŠ” user_ids ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                if current_level == 0:\n",
    "                    cluster_user_ids = [current_user_ids[i] for i in cluster_indices]\n",
    "                else:\n",
    "                    # ìƒìœ„ ë ˆë²¨ì—ì„œëŠ” ì´ì „ í´ëŸ¬ìŠ¤í„° ID ê¸°ì¤€ìœ¼ë¡œ `user_ids` ì¶”ê°€\n",
    "                    cluster_user_ids = []\n",
    "                    for idx in cluster_indices:\n",
    "                        child_cluster_id = current_user_ids[idx]\n",
    "                        for child_meta in self.tree[current_level - 1]:\n",
    "                            if child_meta[\"cluster_id\"] == child_cluster_id:\n",
    "                                cluster_user_ids.extend(child_meta[\"user_ids\"])\n",
    "\n",
    "                # ğŸ”¥ ëŒ€í‘œ í…ìŠ¤íŠ¸ (ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì„ ì •)\n",
    "                representative_text = max(cluster_texts, key=len)\n",
    "\n",
    "                cluster_embeddings = embeddings[cluster_indices]\n",
    "                mean_embedding = cluster_embeddings.mean(axis=0)\n",
    "\n",
    "                # ğŸ”¥ ì˜¬ë°”ë¥¸ parent_id ì„¤ì • (ë¶€ëª¨ í´ëŸ¬ìŠ¤í„° 1ê°œë§Œ ì¶”ê°€)\n",
    "                metadata = {\n",
    "                    \"cluster_id\": f\"level_{current_level}_cluster_{cluster_id}\",\n",
    "                    \"level\": current_level,\n",
    "                    \"user_ids\": cluster_user_ids,  # âœ… ì²« ë²ˆì§¸ ë ˆë²¨ì—ì„œë„ user_ids ì¶”ê°€\n",
    "                    \"embedding\": mean_embedding,\n",
    "                    \"parent_id\": parent_ids if parent_ids else [],\n",
    "                    \"child_ids\": None\n",
    "                }\n",
    "\n",
    "                cluster_metadata.append(metadata)\n",
    "                next_level_texts.append(representative_text)\n",
    "                next_level_user_ids.append(metadata[\"cluster_id\"])\n",
    "\n",
    "            # í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "            self.tree[current_level] = cluster_metadata\n",
    "            current_texts = next_level_texts\n",
    "            current_user_ids = next_level_user_ids\n",
    "            parent_ids = current_user_ids\n",
    "            current_level += 1\n",
    "\n",
    "        return self.tree\n",
    "\n",
    "    def search_user_cluster(self, target_user_id: str, target_user_text: str, threshold=0.01):\n",
    "        query_embedding = self.embedding_generator.embed_texts([target_user_text])[0]\n",
    "        current_level = max(self.tree.keys())\n",
    "        previous_similarity = None\n",
    "        best_cluster = None\n",
    "\n",
    "        while current_level >= 0:\n",
    "            clusters = self.tree[current_level]\n",
    "            clusters_filtered = []\n",
    "            clusters_filtered_embeddings = []\n",
    "\n",
    "            for cluster in clusters:\n",
    "                cluster_user_ids = cluster[\"user_ids\"]\n",
    "\n",
    "                # ğŸ”¥ í´ëŸ¬ìŠ¤í„° IDê°€ ì•„ë‹Œ ì‹¤ì œ ìœ ì € IDë§Œ í•„í„°ë§\n",
    "                texts_to_embed = [\n",
    "                    self.user_id_to_text[uid] for uid in cluster_user_ids\n",
    "                    if uid != target_user_id and 'cluster' not in uid\n",
    "                ]\n",
    "                if not texts_to_embed:\n",
    "                    continue\n",
    "\n",
    "                embeddings_cluster = self.embedding_generator.embed_texts(texts_to_embed)\n",
    "                mean_embedding = embeddings_cluster.mean(axis=0)\n",
    "\n",
    "                clusters_filtered.append(cluster)\n",
    "                clusters_filtered_embeddings.append(mean_embedding)\n",
    "\n",
    "            if not clusters_filtered:\n",
    "                break\n",
    "\n",
    "            similarities = cosine_similarity([query_embedding], clusters_filtered_embeddings).flatten()\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_cluster = clusters_filtered[best_idx]\n",
    "            current_similarity = similarities[best_idx]\n",
    "\n",
    "            if previous_similarity and abs(previous_similarity - current_similarity) / previous_similarity > threshold:\n",
    "                break\n",
    "\n",
    "            previous_similarity = current_similarity\n",
    "            current_level -= 1\n",
    "\n",
    "        best_cluster_users_excluded = [uid for uid in best_cluster[\"user_ids\"] if uid != target_user_id]\n",
    "        return best_cluster[\"cluster_id\"], best_cluster_users_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlaalsduf/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-27 06:48:52.050311: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-27 06:48:52.066557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743025732.083691 2719886 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743025732.088677 2719886 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743025732.101687 2719886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743025732.101714 2719886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743025732.101716 2719886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743025732.101717 2719886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-27 06:48:52.105977: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìœ ì € 2ê°€ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°: level_2_cluster_16\n",
      "í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ìœ ì‚¬ ìœ ì € ëª©ë¡ (ë³¸ì¸ ì œì™¸): ['46', '127', '304', '1089', '1302', '1424', '1610', '1726', '1927', '2101', '2126', '2146', '2263', '2297', '2363', '2441', '2514', '3144', '3352', '3460', '3567', '3636', '3673', '4000', '4031', '4109', '4160', '4587', '4690', '4874', '4892', '5040', '5069', '5469', '5703', '5895', '6034', '7', '277', '296', '431', '908', '1030', '1131', '1200', '1398', '1520', '1649', '1866', '2466', '2598', '2663', '3048', '3068', '3307', '3337', '3459', '3461', '3487', '3662', '3818', '3978', '4093', '4183', '4417', '4489', '4499', '4626', '5003', '5029', '5095', '5854', '5870', '5871', '5884', '5912', '5947', '6020', '185', '279', '422', '542', '633', '677', '700', '917', '1260', '1269', '1533', '1548', '1567', '1682', '2062', '2114', '2163', '2211', '2357', '2369', '2388', '2479', '2625', '2642', '2767', '2967', '3027', '3162', '3187', '3455', '3490', '3616', '3733', '3822', '3828', '3855', '4200', '4207', '4531', '4579', '5309', '5389', '5737', '5749', '5859']\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì„í¬íŠ¸ ë¯¸ë¦¬ ì¤€ë¹„\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# ë§Œë“¤ì–´ì§„ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device= device)\n",
    "    \n",
    "    def embed_texts(self, texts: list[str]) -> np.ndarray:\n",
    "        # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "        return self.model.encode(texts, convert_to_numpy=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ì´ˆê¸°í™” ì˜ˆì‹œ\n",
    "embedding_gen = EmbeddingGenerator()\n",
    "clusterer = GMMClusterer()\n",
    "\n",
    "# ì´ˆê¸° RAPTOR íŠ¸ë¦¬ ìƒì„± (í•œ ë²ˆë§Œ ìˆ˜í–‰)\n",
    "# í´ë˜ìŠ¤ ì´ˆê¸°í™” ë¶€ë¶„\n",
    "raptor_tree = RaptorTree(\n",
    "    embedding_gen, \n",
    "    clusterer,\n",
    "    min_clusters=2,\n",
    "    max_level=5,          # ë” ë†’ì´ê±°ë‚˜ ë‚®ì¶°ì„œ ì¡°ì •\n",
    "    top_level_clusters=100 # ìµœìƒìœ„ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì—¬ë³¸ë‹¤\n",
    ")\n",
    "tree_structure = raptor_tree.build_tree(df.chunk_header.tolist(), df.UserId.astype(str).tolist())\n",
    "\n",
    "# íŠ¹ì • ì‚¬ìš©ì ê²€ìƒ‰ ìˆ˜í–‰ (Self-exclusion ë°©ì‹ ì ìš©)\n",
    "target_user_id = \"2\"\n",
    "target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "    target_user_id, target_user_text, threshold=0.005\n",
    ")\n",
    "\n",
    "print(f\"ìœ ì € {target_user_id}ê°€ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°: {best_cluster_id}\")\n",
    "print(f\"í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ìœ ì‚¬ ìœ ì € ëª©ë¡ (ë³¸ì¸ ì œì™¸): {similar_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2719886/1377154823.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"vectorstore_index_ratings_min5_no\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # ì—­ì§ë ¬í™” í—ˆìš©\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1917 (Action|Adventure|Sci-Fi|Thriller) ratings: 3']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "file_path='data/train_movie.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# 'movie_explain' ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "data['movie_explain'] = data['movie_explain'].apply(ast.literal_eval)\n",
    "file_path_test='data/test_movie.csv'\n",
    "# ìµœì‹  êµ¬ë§¤ ê¸°ë¡ì„ ê°€ì ¸ì˜´\n",
    "purchase_history=data.iloc[1]['movie_explain']\n",
    "# ì •ë‹µ ë°ì´í„°ì…‹ ê°€ì ¸ì˜´\n",
    "df_test=pd.read_csv(file_path_test)\n",
    "# ì •ë‹µ\n",
    "df_test.iloc[1].movie_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2719886/3104281221.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  records = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ìµœì‹  êµ¬ë§¤ ê¸°ë¡ (Query)\n",
    "query = \" \".join(purchase_history[-1:])  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "# ğŸ”¹ similar_usersì˜ íƒ€ì… í™•ì¸ í›„ ë³€í™˜\n",
    "similar_users_str = set(map(str, similar_users))  # ë¬¸ìì—´ ë³€í™˜\n",
    "\n",
    "# ğŸ”¹ FAISSì—ì„œ `similar_users`ë§Œ ê²€ìƒ‰í•˜ë„ë¡ í•„í„° ì¶”ê°€\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 500,\n",
    "        # \"filter\": user_filter  # ğŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "    }\n",
    ")\n",
    "\n",
    "# ğŸ”¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "records = retriever.get_relevant_documents(query)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "intersection = similar_users_str.intersection(set(record_user_ids))\n",
    "\n",
    "# ğŸ”¹ 'similar_users'ì— ì†í•˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§\n",
    "filtered_records = [\n",
    "    record for record in records \n",
    "    if str(record.metadata['UserId']) in similar_users_str\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_documents_with_context(\n",
    "    vectorstore: FAISS,\n",
    "    filtered_records: List[Document],\n",
    "    context_window: int = 1\n",
    ") -> List[List[Document]]:\n",
    "    \"\"\"\n",
    "    intersection ìœ ì €ë“¤ì˜ ê²€ìƒ‰ ê²°ê³¼ì™€ ê° ê²°ê³¼ì˜ ì•ë’¤ ë¬¸ì„œë“¤ì„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤\n",
    "        filtered_records: ê²€ìƒ‰ëœ ìœ ì €ë“¤ì˜ ê¸°ë¡\n",
    "        context_window: ì•ë’¤ë¡œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜ (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        List[List[Document]]: ê° ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ [ì´ì „ ë¬¸ì„œë“¤, í˜„ì¬ ë¬¸ì„œ, ë‹¤ìŒ ë¬¸ì„œë“¤]ì„ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    # ğŸ”¹ ëª¨ë“  ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±\n",
    "    all_docs = {}\n",
    "    for doc_id, doc in enumerate(vectorstore.docstore._dict.values()):\n",
    "        user_id = str(doc.metadata['UserId'])\n",
    "        chunk_idx = doc.metadata['chunk_index']\n",
    "        \n",
    "        if user_id not in all_docs:\n",
    "            all_docs[user_id] = {}\n",
    "        all_docs[user_id][chunk_idx] = doc\n",
    "    \n",
    "    # ğŸ”¹ Context ì¶”ê°€\n",
    "    context_results = []\n",
    "    \n",
    "    for doc in filtered_records:\n",
    "        current_user_id = str(doc.metadata['UserId'])\n",
    "        current_chunk_index = doc.metadata['chunk_index']\n",
    "        \n",
    "        context_docs = []\n",
    "        \n",
    "        # ğŸ”¹ ì´ì „ ë¬¸ì„œë“¤ ì¶”ê°€\n",
    "        for i in range(current_chunk_index - context_window, current_chunk_index):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        # ğŸ”¹ í˜„ì¬ ë¬¸ì„œ ì¶”ê°€\n",
    "        context_docs.append(doc)\n",
    "        \n",
    "        # ğŸ”¹ ë‹¤ìŒ ë¬¸ì„œë“¤ ì¶”ê°€\n",
    "        for i in range(current_chunk_index + 1, current_chunk_index + context_window + 1):\n",
    "            if current_user_id in all_docs and i in all_docs[current_user_id]:\n",
    "                context_docs.append(all_docs[current_user_id][i])\n",
    "        \n",
    "        context_results.append(context_docs)\n",
    "    \n",
    "    return context_results\n",
    "\n",
    "# ğŸ”¹ intersection ìœ ì €ë“¤ì˜ ì²­í¬ ì•ë’¤ ì²­í¬ ì¶”ì¶œ\n",
    "context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 788: watched by 3 users\n",
      "Movie 1573: watched by 3 users\n",
      "Movie 2094: watched by 2 users\n",
      "Movie 1320: watched by 2 users\n",
      "Movie 329: watched by 2 users\n",
      "Movie 1917: watched by 2 users\n",
      "Movie 1779: watched by 2 users\n",
      "Movie 173: watched by 2 users\n",
      "Movie 3354: watched by 2 users\n",
      "Movie 880: watched by 2 users\n",
      "Movie 748: watched by 2 users\n",
      "Movie 1580: watched by 2 users\n",
      "Movie 1589: watched by 2 users\n",
      "Movie 1748: watched by 2 users\n",
      "Movie 802: watched by 2 users\n",
      "Movie 3020: watched by 1 users\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ intersection ìœ ì €ë“¤ì˜ ì²­í¬ ì•ë’¤ ì²­í¬ ì¶”ì¶œ\n",
    "context_results = get_documents_with_context(vectorstore, filtered_records, context_window=1)\n",
    "\n",
    "flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "# ì‚¬ìš©ìë³„ ì˜í™” ID ê°€ì ¸ì˜¤ê¸°\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# Top-10 ì˜í™” ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# ì´ì „ ê¸°ë¡ì— í¬í•¨ë˜ì§€ ì•Šì€ ì˜í™”ë§Œ í•„í„°ë§\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie 788: watched by 3 users\n",
      "Movie 1573: watched by 3 users\n",
      "Movie 2094: watched by 2 users\n",
      "Movie 1320: watched by 2 users\n",
      "Movie 329: watched by 2 users\n",
      "Movie 1917: watched by 2 users\n",
      "Movie 1779: watched by 2 users\n",
      "Movie 173: watched by 2 users\n",
      "Movie 3354: watched by 2 users\n",
      "Movie 880: watched by 2 users\n",
      "Movie 748: watched by 2 users\n",
      "Movie 1580: watched by 2 users\n",
      "Movie 1589: watched by 2 users\n",
      "Movie 1748: watched by 2 users\n",
      "Movie 802: watched by 2 users\n",
      "Movie 3020: watched by 1 users\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_user_movies(data):\n",
    "    user_movies = defaultdict(list)\n",
    "\n",
    "    for doc in data:\n",
    "        user_id = doc.metadata['UserId']\n",
    "        page_content = doc.page_content\n",
    "\n",
    "        # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì˜í™” ID ì¶”ì¶œ (ratings ì´ì „ ë‚´ìš©ë§Œ)\n",
    "        movie_ids = re.findall(r'(\\d+)(?= \\()', page_content)\n",
    "\n",
    "        # ì‚¬ìš©ìë³„ë¡œ ì˜í™” ID ì¶”ê°€\n",
    "        user_movies[user_id].extend(movie_ids)\n",
    "\n",
    "    return user_movies\n",
    "\n",
    "def create_user_movie_graph(user_movies):\n",
    "    # ê·¸ë˜í”„ ìƒì„±\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ì¶”ê°€ ë° ì—£ì§€ ìƒì„±\n",
    "    for user_id, movies in user_movies.items():\n",
    "        user_node = f\"User {user_id}\"\n",
    "        G.add_node(user_node, type='user')\n",
    "\n",
    "        for movie_id in movies:\n",
    "            movie_node = f\"Movie {movie_id}\"\n",
    "            G.add_node(movie_node, type='movie')\n",
    "            G.add_edge(user_node, movie_node)\n",
    "\n",
    "    return G\n",
    "\n",
    "def visualize_graph(G):\n",
    "    # ê·¸ë˜í”„ ì‹œê°í™”\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # ì‚¬ìš©ìì™€ ì˜í™” ë…¸ë“œ ë¶„ë¦¬\n",
    "    users = [node for node in G.nodes() if G.nodes[node]['type'] == 'user']\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "    # ë…¸ë“œ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=users, node_color='lightblue', node_size=300, alpha=0.8)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=movies, node_color='lightgreen', node_size=200, alpha=0.8)\n",
    "\n",
    "    # ì—£ì§€ ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "    # ë ˆì´ë¸” ê·¸ë¦¬ê¸°\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(\"User-Movie Relationship Graph\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ê·¸ë˜í”„ ì €ì¥\n",
    "    plt.savefig('user_movie_graph.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "import re\n",
    "\n",
    "# ê³¼ê±° ê¸°ë¡ì—ì„œ ì˜í™” ID ì¶”ì¶œ\n",
    "def extract_previous_movie_ids(purchase_history):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œ (ìˆ«ìì™€ ê´„í˜¸ ì „ê¹Œì§€ë§Œ ê°€ì ¸ì˜´)\n",
    "    previous_movies = re.findall(r'(\\d+)(?=\\s\\()', ' '.join(purchase_history))\n",
    "    return set(previous_movies)\n",
    "\n",
    "# ë™ì‹œ ì‹œì²­ ì˜í™”ì—ì„œ ì´ì „ ê¸°ë¡ì„ ì œì™¸í•˜ëŠ” í•¨ìˆ˜\n",
    "def filter_movies_by_history(top_movies, previous_movie_ids):\n",
    "    # ì˜í™” IDë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ í›„ ì œì™¸\n",
    "    filtered_movies = [(movie, count) for movie, count in top_movies if movie.split()[1] not in previous_movie_ids]\n",
    "    return filtered_movies\n",
    "def get_top_10_common_movies(G):\n",
    "    # ì˜í™” ë…¸ë“œë§Œ í•„í„°ë§\n",
    "    movies = [node for node in G.nodes() if G.nodes[node]['type'] == 'movie']\n",
    "\n",
    "    # ì˜í™”ë³„ ì—°ê²°ëœ ì‚¬ìš©ì ìˆ˜ ê³„ì‚°\n",
    "    movie_view_counts = {movie: len(list(G.neighbors(movie))) for movie in movies}\n",
    "\n",
    "    # ì‚¬ìš©ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ 10ê°œ ì¶”ì¶œ\n",
    "    top_10_movies = sorted(movie_view_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    # ê²°ê³¼ ë°˜í™˜\n",
    "    return top_10_movies\n",
    "\n",
    "# ì‚¬ìš©ìë³„ ì˜í™” ID ê°€ì ¸ì˜¤ê¸°\n",
    "user_movies = get_user_movies(flattened_results)\n",
    "\n",
    "# ì‚¬ìš©ì-ì˜í™” ê·¸ë˜í”„ ìƒì„±\n",
    "G = create_user_movie_graph(user_movies)\n",
    "\n",
    "previous_movie_ids = extract_previous_movie_ids(purchase_history)\n",
    "\n",
    "# Top-10 ì˜í™” ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "# ì´ì „ ê¸°ë¡ì— í¬í•¨ë˜ì§€ ì•Šì€ ì˜í™”ë§Œ í•„í„°ë§\n",
    "filtered_movies = filter_movies_by_history(top_movies, previous_movie_ids)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for movie, count in filtered_movies:\n",
    "    print(f\"{movie}: watched by {count} users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2304214/1764082392.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_results = pd.concat([all_results, results], ignore_index=True)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [5:58:12<00:00,  7.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 0.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# (1) ì‹¤í—˜ íŒŒë¼ë¯¸í„° (ê³ ì •ê°’)\n",
    "threshold = 0.007\n",
    "k_val = 750\n",
    "window_size = 1\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© DataFrame\n",
    "all_results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\",\n",
    "                                    \"threshold\", \"faiss_k\", \"window\"])\n",
    "\n",
    "# (2) ìœ ì € ë‹¨ì¼ ì„¸íŒ… ì‹¤í—˜ (ì˜ˆ: 5ëª… ìœ ì € ë°˜ë³µ)\n",
    "for idx in tqdm(range(3000)):\n",
    "    results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "    target_user_id = str(idx + 1)  # ìœ ì € IDëŠ” 1ë¶€í„° ì‹œì‘\n",
    "    target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "    # (2-1) Raptor Tree ê²€ìƒ‰\n",
    "    best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "        target_user_id,\n",
    "        target_user_text,\n",
    "        threshold=threshold\n",
    "    )\n",
    "\n",
    "    # ìµœì‹  êµ¬ë§¤ ê¸°ë¡ì—ì„œ ë§ˆì§€ë§‰ í•­ëª©ì„ ì§ˆì˜ë¡œ ì‚¬ìš©\n",
    "    purchase_history = data.iloc[idx]['movie_explain']\n",
    "    query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "    # (2-2) FAISS ê²€ìƒ‰\n",
    "    retriever_k = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": k_val}\n",
    "    )\n",
    "    records = retriever_k.get_relevant_documents(query)\n",
    "\n",
    "    record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "    intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "\n",
    "    filtered_records = [\n",
    "        record for record in records\n",
    "        if str(record.metadata['UserId']) in intersection\n",
    "    ]\n",
    "\n",
    "    # (2-3) ë¬¸ë§¥ í™•ì¥ (window size ì ìš©)\n",
    "    context_results = get_documents_with_context(\n",
    "        vectorstore,\n",
    "        filtered_records,\n",
    "        context_window=window_size\n",
    "    )\n",
    "    flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "    # ìœ ì €-ì˜í™” ê·¸ë˜í”„ ìƒì„± ë° ì¶”ì²œ ì˜í™” ì¶”ì¶œ\n",
    "    user_movies = get_user_movies(flattened_results)\n",
    "    G = create_user_movie_graph(user_movies)\n",
    "    top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "    # ê³¼ê±° ê¸°ë¡ ì œê±°\n",
    "    filtered_movies = filter_movies_by_history(\n",
    "        top_movies,\n",
    "        extract_previous_movie_ids(purchase_history)\n",
    "    )\n",
    "\n",
    "    # ì •ë‹µ ë° ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ\n",
    "    answer = df_test.iloc[idx].movie_explain\n",
    "    match = re.search(r\"(\\d+)\", answer)\n",
    "    answer_id = match.group(1) if match else None\n",
    "\n",
    "    filtered_movie_ids = [\n",
    "        re.search(r\"(\\d+)\", movie).group(1)\n",
    "        for movie, _ in filtered_movies\n",
    "        if re.search(r\"(\\d+)\", movie)\n",
    "    ]\n",
    "\n",
    "    hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "    # ìœ ì €ë³„ ê²°ê³¼ ì €ì¥\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        \"UserId\": [target_user_id],\n",
    "        \"Hit\": [hit],\n",
    "        \"Answer\": [answer_id],\n",
    "        \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ê°€\n",
    "    results[\"threshold\"] = threshold\n",
    "    results[\"faiss_k\"] = k_val\n",
    "    results[\"window\"] = window_size\n",
    "\n",
    "    all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "\n",
    "# (3) ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
    "performance = all_results[\"Hit\"].mean()\n",
    "print(f\"Hit Rate: {performance:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3040 [00:00<?, ?it/s]/tmp/ipykernel_2719886/1084212833.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_results = pd.concat([all_results, results], ignore_index=True)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3039/3040 [5:53:04<00:06,  6.97s/it]  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     18\u001b[0m target_user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# ìœ ì € IDëŠ” 1ë¶€í„° ì‹œì‘\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m target_user_text \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserId\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarget_user_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_header\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# (2-1) Raptor Tree ê²€ìƒ‰\u001b[39;00m\n\u001b[1;32m     22\u001b[0m best_cluster_id, similar_users \u001b[38;5;241m=\u001b[39m raptor_tree\u001b[38;5;241m.\u001b[39msearch_user_cluster(\n\u001b[1;32m     23\u001b[0m     target_user_id,\n\u001b[1;32m     24\u001b[0m     target_user_text,\n\u001b[1;32m     25\u001b[0m     threshold\u001b[38;5;241m=\u001b[39mthreshold\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# (1) ì‹¤í—˜ íŒŒë¼ë¯¸í„° (ê³ ì •ê°’)\n",
    "threshold = 0.007\n",
    "k_val = 750\n",
    "window_size = 1\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© DataFrame\n",
    "all_results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\",\n",
    "                                    \"threshold\", \"faiss_k\", \"window\"])\n",
    "\n",
    "# (2) ìœ ì € ë‹¨ì¼ ì„¸íŒ… ì‹¤í—˜ (ì˜ˆ: 5ëª… ìœ ì € ë°˜ë³µ)\n",
    "for idx in tqdm(range(3001, 6041)):\n",
    "    results = pd.DataFrame(columns=[\"UserId\", \"Hit\", \"Answer\", \"Recommended\"])\n",
    "\n",
    "    target_user_id = str(idx + 1)  # ìœ ì € IDëŠ” 1ë¶€í„° ì‹œì‘\n",
    "    target_user_text = df.loc[df.UserId == int(target_user_id), 'chunk_header'].iloc[0]\n",
    "\n",
    "    # (2-1) Raptor Tree ê²€ìƒ‰\n",
    "    best_cluster_id, similar_users = raptor_tree.search_user_cluster(\n",
    "        target_user_id,\n",
    "        target_user_text,\n",
    "        threshold=threshold\n",
    "    )\n",
    "\n",
    "    # ìµœì‹  êµ¬ë§¤ ê¸°ë¡ì—ì„œ ë§ˆì§€ë§‰ í•­ëª©ì„ ì§ˆì˜ë¡œ ì‚¬ìš©\n",
    "    purchase_history = data.iloc[idx]['movie_explain']\n",
    "    query = \" \".join(purchase_history[-1:])\n",
    "\n",
    "    # (2-2) FAISS ê²€ìƒ‰\n",
    "    retriever_k = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": k_val}\n",
    "    )\n",
    "    records = retriever_k.get_relevant_documents(query)\n",
    "\n",
    "    record_user_ids = [str(record.metadata['UserId']) for record in records]\n",
    "    intersection = set(map(str, similar_users)).intersection(set(record_user_ids))\n",
    "\n",
    "    filtered_records = [\n",
    "        record for record in records\n",
    "        if str(record.metadata['UserId']) in intersection\n",
    "    ]\n",
    "\n",
    "    # (2-3) ë¬¸ë§¥ í™•ì¥ (window size ì ìš©)\n",
    "    context_results = get_documents_with_context(\n",
    "        vectorstore,\n",
    "        filtered_records,\n",
    "        context_window=window_size\n",
    "    )\n",
    "    flattened_results = [doc for sublist in context_results for doc in sublist]\n",
    "\n",
    "    # ìœ ì €-ì˜í™” ê·¸ë˜í”„ ìƒì„± ë° ì¶”ì²œ ì˜í™” ì¶”ì¶œ\n",
    "    user_movies = get_user_movies(flattened_results)\n",
    "    G = create_user_movie_graph(user_movies)\n",
    "    top_movies = get_top_10_common_movies(G)\n",
    "\n",
    "    # ê³¼ê±° ê¸°ë¡ ì œê±°\n",
    "    filtered_movies = filter_movies_by_history(\n",
    "        top_movies,\n",
    "        extract_previous_movie_ids(purchase_history)\n",
    "    )\n",
    "\n",
    "    # ì •ë‹µ ë° ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ\n",
    "    answer = df_test.iloc[idx].movie_explain\n",
    "    match = re.search(r\"(\\d+)\", answer)\n",
    "    answer_id = match.group(1) if match else None\n",
    "\n",
    "    filtered_movie_ids = [\n",
    "        re.search(r\"(\\d+)\", movie).group(1)\n",
    "        for movie, _ in filtered_movies\n",
    "        if re.search(r\"(\\d+)\", movie)\n",
    "    ]\n",
    "\n",
    "    hit = 1 if answer_id in filtered_movie_ids else 0\n",
    "\n",
    "    # ìœ ì €ë³„ ê²°ê³¼ ì €ì¥\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        \"UserId\": [target_user_id],\n",
    "        \"Hit\": [hit],\n",
    "        \"Answer\": [answer_id],\n",
    "        \"Recommended\": [\", \".join(filtered_movie_ids)]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ê°€\n",
    "    results[\"threshold\"] = threshold\n",
    "    results[\"faiss_k\"] = k_val\n",
    "    results[\"window\"] = window_size\n",
    "\n",
    "    all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "\n",
    "# (3) ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
    "performance = all_results[\"Hit\"].mean()\n",
    "print(f\"Hit Rate: {performance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
